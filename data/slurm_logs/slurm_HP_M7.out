/pfs/lustrep2/projappl/project_465002413/ChessGPT/chessgpt_multitask.py:33: DtypeWarning: Columns (3,5) have mixed types. Specify dtype option on import or set low_memory=False.
  df = pd.read_csv(csv_path)
/pfs/lustrep2/projappl/project_465002413/ChessGPT/chessgpt_multitask.py:80: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.
The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.

For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.


  df_white_black['white_elo'].fillna(df_white_black['white_elo'].mean(), inplace=True)
/pfs/lustrep2/projappl/project_465002413/ChessGPT/chessgpt_multitask.py:81: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.
The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.

For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.


  df_white_black['black_elo'].fillna(df_white_black['black_elo'].mean(), inplace=True)
Total: 20,853
Train: 17,725 (85.00%)
Val:   1,042 (5.00%)
Test:  2,086 (10.00%)
|-|-|-|-|-|mean_elo: 2663.78369140625|-|-|-|-|
|-|-|-|-|-|std_elo: 110.49032592773438|-|-|-|-|

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  9.32it/s]
Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00, 17.44it/s]
HYPERPARAM: 
 shared_dim1=1024, 
 shared_dim2=512, 
 head_dim=256, 
 dropout=0.0
HYPERPARAM: tokenizer, max_length=256
139
Unfroze last transformer layer.
HYPERPARAM: lrs: 5e-4  &  1e-4
HYPERPARAM: weight_decays: 0.1  &  0.00001
Using device: cuda
HYPERPARAM: alpha: 0.2
Done 32 Batches of 139
class_loss = 2.9973275661468506
macro_f1 = 0.004454611124488321
weighted_f1 = 0.00415374525216905
regression_loss = 0.41151243448257446
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [6.25e-05, 1.25e-05]
Done 64 Batches of 139
class_loss = 2.9972317218780518
macro_f1 = 0.005118249205788916
weighted_f1 = 0.005521920646126435
regression_loss = 0.3388890326023102
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.000125, 2.5e-05]
Done 96 Batches of 139
class_loss = 2.989957809448242
macro_f1 = 0.004888541259288228
weighted_f1 = 0.0050252274458143795
regression_loss = 0.34260183572769165
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0001875, 3.7500000000000003e-05]
Done 128 Batches of 139
class_loss = 2.9964284896850586
macro_f1 = 0.013838639519131996
weighted_f1 = 0.01414634648711579
regression_loss = 0.31354478001594543
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00025, 5e-05]
Done 139 Batches of 139
class_loss = 2.989704132080078
macro_f1 = 0.016852025444382468
weighted_f1 = 0.016913866634316266
regression_loss = 0.38033732771873474
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0003125, 6.25e-05]
3.0640487670898438
3.062143325805664
3.058939218521118
3.0387380123138428
3.0500030517578125
3.0351691246032715
3.0425925254821777
3.045891761779785
3.0511574745178223
avg_train_loss 3.066444961287135
avg_val_loss 3.049853695763482
Done 32 Batches of 139
class_loss = 2.976452350616455
macro_f1 = 0.030860848059260943
weighted_f1 = 0.03264384840717577
regression_loss = 0.3356062173843384
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.000375, 7.500000000000001e-05]
Done 64 Batches of 139
class_loss = 2.961418867111206
macro_f1 = 0.036051261338403445
weighted_f1 = 0.03645911621204935
regression_loss = 0.34529799222946167
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0004375, 8.75e-05]
Done 96 Batches of 139
class_loss = 2.9283127784729004
macro_f1 = 0.0696265339507415
weighted_f1 = 0.07023734703449613
regression_loss = 0.37719041109085083
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0005, 0.0001]
Done 128 Batches of 139
class_loss = 2.7657382488250732
macro_f1 = 0.08582100512149696
weighted_f1 = 0.08590167636511188
regression_loss = 0.3350234925746918
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0004997993579454254, 9.995987158908508e-05]
Done 139 Batches of 139
class_loss = 2.7217857837677
macro_f1 = 0.08409762076638785
weighted_f1 = 0.08414324172793229
regression_loss = 0.522147536277771
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0004995987158908507, 9.991974317817014e-05]
2.853745460510254
2.892425775527954
2.8314855098724365
2.7357232570648193
2.8244006633758545
2.8903300762176514
2.7602460384368896
2.822254180908203
2.521226167678833
avg_train_loss 2.9699852380821175
avg_val_loss 2.792426347732544
Done 32 Batches of 139
class_loss = 2.8088767528533936
macro_f1 = 0.08038065244645266
weighted_f1 = 0.08120784734166281
regression_loss = 0.2830207049846649
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0004993980738362761, 9.987961476725522e-05]
Done 64 Batches of 139
class_loss = 2.7155258655548096
macro_f1 = 0.10598252120835042
weighted_f1 = 0.10546596809996048
regression_loss = 0.286676287651062
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0004991974317817015, 9.98394863563403e-05]
Done 96 Batches of 139
class_loss = 2.6328177452087402
macro_f1 = 0.12694926450208635
weighted_f1 = 0.12713333190253615
regression_loss = 0.37446916103363037
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0004989967897271268, 9.979935794542536e-05]
Done 128 Batches of 139
class_loss = 2.519360065460205
macro_f1 = 0.13171514513808882
weighted_f1 = 0.13162778519426008
regression_loss = 0.3870934247970581
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0004987961476725522, 9.975922953451044e-05]
Done 139 Batches of 139
class_loss = 2.478363037109375
macro_f1 = 0.1363283867423452
weighted_f1 = 0.13633875227730133
regression_loss = 0.2682008147239685
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0004985955056179776, 9.971910112359551e-05]
2.6878914833068848
2.6910879611968994
2.7584240436553955
2.608607053756714
2.6251208782196045
2.541501760482788
2.549450159072876
2.6312379837036133
2.6555449962615967
avg_train_loss 2.725204222493892
avg_val_loss 2.6387629244062634
Done 32 Batches of 139
class_loss = 2.4658074378967285
macro_f1 = 0.15076955627498645
weighted_f1 = 0.14837721996638678
regression_loss = 0.34505695104599
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.000498394863563403, 9.967897271268059e-05]
Done 64 Batches of 139
class_loss = 2.4718427658081055
macro_f1 = 0.17729667275630456
weighted_f1 = 0.17584373103990855
regression_loss = 0.2686150074005127
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0004981942215088283, 9.963884430176565e-05]
Done 96 Batches of 139
class_loss = 2.430734634399414
macro_f1 = 0.1979940256180967
weighted_f1 = 0.1981726823948742
regression_loss = 0.352939248085022
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0004979935794542536, 9.959871589085073e-05]
Done 128 Batches of 139
class_loss = 2.4021830558776855
macro_f1 = 0.2082506793605182
weighted_f1 = 0.20827482893460705
regression_loss = 0.3458963632583618
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.000497792937399679, 9.955858747993579e-05]
Done 139 Batches of 139
class_loss = 2.438072443008423
macro_f1 = 0.2136120098677959
weighted_f1 = 0.21348550405716443
regression_loss = 0.2777891159057617
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0004975922953451043, 9.951845906902088e-05]
2.551060676574707
2.4945907592773438
2.4945199489593506
2.4569990634918213
2.431429624557495
2.406907558441162
2.278977632522583
2.412214756011963
2.4174587726593018
avg_train_loss 2.513314185382651
avg_val_loss 2.4382398658328586
Done 32 Batches of 139
class_loss = 2.264247417449951
macro_f1 = 0.24973832376172628
weighted_f1 = 0.25121425028913325
regression_loss = 0.2881914973258972
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0004973916532905297, 9.947833065810594e-05]
Done 64 Batches of 139
class_loss = 2.2352824211120605
macro_f1 = 0.24943823624060063
weighted_f1 = 0.2514380060058309
regression_loss = 0.38373255729675293
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0004971910112359551, 9.943820224719102e-05]
Done 96 Batches of 139
class_loss = 2.1037392616271973
macro_f1 = 0.26088139184413844
weighted_f1 = 0.2614880951216663
regression_loss = 0.3167935907840729
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0004969903691813805, 9.939807383627608e-05]
Done 128 Batches of 139
class_loss = 2.218777894973755
macro_f1 = 0.27647401151750006
weighted_f1 = 0.2765694271457832
regression_loss = 0.36176347732543945
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0004967897271268058, 9.935794542536117e-05]
Done 139 Batches of 139
class_loss = 2.1899454593658447
macro_f1 = 0.2790871043870165
weighted_f1 = 0.27900226111642623
regression_loss = 0.32776883244514465
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0004965890850722311, 9.931781701444623e-05]
2.3505823612213135
2.3675830364227295
2.2431576251983643
2.3155555725097656
2.172802448272705
2.2937960624694824
2.1045327186584473
2.1837685108184814
1.951806664466858
avg_train_loss 2.3000840471802855
avg_val_loss 2.220398333337572
2.2348194122314453
2.2281651496887207
2.183791160583496
1.9765815734863281
2.1843008995056152
2.3012590408325195
2.2949578762054443
2.39199161529541
2.354091167449951
2.124891519546509
2.0900304317474365
2.179978609085083
1.9961621761322021
2.2571866512298584
2.213933229446411
2.121370553970337
1.8480197191238403
