/pfs/lustrep2/projappl/project_465002413/ChessGPT/chessgpt_multitask.py:40: DtypeWarning: Columns (3,5) have mixed types. Specify dtype option on import or set low_memory=False.
  df = pd.read_csv(csv_path)
/pfs/lustrep2/projappl/project_465002413/ChessGPT/chessgpt_multitask.py:86: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.
The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.

For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.


  df_white_black['white_elo'].fillna(df_white_black['white_elo'].mean(), inplace=True)
/pfs/lustrep2/projappl/project_465002413/ChessGPT/chessgpt_multitask.py:87: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.
The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.

For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.


  df_white_black['black_elo'].fillna(df_white_black['black_elo'].mean(), inplace=True)
Total: 163,249
Train: 138,761 (85.00%)
Val:   8,162 (5.00%)
Test:  16,326 (10.00%)
|-|-|-|-|-|mean_elo: 2663.914794921875|-|-|-|-|
|-|-|-|-|-|std_elo: 111.10905456542969|-|-|-|-|

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.20s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  2.85s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.36s/it]
HYPERPARAM: 
 shared_dim1=1024, 
 shared_dim2=512, 
 head_dim=256, 
 dropout=0.0
HYPERPARAM: tokenizer, max_length=256
1085
Unfroze last transformer layer.
HYPERPARAM: lrs: 5e-4  &  1e-4
HYPERPARAM: weight_decays: 0.0
Using device: cuda
HYPERPARAM: alpha: 0.2
-------------------------------------------------
Done 32 Batches of 1085
class_loss = 2.995088815689087
macro_f1 = 0.004980842911877394
weighted_f1 = 0.005221851439871462
regression_loss = 0.2918097674846649
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [2e-05, 4.000000000000001e-06]
-------------------------------------------------
Done 64 Batches of 1085
class_loss = 2.9926018714904785
macro_f1 = 0.004805194805194805
weighted_f1 = 0.004850481859410431
regression_loss = 0.3484655022621155
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [4e-05, 8.000000000000001e-06]
-------------------------------------------------
Done 96 Batches of 1085
class_loss = 2.988940715789795
macro_f1 = 0.004814466885861907
weighted_f1 = 0.004869896603297817
regression_loss = 0.3776895999908447
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [6e-05, 1.2e-05]
-------------------------------------------------
Done 128 Batches of 1085
class_loss = 2.992729425430298
macro_f1 = 0.006616016079790038
weighted_f1 = 0.006587942046032352
regression_loss = 0.3165876269340515
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [8e-05, 1.6000000000000003e-05]
-------------------------------------------------
Done 160 Batches of 1085
class_loss = 2.993595600128174
macro_f1 = 0.015470162221348305
weighted_f1 = 0.01574699191041764
regression_loss = 0.36151260137557983
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0001, 2e-05]
-------------------------------------------------
Done 192 Batches of 1085
class_loss = 2.9930472373962402
macro_f1 = 0.02387735930047338
weighted_f1 = 0.02454202069098148
regression_loss = 0.41173869371414185
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00012, 2.4e-05]
-------------------------------------------------
Done 224 Batches of 1085
class_loss = 2.9888150691986084
macro_f1 = 0.026544429853999203
weighted_f1 = 0.027079611103919606
regression_loss = 0.3599928021430969
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00014000000000000001, 2.8000000000000003e-05]
-------------------------------------------------
Done 256 Batches of 1085
class_loss = 2.9802799224853516
macro_f1 = 0.034374218545763634
weighted_f1 = 0.034888695836155226
regression_loss = 0.4251874089241028
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00016, 3.2000000000000005e-05]
-------------------------------------------------
Done 288 Batches of 1085
class_loss = 2.9769935607910156
macro_f1 = 0.036043267843857876
weighted_f1 = 0.036585321674985724
regression_loss = 0.4002218246459961
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00017999999999999998, 3.6e-05]
-------------------------------------------------
Done 320 Batches of 1085
class_loss = 2.9540717601776123
macro_f1 = 0.037346872898311095
weighted_f1 = 0.03776080625377685
regression_loss = 0.38208234310150146
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0002, 4e-05]
-------------------------------------------------
Done 352 Batches of 1085
class_loss = 2.9525158405303955
macro_f1 = 0.04173058433274025
weighted_f1 = 0.042177793694634196
regression_loss = 0.3391088843345642
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00022, 4.4000000000000006e-05]
-------------------------------------------------
Done 384 Batches of 1085
class_loss = 2.872810125350952
macro_f1 = 0.04239845965286918
weighted_f1 = 0.04281161201508134
regression_loss = 0.31365537643432617
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00024, 4.8e-05]
-------------------------------------------------
Done 416 Batches of 1085
class_loss = 2.802673816680908
macro_f1 = 0.04335354866259082
weighted_f1 = 0.04377004036862347
regression_loss = 0.31260251998901367
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00026000000000000003, 5.2000000000000004e-05]
-------------------------------------------------
Done 448 Batches of 1085
class_loss = 2.7564756870269775
macro_f1 = 0.043781822232093384
weighted_f1 = 0.044188188328609136
regression_loss = 0.4096701741218567
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00028000000000000003, 5.6000000000000006e-05]
-------------------------------------------------
Done 480 Batches of 1085
class_loss = 2.774868965148926
macro_f1 = 0.05235198276022944
weighted_f1 = 0.05269793532496643
regression_loss = 0.3393053114414215
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0003, 6e-05]
-------------------------------------------------
Done 512 Batches of 1085
class_loss = 2.7317755222320557
macro_f1 = 0.06241462835742713
weighted_f1 = 0.06270569095051415
regression_loss = 0.39272475242614746
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00032, 6.400000000000001e-05]
-------------------------------------------------
Done 544 Batches of 1085
class_loss = 2.630990505218506
macro_f1 = 0.07035199887579675
weighted_f1 = 0.07053847293341145
regression_loss = 0.35618171095848083
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00034, 6.800000000000001e-05]
-------------------------------------------------
Done 576 Batches of 1085
class_loss = 2.582669496536255
macro_f1 = 0.08046999883622401
weighted_f1 = 0.08059685005797831
regression_loss = 0.3200532793998718
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00035999999999999997, 7.2e-05]
-------------------------------------------------
Done 608 Batches of 1085
class_loss = 2.7887637615203857
macro_f1 = 0.08600393801440025
weighted_f1 = 0.0861408906973017
regression_loss = 0.34340834617614746
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00038, 7.6e-05]
-------------------------------------------------
Done 640 Batches of 1085
class_loss = 2.629370927810669
macro_f1 = 0.08910963139018385
weighted_f1 = 0.08931224691717353
regression_loss = 0.399261474609375
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0004, 8e-05]
-------------------------------------------------
Done 672 Batches of 1085
class_loss = 2.6239421367645264
macro_f1 = 0.09283336089576608
weighted_f1 = 0.09298492507226273
regression_loss = 0.29682934284210205
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00042, 8.4e-05]
-------------------------------------------------
Done 704 Batches of 1085
class_loss = 2.6089749336242676
macro_f1 = 0.09820432038060849
weighted_f1 = 0.09836163968690972
regression_loss = 0.2955031394958496
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00044, 8.800000000000001e-05]
-------------------------------------------------
Done 736 Batches of 1085
class_loss = 2.593012809753418
macro_f1 = 0.10280644549916103
weighted_f1 = 0.10287466434729925
regression_loss = 0.301647424697876
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00046, 9.200000000000001e-05]
-------------------------------------------------
Done 768 Batches of 1085
class_loss = 2.5725879669189453
macro_f1 = 0.10727154091930469
weighted_f1 = 0.10727061090484942
regression_loss = 0.4184572100639343
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00048, 9.6e-05]
-------------------------------------------------
Done 800 Batches of 1085
class_loss = 2.4155471324920654
macro_f1 = 0.1118857501795995
weighted_f1 = 0.11186444344160724
regression_loss = 0.5307602286338806
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0005, 0.0001]
-------------------------------------------------
Done 832 Batches of 1085
class_loss = 2.4383721351623535
macro_f1 = 0.11577784945398426
weighted_f1 = 0.11571462573540071
regression_loss = 0.3311612010002136
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0004992592592592593, 9.985185185185185e-05]
-------------------------------------------------
Done 864 Batches of 1085
class_loss = 2.5092709064483643
macro_f1 = 0.12080984418413007
weighted_f1 = 0.12078901220038069
regression_loss = 0.3206508159637451
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0004985185185185186, 9.970370370370371e-05]
-------------------------------------------------
Done 896 Batches of 1085
class_loss = 2.236877918243408
macro_f1 = 0.125363245572085
weighted_f1 = 0.12527855988448805
regression_loss = 0.3830580413341522
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0004977777777777778, 9.955555555555556e-05]
-------------------------------------------------
Done 928 Batches of 1085
class_loss = 2.350968599319458
macro_f1 = 0.12976480722272182
weighted_f1 = 0.12960951092644182
regression_loss = 0.45016467571258545
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.000497037037037037, 9.940740740740742e-05]
-------------------------------------------------
Done 960 Batches of 1085
class_loss = 2.3044848442077637
macro_f1 = 0.13444296442693376
weighted_f1 = 0.13424065101252328
regression_loss = 0.3444586396217346
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0004962962962962963, 9.925925925925926e-05]
-------------------------------------------------
Done 992 Batches of 1085
class_loss = 2.3413589000701904
macro_f1 = 0.13988125093884357
weighted_f1 = 0.13971675375127066
regression_loss = 0.4212372303009033
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0004955555555555556, 9.911111111111112e-05]
-------------------------------------------------
Done 1024 Batches of 1085
class_loss = 2.242887258529663
macro_f1 = 0.14500633756811027
weighted_f1 = 0.14484766098195537
regression_loss = 0.31820762157440186
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0004948148148148148, 9.896296296296297e-05]
-------------------------------------------------
Done 1056 Batches of 1085
class_loss = 2.2744414806365967
macro_f1 = 0.15059949661549715
weighted_f1 = 0.15039857466033107
regression_loss = 0.43651068210601807
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0004940740740740741, 9.881481481481482e-05]
-------------------------------------------------
Done 1085 Batches of 1085
class_loss = 2.086832046508789
macro_f1 = 0.15572618441044947
weighted_f1 = 0.1555168307739552
regression_loss = 0.35396453738212585
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0004933333333333334, 9.866666666666668e-05]
2.1285345554351807
2.1777145862579346
2.304546356201172
1.9949893951416016
2.1228725910186768
2.1525251865386963
2.1634647846221924
2.134908437728882
2.184558868408203
2.43442702293396
2.2115421295166016
2.223540782928467
2.1105477809906006
2.2316455841064453
2.1713640689849854
2.2791428565979004
2.099621534347534
2.3062918186187744
2.127676486968994
2.2032434940338135
2.1863632202148438
2.1188366413116455
2.142284393310547
2.1847927570343018
2.1752350330352783
2.1760902404785156
2.1249632835388184
2.159310817718506
2.4375336170196533
2.391935348510742
2.1671552658081055
2.256389617919922
2.051572799682617
2.276388645172119
2.185661792755127
2.1209590435028076
2.14392352104187
2.2128772735595703
2.075042247772217
2.1415748596191406
2.0006215572357178
2.110388994216919
2.1574018001556396
2.0667476654052734
2.283783435821533
2.158923387527466
2.132277011871338
2.2421021461486816
2.132629871368408
2.1134653091430664
2.081192970275879
2.0092999935150146
2.222123146057129
2.3075850009918213
2.212827682495117
2.1634979248046875
2.1933560371398926
2.0266740322113037
2.111032485961914
2.2146260738372803
2.1318869590759277
2.192234516143799
2.170602321624756
2.1086878776550293
Epoch 0, training metrics:
--- CLASSIFICATION METRICS --- 
F1 scores: [Macro=0.156, Weighted=0.156] 
Balanced Accuracy = 0.158
Average loss = 2.75489
Epoch 0, validation metrics:
--- CLASSIFICATION METRICS --- 
F1 scores: [Macro=0.310, Weighted=0.309] 
Balanced Accuracy = 0.334
Average loss = 2.17244
avg_train_loss 2.754885750432168
avg_val_loss 2.1724377647042274
-------------------------------------------------
Done 32 Batches of 1085
class_loss = 2.1134090423583984
macro_f1 = 0.30440220725884204
weighted_f1 = 0.30251502102993627
regression_loss = 0.38210827112197876
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0004925925925925925, 9.851851851851852e-05]
-------------------------------------------------
Done 64 Batches of 1085
class_loss = 2.0487802028656006
macro_f1 = 0.3208107758839972
weighted_f1 = 0.3185587712381697
regression_loss = 0.2575950622558594
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0004918518518518519, 9.837037037037038e-05]
-------------------------------------------------
Done 96 Batches of 1085
class_loss = 2.008159875869751
macro_f1 = 0.3282066936027064
weighted_f1 = 0.3266610090456533
regression_loss = 0.2846132814884186
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0004911111111111111, 9.822222222222223e-05]
-------------------------------------------------
Done 128 Batches of 1085
class_loss = 1.8134500980377197
macro_f1 = 0.3365102284448147
weighted_f1 = 0.3355177835056857
regression_loss = 0.29038190841674805
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0004903703703703704, 9.807407407407407e-05]
-------------------------------------------------
Done 160 Batches of 1085
class_loss = 1.8980395793914795
macro_f1 = 0.3412228019011689
weighted_f1 = 0.34009477505333346
regression_loss = 0.40365177392959595
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0004896296296296297, 9.792592592592593e-05]
-------------------------------------------------
Done 192 Batches of 1085
class_loss = 1.8633559942245483
macro_f1 = 0.3489666190773605
weighted_f1 = 0.34773500687796816
regression_loss = 0.3171505331993103
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0004888888888888889, 9.777777777777778e-05]
-------------------------------------------------
Done 224 Batches of 1085
class_loss = 1.9275587797164917
macro_f1 = 0.3592946410181721
weighted_f1 = 0.3580249049399271
regression_loss = 0.31640350818634033
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0004881481481481482, 9.762962962962964e-05]
-------------------------------------------------
Done 256 Batches of 1085
class_loss = 1.8814862966537476
macro_f1 = 0.36564191861747264
weighted_f1 = 0.36466137892944567
regression_loss = 0.3475532531738281
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00048740740740740743, 9.748148148148149e-05]
-------------------------------------------------
Done 288 Batches of 1085
class_loss = 1.735168695449829
macro_f1 = 0.3703593643830354
weighted_f1 = 0.36941725251456115
regression_loss = 0.46392005681991577
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0004866666666666667, 9.733333333333335e-05]
-------------------------------------------------
Done 320 Batches of 1085
class_loss = 1.7438080310821533
macro_f1 = 0.3752708536136109
weighted_f1 = 0.37446926276657627
regression_loss = 0.24280159175395966
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00048592592592592595, 9.718518518518519e-05]
-------------------------------------------------
Done 352 Batches of 1085
class_loss = 1.8315284252166748
macro_f1 = 0.3791098009970573
weighted_f1 = 0.37858541377570826
regression_loss = 0.36403465270996094
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0004851851851851852, 9.703703703703704e-05]
-------------------------------------------------
Done 384 Batches of 1085
class_loss = 1.6719619035720825
macro_f1 = 0.3855549532357188
weighted_f1 = 0.3851569218426952
regression_loss = 0.28704947233200073
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00048444444444444446, 9.68888888888889e-05]
-------------------------------------------------
Done 416 Batches of 1085
class_loss = 1.5971460342407227
macro_f1 = 0.3915281454040615
weighted_f1 = 0.39096050137070043
regression_loss = 0.3323264718055725
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0004837037037037037, 9.674074074074074e-05]
-------------------------------------------------
Done 448 Batches of 1085
class_loss = 1.667946457862854
macro_f1 = 0.39616781196591105
weighted_f1 = 0.3958649584012603
regression_loss = 0.2995343804359436
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.000482962962962963, 9.65925925925926e-05]
-------------------------------------------------
Done 480 Batches of 1085
class_loss = 1.6767477989196777
macro_f1 = 0.4011619559140578
weighted_f1 = 0.4010574489825529
regression_loss = 0.2995625138282776
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0004822222222222222, 9.644444444444445e-05]
-------------------------------------------------
Done 512 Batches of 1085
class_loss = 1.4456090927124023
macro_f1 = 0.40679115899742657
weighted_f1 = 0.40661637869773287
regression_loss = 0.29103922843933105
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00048148148148148144, 9.62962962962963e-05]
-------------------------------------------------
Done 544 Batches of 1085
class_loss = 1.5285189151763916
macro_f1 = 0.41259530870179084
weighted_f1 = 0.41262384681341513
regression_loss = 0.2791479825973511
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0004807407407407408, 9.614814814814816e-05]
-------------------------------------------------
Done 576 Batches of 1085
class_loss = 1.6564737558364868
macro_f1 = 0.4192679463218603
weighted_f1 = 0.41943103666447523
regression_loss = 0.28267714381217957
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00048, 9.6e-05]
-------------------------------------------------
Done 608 Batches of 1085
class_loss = 1.5377695560455322
macro_f1 = 0.4240474649798517
weighted_f1 = 0.4240487379236254
regression_loss = 0.3051045536994934
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0004792592592592593, 9.585185185185186e-05]
-------------------------------------------------
Done 640 Batches of 1085
class_loss = 1.4937822818756104
macro_f1 = 0.4287108047394616
weighted_f1 = 0.4286790524003757
regression_loss = 0.3387208580970764
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00047851851851851853, 9.570370370370371e-05]
-------------------------------------------------
Done 672 Batches of 1085
class_loss = 1.468549370765686
macro_f1 = 0.4337852089010304
weighted_f1 = 0.4337471625287083
regression_loss = 0.3040962815284729
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0004777777777777778, 9.555555555555557e-05]
-------------------------------------------------
Done 704 Batches of 1085
class_loss = 1.493904709815979
macro_f1 = 0.43842318393564633
weighted_f1 = 0.43828661719224965
regression_loss = 0.35548165440559387
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00047703703703703705, 9.540740740740741e-05]
-------------------------------------------------
Done 736 Batches of 1085
class_loss = 1.4150251150131226
macro_f1 = 0.443254588751934
weighted_f1 = 0.4430978119018696
regression_loss = 0.3334959149360657
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0004762962962962963, 9.525925925925926e-05]
-------------------------------------------------
Done 768 Batches of 1085
class_loss = 1.3333483934402466
macro_f1 = 0.4480795094919112
weighted_f1 = 0.4480042723013556
regression_loss = 0.33050474524497986
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00047555555555555556, 9.511111111111112e-05]
-------------------------------------------------
Done 800 Batches of 1085
class_loss = 1.3657201528549194
macro_f1 = 0.4521195789542749
weighted_f1 = 0.4520494340441317
regression_loss = 0.2557564377784729
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0004748148148148148, 9.496296296296297e-05]
-------------------------------------------------
Done 832 Batches of 1085
class_loss = 1.3120747804641724
macro_f1 = 0.45618891628426067
weighted_f1 = 0.45613511390987727
regression_loss = 0.3246544599533081
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0004740740740740741, 9.481481481481483e-05]
-------------------------------------------------
Done 864 Batches of 1085
class_loss = 1.1308460235595703
macro_f1 = 0.46001049316521525
weighted_f1 = 0.45997980599114946
regression_loss = 0.3330915868282318
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00047333333333333336, 9.466666666666667e-05]
-------------------------------------------------
Done 896 Batches of 1085
class_loss = 1.2131507396697998
macro_f1 = 0.4645547222453443
weighted_f1 = 0.4645361326680821
regression_loss = 0.2961598038673401
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00047259259259259265, 9.451851851851853e-05]
-------------------------------------------------
Done 928 Batches of 1085
class_loss = 1.438725471496582
macro_f1 = 0.4684519611576608
weighted_f1 = 0.468373630536085
regression_loss = 0.34227797389030457
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0004718518518518519, 9.437037037037038e-05]
-------------------------------------------------
Done 960 Batches of 1085
class_loss = 1.252610445022583
macro_f1 = 0.47292817815241667
weighted_f1 = 0.47286715494095144
regression_loss = 0.29134154319763184
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0004711111111111111, 9.422222222222223e-05]
-------------------------------------------------
Done 992 Batches of 1085
class_loss = 1.1382925510406494
macro_f1 = 0.47635603459124554
weighted_f1 = 0.4763348527049367
regression_loss = 0.2805640697479248
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0004703703703703704, 9.407407407407408e-05]
-------------------------------------------------
Done 1024 Batches of 1085
class_loss = 1.3807320594787598
macro_f1 = 0.48007460839556726
weighted_f1 = 0.48002291122055185
regression_loss = 0.2582869827747345
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00046962962962962963, 9.392592592592593e-05]
-------------------------------------------------
Done 1056 Batches of 1085
class_loss = 0.8966288566589355
macro_f1 = 0.4834676778977653
weighted_f1 = 0.4833702817775164
regression_loss = 0.3001559376716614
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0004688888888888889, 9.377777777777779e-05]
-------------------------------------------------
Done 1085 Batches of 1085
class_loss = 1.6300678253173828
macro_f1 = 0.4868793952339424
weighted_f1 = 0.48677190794836195
regression_loss = 0.24961021542549133
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00046814814814814815, 9.362962962962964e-05]
1.2407028675079346
1.3521449565887451
1.4931010007858276
1.1572493314743042
1.1298003196716309
1.22725510597229
1.2976765632629395
1.2278735637664795
1.2668737173080444
1.4706817865371704
1.3253977298736572
1.301121473312378
1.2708494663238525
1.546553373336792
1.26161527633667
1.3792792558670044
1.150420069694519
1.419451355934143
1.2492338418960571
1.3964966535568237
1.2645072937011719
1.3016588687896729
1.301224708557129
1.1139848232269287
1.4193122386932373
1.3662970066070557
1.2165837287902832
1.0827921628952026
1.3245550394058228
1.5553423166275024
1.3983310461044312
1.4232478141784668
1.013549566268921
1.524808406829834
1.2440818548202515
1.2510277032852173
1.2000125646591187
1.3364864587783813
1.2665493488311768
1.336786150932312
0.9764773845672607
1.2034475803375244
1.2381418943405151
1.3089860677719116
1.4695109128952026
1.237443447113037
1.2948983907699585
1.392370581626892
1.2583764791488647
1.251335859298706
1.2251858711242676
1.0754587650299072
1.297424554824829
1.3432142734527588
1.5159404277801514
1.1929078102111816
1.3065046072006226
1.02965247631073
1.1527899503707886
1.1453557014465332
1.4445866346359253
1.236289620399475
1.2222962379455566
1.2060120105743408
Epoch 1, training metrics:
--- CLASSIFICATION METRICS --- 
F1 scores: [Macro=0.487, Weighted=0.487] 
Balanced Accuracy = 0.499
Average loss = 1.64767
Epoch 1, validation metrics:
--- CLASSIFICATION METRICS --- 
F1 scores: [Macro=0.604, Weighted=0.604] 
Balanced Accuracy = 0.610
Average loss = 1.28327
avg_train_loss 1.6476738245805838
avg_val_loss 1.2832738179713488
-------------------------------------------------
Done 32 Batches of 1085
class_loss = 1.1206151247024536
macro_f1 = 0.6153455016294767
weighted_f1 = 0.6180768533545925
regression_loss = 0.25729215145111084
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0004674074074074074, 9.348148148148148e-05]
-------------------------------------------------
Done 64 Batches of 1085
class_loss = 1.209928035736084
macro_f1 = 0.6215872442632266
weighted_f1 = 0.6232257436775487
regression_loss = 0.27531787753105164
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00046666666666666666, 9.333333333333334e-05]
-------------------------------------------------
Done 96 Batches of 1085
class_loss = 1.2895944118499756
macro_f1 = 0.6214288220117189
weighted_f1 = 0.6227356909011812
regression_loss = 0.24060389399528503
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0004659259259259259, 9.318518518518519e-05]
-------------------------------------------------
Done 128 Batches of 1085
class_loss = 1.1409270763397217
macro_f1 = 0.6226124905425591
weighted_f1 = 0.6232366260566398
regression_loss = 0.3378567695617676
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00046518518518518523, 9.303703703703705e-05]
-------------------------------------------------
Done 160 Batches of 1085
class_loss = 0.990724503993988
macro_f1 = 0.6260886645576695
weighted_f1 = 0.626733132569708
regression_loss = 0.41411328315734863
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00046444444444444446, 9.28888888888889e-05]
-------------------------------------------------
Done 192 Batches of 1085
class_loss = 1.0105853080749512
macro_f1 = 0.6299239579057404
weighted_f1 = 0.6305714908483121
regression_loss = 0.2597402334213257
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00046370370370370375, 9.274074074074076e-05]
-------------------------------------------------
Done 224 Batches of 1085
class_loss = 1.004197120666504
macro_f1 = 0.6314461948694489
weighted_f1 = 0.6319547940809824
regression_loss = 0.3317002058029175
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.000462962962962963, 9.25925925925926e-05]
-------------------------------------------------
Done 256 Batches of 1085
class_loss = 1.149170160293579
macro_f1 = 0.6298726536617976
weighted_f1 = 0.6307705247920307
regression_loss = 0.28228646516799927
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0004622222222222222, 9.244444444444445e-05]
-------------------------------------------------
Done 288 Batches of 1085
class_loss = 1.1017355918884277
macro_f1 = 0.630338727536105
weighted_f1 = 0.6310744373025621
regression_loss = 0.29998141527175903
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0004614814814814815, 9.229629629629631e-05]
-------------------------------------------------
Done 320 Batches of 1085
class_loss = 1.066643476486206
macro_f1 = 0.6315449006460798
weighted_f1 = 0.6322191116484303
regression_loss = 0.31170427799224854
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00046074074074074073, 9.214814814814815e-05]
-------------------------------------------------
Done 352 Batches of 1085
class_loss = 0.9720293879508972
macro_f1 = 0.633829513744667
weighted_f1 = 0.634495828027455
regression_loss = 0.29450106620788574
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00046, 9.200000000000001e-05]
-------------------------------------------------
Done 384 Batches of 1085
class_loss = 0.9760546684265137
macro_f1 = 0.6380084477654026
weighted_f1 = 0.6387326841675116
regression_loss = 0.36435309052467346
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00045925925925925925, 9.185185185185186e-05]
-------------------------------------------------
Done 416 Batches of 1085
class_loss = 1.0330710411071777
macro_f1 = 0.6381821465851159
weighted_f1 = 0.6388636557866957
regression_loss = 0.269016832113266
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0004585185185185185, 9.17037037037037e-05]
-------------------------------------------------
Done 448 Batches of 1085
class_loss = 1.070541501045227
macro_f1 = 0.6378883284873296
weighted_f1 = 0.6383720210357937
regression_loss = 0.2874031662940979
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0004577777777777778, 9.155555555555557e-05]
-------------------------------------------------
Done 480 Batches of 1085
class_loss = 1.1072412729263306
macro_f1 = 0.6388991151834755
weighted_f1 = 0.6393891816333855
regression_loss = 0.303021639585495
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00045703703703703705, 9.140740740740741e-05]
-------------------------------------------------
Done 512 Batches of 1085
class_loss = 1.2639751434326172
macro_f1 = 0.6389599460746944
weighted_f1 = 0.6391304458995503
regression_loss = 0.3530205190181732
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00045629629629629633, 9.125925925925927e-05]
-------------------------------------------------
Done 544 Batches of 1085
class_loss = 1.0633056163787842
macro_f1 = 0.639761909057559
weighted_f1 = 0.6398712728504969
regression_loss = 0.2736474871635437
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00045555555555555556, 9.111111111111112e-05]
-------------------------------------------------
Done 576 Batches of 1085
class_loss = 0.9120871424674988
macro_f1 = 0.6409336742922187
weighted_f1 = 0.6410005255877634
regression_loss = 0.27313700318336487
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00045481481481481485, 9.096296296296298e-05]
-------------------------------------------------
Done 608 Batches of 1085
class_loss = 1.0647854804992676
macro_f1 = 0.6437061450908935
weighted_f1 = 0.6438521947862897
regression_loss = 0.2621484398841858
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0004540740740740741, 9.081481481481482e-05]
-------------------------------------------------
Done 640 Batches of 1085
class_loss = 0.9435397982597351
macro_f1 = 0.644772868977258
weighted_f1 = 0.6448752786704071
regression_loss = 0.3214096426963806
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0004533333333333333, 9.066666666666667e-05]
-------------------------------------------------
Done 672 Batches of 1085
class_loss = 0.9248077273368835
macro_f1 = 0.645906269656299
weighted_f1 = 0.6459101941227574
regression_loss = 0.29298773407936096
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0004525925925925926, 9.051851851851853e-05]
-------------------------------------------------
Done 704 Batches of 1085
class_loss = 0.9335578083992004
macro_f1 = 0.6467066830450359
weighted_f1 = 0.6465680161003021
regression_loss = 0.2851751446723938
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00045185185185185183, 9.037037037037038e-05]
-------------------------------------------------
Done 736 Batches of 1085
class_loss = 0.9809209704399109
macro_f1 = 0.647860385055507
weighted_f1 = 0.6478092759713109
regression_loss = 0.34325942397117615
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0004511111111111111, 9.022222222222224e-05]
-------------------------------------------------
Done 768 Batches of 1085
class_loss = 0.9243785738945007
macro_f1 = 0.6492936747382461
weighted_f1 = 0.6493009108999525
regression_loss = 0.26281052827835083
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0004503703703703704, 9.007407407407408e-05]
-------------------------------------------------
Done 800 Batches of 1085
class_loss = 1.073451042175293
macro_f1 = 0.6502912130502935
weighted_f1 = 0.6502222748830572
regression_loss = 0.23184391856193542
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0004496296296296297, 8.992592592592594e-05]
-------------------------------------------------
Done 832 Batches of 1085
class_loss = 0.9342960715293884
macro_f1 = 0.6512977426590792
weighted_f1 = 0.6511913166976122
regression_loss = 0.2456182837486267
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0004488888888888889, 8.977777777777779e-05]
-------------------------------------------------
Done 864 Batches of 1085
class_loss = 1.0344101190567017
macro_f1 = 0.652081366219038
weighted_f1 = 0.651904735510391
regression_loss = 0.2866242825984955
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00044814814814814815, 8.962962962962963e-05]
-------------------------------------------------
Done 896 Batches of 1085
class_loss = 0.9290438294410706
macro_f1 = 0.6527055588329007
weighted_f1 = 0.6525326703680975
regression_loss = 0.3237532377243042
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00044740740740740743, 8.94814814814815e-05]
-------------------------------------------------
Done 928 Batches of 1085
class_loss = 1.0837427377700806
macro_f1 = 0.6538381411677712
weighted_f1 = 0.6538831360649304
regression_loss = 0.2758978009223938
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00044666666666666666, 8.933333333333334e-05]
-------------------------------------------------
Done 960 Batches of 1085
class_loss = 1.092306137084961
macro_f1 = 0.6548715403350164
weighted_f1 = 0.6549494935382625
regression_loss = 0.24372832477092743
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00044592592592592595, 8.918518518518519e-05]
-------------------------------------------------
Done 992 Batches of 1085
class_loss = 0.9199737310409546
macro_f1 = 0.6562575954814602
weighted_f1 = 0.6562806992418375
regression_loss = 0.29651951789855957
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0004451851851851852, 8.903703703703705e-05]
-------------------------------------------------
Done 1024 Batches of 1085
class_loss = 0.8882237672805786
macro_f1 = 0.6570902768826004
weighted_f1 = 0.6571410585959315
regression_loss = 0.3845252990722656
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0004444444444444444, 8.888888888888889e-05]
-------------------------------------------------
Done 1056 Batches of 1085
class_loss = 0.9699808955192566
macro_f1 = 0.6579840423540044
weighted_f1 = 0.6580207609329576
regression_loss = 0.27308470010757446
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0004437037037037037, 8.874074074074075e-05]
-------------------------------------------------
Done 1085 Batches of 1085
class_loss = 1.1266454458236694
macro_f1 = 0.6586912385089978
weighted_f1 = 0.6586694615432985
regression_loss = 0.37879687547683716
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.000442962962962963, 8.85925925925926e-05]
1.1591984033584595
0.970887303352356
1.1742527484893799
0.9567447900772095
0.9202357530593872
0.9776233434677124
1.105309247970581
0.9371119737625122
1.0697414875030518
1.228928565979004
1.0287024974822998
1.0377901792526245
1.0259053707122803
1.2963249683380127
0.9725570678710938
1.1164063215255737
0.9587419033050537
1.0900633335113525
1.1203516721725464
0.9956523180007935
1.0163135528564453
1.0296285152435303
1.091259241104126
0.9038911461830139
1.1224373579025269
0.9799795746803284
0.9669120907783508
0.9100432991981506
1.1174182891845703
1.2395926713943481
1.192044734954834
1.2044199705123901
0.779549777507782
1.1355115175247192
1.040191650390625
1.0148380994796753
0.8906603455543518
1.1424503326416016
1.0902408361434937
1.1322453022003174
0.7217847108840942
0.9373775720596313
0.9966267347335815
1.0668282508850098
1.1910231113433838
0.9764571785926819
1.0638020038604736
1.1002488136291504
0.9913086295127869
0.9974684715270996
1.0582331418991089
0.9084423184394836
1.0496833324432373
1.0686321258544922
1.2074111700057983
0.9489646553993225
1.0021337270736694
0.8382067084312439
0.9133307933807373
0.9032884240150452
1.1659506559371948
1.0624852180480957
0.8917351365089417
0.9827388525009155
Epoch 2, training metrics:
--- CLASSIFICATION METRICS --- 
F1 scores: [Macro=0.659, Weighted=0.659] 
Balanced Accuracy = 0.665
Average loss = 1.10668
Epoch 2, validation metrics:
--- CLASSIFICATION METRICS --- 
F1 scores: [Macro=0.687, Weighted=0.687] 
Balanced Accuracy = 0.691
Average loss = 1.03416
avg_train_loss 1.1066841133179204
avg_val_loss 1.0341612389311194
-------------------------------------------------
Done 32 Batches of 1085
class_loss = 0.6333482265472412
macro_f1 = 0.7099455533059921
weighted_f1 = 0.7094147307819578
regression_loss = 0.2540988326072693
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00044222222222222227, 8.844444444444445e-05]
-------------------------------------------------
Done 64 Batches of 1085
class_loss = 0.9919753074645996
macro_f1 = 0.7118625792370545
weighted_f1 = 0.7130417839260097
regression_loss = 0.24944552779197693
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0004414814814814815, 8.82962962962963e-05]
-------------------------------------------------
Done 96 Batches of 1085
class_loss = 0.7561007738113403
macro_f1 = 0.7149796100834165
weighted_f1 = 0.7157705519070764
regression_loss = 0.2665814757347107
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0004407407407407408, 8.814814814814815e-05]
-------------------------------------------------
Done 128 Batches of 1085
class_loss = 0.905224621295929
macro_f1 = 0.7139510915549647
weighted_f1 = 0.714149370369156
regression_loss = 0.2733193039894104
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00044, 8.800000000000001e-05]
-------------------------------------------------
Done 160 Batches of 1085
class_loss = 0.7711182236671448
macro_f1 = 0.715372784032241
weighted_f1 = 0.7161959174398832
regression_loss = 0.2776954472064972
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00043925925925925925, 8.785185185185186e-05]
-------------------------------------------------
Done 192 Batches of 1085
class_loss = 0.611735999584198
macro_f1 = 0.7156718942471587
weighted_f1 = 0.7164087410638008
regression_loss = 0.27889546751976013
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00043851851851851853, 8.77037037037037e-05]
-------------------------------------------------
Done 224 Batches of 1085
class_loss = 0.9261211156845093
macro_f1 = 0.7192333984664863
weighted_f1 = 0.7196639590367906
regression_loss = 0.21314862370491028
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00043777777777777776, 8.755555555555556e-05]
-------------------------------------------------
Done 256 Batches of 1085
class_loss = 0.6261197328567505
macro_f1 = 0.7189696530871134
weighted_f1 = 0.7199545860723318
regression_loss = 0.21658122539520264
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00043703703703703705, 8.740740740740741e-05]
-------------------------------------------------
Done 288 Batches of 1085
class_loss = 0.8923039436340332
macro_f1 = 0.718373706029581
weighted_f1 = 0.7192200807231386
regression_loss = 0.26081305742263794
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0004362962962962963, 8.725925925925927e-05]
-------------------------------------------------
Done 320 Batches of 1085
class_loss = 0.7280252575874329
macro_f1 = 0.7178026860963063
weighted_f1 = 0.7189834604385611
regression_loss = 0.21472987532615662
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0004355555555555555, 8.711111111111112e-05]
-------------------------------------------------
Done 352 Batches of 1085
class_loss = 0.8883972764015198
macro_f1 = 0.717558864534837
weighted_f1 = 0.7187482977792314
regression_loss = 0.20196819305419922
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00043481481481481485, 8.696296296296296e-05]
-------------------------------------------------
Done 384 Batches of 1085
class_loss = 1.103495717048645
macro_f1 = 0.7174120101951632
weighted_f1 = 0.718300238452102
regression_loss = 0.23815180361270905
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0004340740740740741, 8.681481481481482e-05]
-------------------------------------------------
Done 416 Batches of 1085
class_loss = 0.7310886383056641
macro_f1 = 0.7187343932413675
weighted_f1 = 0.7195416901766537
regression_loss = 0.2458237111568451
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00043333333333333337, 8.666666666666667e-05]
-------------------------------------------------
Done 448 Batches of 1085
class_loss = 0.7422888875007629
macro_f1 = 0.7200560488764637
weighted_f1 = 0.7207938214070152
regression_loss = 0.22470036149024963
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0004325925925925926, 8.651851851851851e-05]
-------------------------------------------------
Done 480 Batches of 1085
class_loss = 0.8171944618225098
macro_f1 = 0.7199081566147989
weighted_f1 = 0.7203812960099478
regression_loss = 0.2474396824836731
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0004318518518518519, 8.637037037037037e-05]
-------------------------------------------------
Done 512 Batches of 1085
class_loss = 0.8812485337257385
macro_f1 = 0.7205942066868647
weighted_f1 = 0.7210750628978808
regression_loss = 0.23496505618095398
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0004311111111111111, 8.622222222222222e-05]
-------------------------------------------------
Done 544 Batches of 1085
class_loss = 0.763790488243103
macro_f1 = 0.7209585835166256
weighted_f1 = 0.7212906586800045
regression_loss = 0.250841349363327
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00043037037037037035, 8.607407407407408e-05]
-------------------------------------------------
Done 576 Batches of 1085
class_loss = 0.7820428013801575
macro_f1 = 0.7214910310381201
weighted_f1 = 0.7215929112246174
regression_loss = 0.34068259596824646
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00042962962962962963, 8.592592592592593e-05]
-------------------------------------------------
Done 608 Batches of 1085
class_loss = 0.9300897717475891
macro_f1 = 0.7216359653548101
weighted_f1 = 0.7217083033834211
regression_loss = 0.32628583908081055
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00042888888888888886, 8.577777777777777e-05]
-------------------------------------------------
Done 640 Batches of 1085
class_loss = 0.7045243978500366
macro_f1 = 0.7221451117093838
weighted_f1 = 0.722190383552157
regression_loss = 0.21772409975528717
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00042814814814814815, 8.562962962962963e-05]
-------------------------------------------------
Done 672 Batches of 1085
class_loss = 1.0059325695037842
macro_f1 = 0.7226777090134148
weighted_f1 = 0.7228321962296795
regression_loss = 0.2880249321460724
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00042740740740740743, 8.548148148148148e-05]
-------------------------------------------------
Done 704 Batches of 1085
class_loss = 0.7020100951194763
macro_f1 = 0.7235194730336102
weighted_f1 = 0.7236062204609046
regression_loss = 0.24391421675682068
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0004266666666666667, 8.533333333333334e-05]
-------------------------------------------------
Done 736 Batches of 1085
class_loss = 0.9295288324356079
macro_f1 = 0.7237346502100873
weighted_f1 = 0.7237395660420408
regression_loss = 0.3040369749069214
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00042592592592592595, 8.518518518518518e-05]
-------------------------------------------------
Done 768 Batches of 1085
class_loss = 0.7154394388198853
macro_f1 = 0.7239577588673667
weighted_f1 = 0.7239693504239101
regression_loss = 0.4010125994682312
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0004251851851851852, 8.503703703703703e-05]
-------------------------------------------------
Done 800 Batches of 1085
class_loss = 0.8405122756958008
macro_f1 = 0.7242833479144014
weighted_f1 = 0.7243508991131149
regression_loss = 0.2623044550418854
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00042444444444444447, 8.488888888888889e-05]
-------------------------------------------------
Done 832 Batches of 1085
class_loss = 0.8649118542671204
macro_f1 = 0.7243537232281416
weighted_f1 = 0.7244651591229674
regression_loss = 0.2519778907299042
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0004237037037037037, 8.474074074074074e-05]
-------------------------------------------------
Done 864 Batches of 1085
class_loss = 0.6421909928321838
macro_f1 = 0.7245970138682499
weighted_f1 = 0.7246363736004977
regression_loss = 0.29649096727371216
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.000422962962962963, 8.45925925925926e-05]
-------------------------------------------------
Done 896 Batches of 1085
class_loss = 0.6389621496200562
macro_f1 = 0.725236377156981
weighted_f1 = 0.7253180665024563
regression_loss = 0.2424643635749817
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0004222222222222222, 8.444444444444444e-05]
-------------------------------------------------
Done 928 Batches of 1085
class_loss = 0.7612705230712891
macro_f1 = 0.7260788730385188
weighted_f1 = 0.7261810392053157
regression_loss = 0.28768664598464966
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00042148148148148145, 8.429629629629629e-05]
-------------------------------------------------
Done 960 Batches of 1085
class_loss = 0.9748644232749939
macro_f1 = 0.7264641131833609
weighted_f1 = 0.7265790494660948
regression_loss = 0.25008702278137207
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00042074074074074073, 8.414814814814815e-05]
-------------------------------------------------
Done 992 Batches of 1085
class_loss = 0.8133028149604797
macro_f1 = 0.7267034460355892
weighted_f1 = 0.7267992871203153
regression_loss = 0.3027319312095642
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00042, 8.4e-05]
-------------------------------------------------
Done 1024 Batches of 1085
class_loss = 0.8701522350311279
macro_f1 = 0.7266093309435779
weighted_f1 = 0.7266807259596847
regression_loss = 0.3105730414390564
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0004192592592592593, 8.385185185185186e-05]
-------------------------------------------------
Done 1056 Batches of 1085
class_loss = 0.9013074636459351
macro_f1 = 0.7268599286979007
weighted_f1 = 0.7269031037137405
regression_loss = 0.28439387679100037
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00041851851851851853, 8.37037037037037e-05]
-------------------------------------------------
Done 1085 Batches of 1085
class_loss = 0.7006317973136902
macro_f1 = 0.7269653652038627
weighted_f1 = 0.7269568622934826
regression_loss = 1.11930513381958
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0004177777777777778, 8.355555555555556e-05]
1.0413305759429932
0.8739270567893982
1.0395644903182983
0.8561654090881348
0.8318259716033936
0.7916092872619629
0.9915721416473389
0.936130166053772
0.9197328090667725
1.1242855787277222
0.8611286282539368
0.9299893379211426
0.9151174426078796
1.1438829898834229
0.9379573464393616
1.0583959817886353
0.9025675058364868
1.0819096565246582
1.0223897695541382
0.9136251211166382
0.9554643630981445
0.9746889472007751
1.0549139976501465
0.8410178422927856
1.0165225267410278
1.055108904838562
0.8512001633644104
0.8137351870536804
1.0124576091766357
1.1978241205215454
1.0605920553207397
1.026865005493164
0.7120485305786133
1.0893970727920532
0.9858432412147522
0.9457694292068481
0.8846167325973511
1.069061279296875
0.9431279897689819
1.0615603923797607
0.6075713038444519
0.8477655053138733
0.8451117873191833
1.0233079195022583
1.1599862575531006
0.866763174533844
0.9802355170249939
0.983657956123352
0.8890640735626221
0.9032468795776367
1.0023938417434692
0.7948522567749023
1.0715292692184448
0.9845018982887268
1.2111778259277344
0.8820818066596985
0.9584892392158508
0.7815093994140625
0.8479983806610107
0.7368971109390259
1.1887727975845337
0.9916614890098572
0.8845104575157166
0.9863021373748779
Epoch 3, training metrics:
--- CLASSIFICATION METRICS --- 
F1 scores: [Macro=0.727, Weighted=0.727] 
Balanced Accuracy = 0.730
Average loss = 0.88550
Epoch 3, validation metrics:
--- CLASSIFICATION METRICS --- 
F1 scores: [Macro=0.714, Weighted=0.714] 
Balanced Accuracy = 0.715
Average loss = 0.95554
avg_train_loss 0.885499889674824
avg_val_loss 0.9555361084640026
-------------------------------------------------
Done 32 Batches of 1085
class_loss = 0.6140354871749878
macro_f1 = 0.7838445693136425
weighted_f1 = 0.7859209301097109
regression_loss = 0.23833733797073364
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00041703703703703705, 8.340740740740741e-05]
-------------------------------------------------
Done 64 Batches of 1085
class_loss = 0.6474725008010864
macro_f1 = 0.7849977380981238
weighted_f1 = 0.7856511133264165
regression_loss = 0.30413156747817993
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0004162962962962963, 8.325925925925925e-05]
-------------------------------------------------
Done 96 Batches of 1085
class_loss = 0.7323689460754395
macro_f1 = 0.7802096240977352
weighted_f1 = 0.7801901265322388
regression_loss = 0.2828987240791321
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00041555555555555557, 8.311111111111111e-05]
-------------------------------------------------
Done 128 Batches of 1085
class_loss = 0.8335680961608887
macro_f1 = 0.7783405552639551
weighted_f1 = 0.7783794882610185
regression_loss = 0.2925524115562439
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0004148148148148148, 8.296296296296296e-05]
-------------------------------------------------
Done 160 Batches of 1085
class_loss = 0.7704458832740784
macro_f1 = 0.7784433275553313
weighted_f1 = 0.7786392270895819
regression_loss = 0.22339200973510742
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0004140740740740741, 8.281481481481482e-05]
-------------------------------------------------
Done 192 Batches of 1085
class_loss = 0.6836699843406677
macro_f1 = 0.7787803660081563
weighted_f1 = 0.7786522565123024
regression_loss = 0.26397836208343506
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0004133333333333333, 8.266666666666667e-05]
-------------------------------------------------
Done 224 Batches of 1085
class_loss = 0.5896875262260437
macro_f1 = 0.7791140165309105
weighted_f1 = 0.7792124536107237
regression_loss = 0.24143637716770172
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0004125925925925926, 8.251851851851851e-05]
-------------------------------------------------
Done 256 Batches of 1085
class_loss = 0.4970226287841797
macro_f1 = 0.7798909555678992
weighted_f1 = 0.7804316357379911
regression_loss = 0.22742387652397156
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0004118518518518519, 8.237037037037037e-05]
-------------------------------------------------
Done 288 Batches of 1085
class_loss = 0.6971486210823059
macro_f1 = 0.7802152203605657
weighted_f1 = 0.7810561339146112
regression_loss = 0.2576579749584198
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0004111111111111111, 8.222222222222222e-05]
-------------------------------------------------
Done 320 Batches of 1085
class_loss = 0.6192569732666016
macro_f1 = 0.7801819983813559
weighted_f1 = 0.7810218149744542
regression_loss = 0.35324493050575256
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0004103703703703704, 8.207407407407408e-05]
-------------------------------------------------
Done 352 Batches of 1085
class_loss = 0.5285999178886414
macro_f1 = 0.7801880276851939
weighted_f1 = 0.7810587927762573
regression_loss = 0.24870651960372925
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00040962962962962963, 8.192592592592592e-05]
-------------------------------------------------
Done 384 Batches of 1085
class_loss = 0.5676461458206177
macro_f1 = 0.7806310856600649
weighted_f1 = 0.7816118918036087
regression_loss = 0.25205957889556885
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0004088888888888889, 8.177777777777778e-05]
-------------------------------------------------
Done 416 Batches of 1085
class_loss = 0.7436577081680298
macro_f1 = 0.7809676896769852
weighted_f1 = 0.7817293095920713
regression_loss = 0.35558629035949707
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00040814814814814815, 8.162962962962963e-05]
-------------------------------------------------
Done 448 Batches of 1085
class_loss = 0.7170699834823608
macro_f1 = 0.7813918141098569
weighted_f1 = 0.7821556666866213
regression_loss = 0.3205888867378235
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0004074074074074074, 8.148148148148148e-05]
-------------------------------------------------
Done 480 Batches of 1085
class_loss = 0.6035411953926086
macro_f1 = 0.7816149605115494
weighted_f1 = 0.7822394170447368
regression_loss = 0.24215158820152283
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00040666666666666667, 8.133333333333334e-05]
-------------------------------------------------
Done 512 Batches of 1085
class_loss = 0.8019724488258362
macro_f1 = 0.7816772195707096
weighted_f1 = 0.7823217810186499
regression_loss = 0.27524298429489136
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0004059259259259259, 8.118518518518518e-05]
-------------------------------------------------
Done 544 Batches of 1085
class_loss = 0.8006039261817932
macro_f1 = 0.7820441146129571
weighted_f1 = 0.7825504844894983
regression_loss = 0.2682095468044281
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00040518518518518524, 8.103703703703704e-05]
-------------------------------------------------
Done 576 Batches of 1085
class_loss = 0.631282389163971
macro_f1 = 0.7827267719378169
weighted_f1 = 0.7830748657931378
regression_loss = 0.2649213373661041
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00040444444444444447, 8.088888888888889e-05]
-------------------------------------------------
Done 608 Batches of 1085
class_loss = 0.5484524965286255
macro_f1 = 0.7830505373100948
weighted_f1 = 0.783363750079614
regression_loss = 0.22904914617538452
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00040370370370370375, 8.074074074074075e-05]
-------------------------------------------------
Done 640 Batches of 1085
class_loss = 0.6530815958976746
macro_f1 = 0.7829992947982413
weighted_f1 = 0.7832122159818162
regression_loss = 0.3296661972999573
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.000402962962962963, 8.05925925925926e-05]
-------------------------------------------------
Done 672 Batches of 1085
class_loss = 0.7111919522285461
macro_f1 = 0.7829614812459125
weighted_f1 = 0.7831758254180389
regression_loss = 0.22093376517295837
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0004022222222222222, 8.044444444444444e-05]
-------------------------------------------------
Done 704 Batches of 1085
class_loss = 0.994305431842804
macro_f1 = 0.7829742164562767
weighted_f1 = 0.7832655128669754
regression_loss = 0.20718848705291748
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0004014814814814815, 8.02962962962963e-05]
-------------------------------------------------
Done 736 Batches of 1085
class_loss = 0.38160163164138794
macro_f1 = 0.7825081092624304
weighted_f1 = 0.7827308381795925
regression_loss = 0.20616789162158966
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00040074074074074073, 8.014814814814815e-05]
-------------------------------------------------
Done 768 Batches of 1085
class_loss = 0.5907032489776611
macro_f1 = 0.7829036938141777
weighted_f1 = 0.7830930430262977
regression_loss = 0.30852028727531433
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0004, 8e-05]
-------------------------------------------------
Done 800 Batches of 1085
class_loss = 0.7238242030143738
macro_f1 = 0.7830713222145783
weighted_f1 = 0.7832151183445175
regression_loss = 0.2261001318693161
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00039925925925925925, 7.985185185185185e-05]
-------------------------------------------------
Done 832 Batches of 1085
class_loss = 0.71987384557724
macro_f1 = 0.7831567642138147
weighted_f1 = 0.7831890654313851
regression_loss = 0.25570181012153625
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0003985185185185185, 7.97037037037037e-05]
-------------------------------------------------
Done 864 Batches of 1085
class_loss = 0.762933075428009
macro_f1 = 0.7834420385686892
weighted_f1 = 0.7834368941034024
regression_loss = 0.30157241225242615
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00039777777777777777, 7.955555555555556e-05]
-------------------------------------------------
Done 896 Batches of 1085
class_loss = 0.6527383923530579
macro_f1 = 0.7837392659298505
weighted_f1 = 0.7837355068885313
regression_loss = 0.2756209373474121
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00039703703703703705, 7.94074074074074e-05]
-------------------------------------------------
Done 928 Batches of 1085
class_loss = 0.5435259342193604
macro_f1 = 0.7840616624395066
weighted_f1 = 0.7840186064894519
regression_loss = 0.2265407145023346
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00039629629629629634, 7.925925925925926e-05]
-------------------------------------------------
Done 960 Batches of 1085
class_loss = 0.7094109058380127
macro_f1 = 0.7842769658441552
weighted_f1 = 0.784234510627327
regression_loss = 0.22612424194812775
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00039555555555555557, 7.911111111111111e-05]
-------------------------------------------------
Done 992 Batches of 1085
class_loss = 0.8239840269088745
macro_f1 = 0.7842324147279179
weighted_f1 = 0.7841828098454475
regression_loss = 0.21387335658073425
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00039481481481481485, 7.896296296296297e-05]
-------------------------------------------------
Done 1024 Batches of 1085
class_loss = 0.4996660053730011
macro_f1 = 0.7845083266514985
weighted_f1 = 0.7844413142924263
regression_loss = 0.233753964304924
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0003940740740740741, 7.881481481481482e-05]
-------------------------------------------------
Done 1056 Batches of 1085
class_loss = 0.6508387923240662
macro_f1 = 0.7849994408365667
weighted_f1 = 0.784948930315632
regression_loss = 0.23313073813915253
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0003933333333333333, 7.866666666666666e-05]
-------------------------------------------------
Done 1085 Batches of 1085
class_loss = 0.43938738107681274
macro_f1 = 0.7851514332011457
weighted_f1 = 0.7851428327316576
regression_loss = 0.23107624053955078
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0003925925925925926, 7.851851851851852e-05]
1.0920400619506836
0.7742592692375183
0.8897891640663147
0.8109943866729736
0.8572983145713806
0.6755515933036804
0.8505958914756775
0.8184484839439392
0.8708670735359192
1.0051639080047607
0.8746377229690552
0.8368216753005981
0.9317485094070435
1.0386217832565308
0.8853179216384888
1.1123113632202148
0.8536559343338013
0.9782652258872986
1.1224273443222046
0.821342408657074
0.8674578070640564
0.9343077540397644
1.1022732257843018
0.8333974480628967
0.9386539459228516
0.9652196764945984
0.8084166646003723
0.8152334690093994
0.900692343711853
1.147560954093933
0.9450696706771851
0.9500321745872498
0.728097677230835
1.0188465118408203
0.9091358184814453
0.8883437514305115
0.8364656567573547
1.0444109439849854
0.8964309692382812
1.0069810152053833
0.5851696133613586
0.7802466750144958
0.7868254780769348
0.9623048901557922
1.1438419818878174
0.8109682202339172
0.9087480306625366
0.9340970516204834
0.8366502523422241
0.8790193796157837
0.886111855506897
0.7211893200874329
0.9203877449035645
0.8644132018089294
1.1158850193023682
0.7284054756164551
0.9774671196937561
0.6893944144248962
0.7213371992111206
0.6900963187217712
1.0485917329788208
0.890882134437561
0.8864677548408508
1.000335454940796
Epoch 4, training metrics:
--- CLASSIFICATION METRICS --- 
F1 scores: [Macro=0.785, Weighted=0.785] 
Balanced Accuracy = 0.787
Average loss = 0.70585
Epoch 4, validation metrics:
--- CLASSIFICATION METRICS --- 
F1 scores: [Macro=0.737, Weighted=0.737] 
Balanced Accuracy = 0.739
Average loss = 0.89697
avg_train_loss 0.7058475518006883
avg_val_loss 0.8969690911471844
-------------------------------------------------
Done 32 Batches of 1085
class_loss = 0.5314395427703857
macro_f1 = 0.8417612542917574
weighted_f1 = 0.8415306119820006
regression_loss = 0.25841400027275085
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00039185185185185183, 7.837037037037037e-05]
-------------------------------------------------
Done 64 Batches of 1085
class_loss = 0.43016037344932556
macro_f1 = 0.8383523047966095
weighted_f1 = 0.8383506360108812
regression_loss = 0.2916768193244934
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0003911111111111111, 7.822222222222223e-05]
-------------------------------------------------
Done 96 Batches of 1085
class_loss = 0.5574761033058167
macro_f1 = 0.8388244785693164
weighted_f1 = 0.839094118605782
regression_loss = 0.29711082577705383
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00039037037037037035, 7.807407407407408e-05]
-------------------------------------------------
Done 128 Batches of 1085
class_loss = 0.44494393467903137
macro_f1 = 0.8366665576247735
weighted_f1 = 0.8367516113469032
regression_loss = 0.22475725412368774
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00038962962962962964, 7.792592592592592e-05]
-------------------------------------------------
Done 160 Batches of 1085
class_loss = 0.4181279242038727
macro_f1 = 0.8355605245182696
weighted_f1 = 0.835258881762368
regression_loss = 0.2066054493188858
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0003888888888888889, 7.777777777777778e-05]
-------------------------------------------------
Done 192 Batches of 1085
class_loss = 0.537634551525116
macro_f1 = 0.8373155654995935
weighted_f1 = 0.8367921758383312
regression_loss = 0.2915700078010559
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00038814814814814815, 7.762962962962963e-05]
-------------------------------------------------
Done 224 Batches of 1085
class_loss = 0.4981290102005005
macro_f1 = 0.8360740005435924
weighted_f1 = 0.8357716930150154
regression_loss = 0.19847963750362396
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00038740740740740744, 7.748148148148149e-05]
-------------------------------------------------
Done 256 Batches of 1085
class_loss = 0.45361173152923584
macro_f1 = 0.834948536082018
weighted_f1 = 0.8346882393631032
regression_loss = 0.17012274265289307
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00038666666666666667, 7.733333333333333e-05]
-------------------------------------------------
Done 288 Batches of 1085
class_loss = 0.41713038086891174
macro_f1 = 0.8358348319385209
weighted_f1 = 0.8354413163579347
regression_loss = 0.2534531354904175
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00038592592592592595, 7.71851851851852e-05]
-------------------------------------------------
Done 320 Batches of 1085
class_loss = 0.4540136754512787
macro_f1 = 0.8367244909773444
weighted_f1 = 0.8362189604997334
regression_loss = 0.2483317255973816
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0003851851851851852, 7.703703703703704e-05]
-------------------------------------------------
Done 352 Batches of 1085
class_loss = 0.4771578907966614
macro_f1 = 0.836565277266416
weighted_f1 = 0.8361352598272536
regression_loss = 0.201107457280159
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0003844444444444444, 7.688888888888889e-05]
-------------------------------------------------
Done 384 Batches of 1085
class_loss = 0.40623801946640015
macro_f1 = 0.8370714078003312
weighted_f1 = 0.8365823759417245
regression_loss = 0.2428882122039795
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0003837037037037037, 7.674074074074075e-05]
-------------------------------------------------
Done 416 Batches of 1085
class_loss = 0.4353649914264679
macro_f1 = 0.8371478006797414
weighted_f1 = 0.8367729658453377
regression_loss = 0.26717549562454224
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00038296296296296293, 7.659259259259259e-05]
-------------------------------------------------
Done 448 Batches of 1085
class_loss = 0.6139481663703918
macro_f1 = 0.8373528645893094
weighted_f1 = 0.8369458332523866
regression_loss = 0.28752219676971436
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0003822222222222223, 7.644444444444445e-05]
-------------------------------------------------
Done 480 Batches of 1085
class_loss = 0.5912824869155884
macro_f1 = 0.8374725151765476
weighted_f1 = 0.8370230566523487
regression_loss = 0.29571592807769775
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0003814814814814815, 7.62962962962963e-05]
-------------------------------------------------
Done 512 Batches of 1085
class_loss = 0.50038743019104
macro_f1 = 0.8372546209806758
weighted_f1 = 0.8369080615647366
regression_loss = 0.29194071888923645
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0003807407407407408, 7.614814814814816e-05]
-------------------------------------------------
Done 544 Batches of 1085
class_loss = 0.4615999460220337
macro_f1 = 0.8369016802113733
weighted_f1 = 0.8365633008461156
regression_loss = 0.25030407309532166
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00038, 7.6e-05]
-------------------------------------------------
Done 576 Batches of 1085
class_loss = 0.47318023443222046
macro_f1 = 0.8374623469316494
weighted_f1 = 0.8370372044708244
regression_loss = 0.20406612753868103
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00037925925925925925, 7.585185185185185e-05]
-------------------------------------------------
Done 608 Batches of 1085
class_loss = 0.37929096817970276
macro_f1 = 0.8375307096989888
weighted_f1 = 0.8371506448720082
regression_loss = 0.2549346089363098
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00037851851851851854, 7.570370370370371e-05]
-------------------------------------------------
Done 640 Batches of 1085
class_loss = 0.38557782769203186
macro_f1 = 0.837898746314052
weighted_f1 = 0.8376450257896353
regression_loss = 0.20298537611961365
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00037777777777777777, 7.555555555555556e-05]
-------------------------------------------------
Done 672 Batches of 1085
class_loss = 0.42106616497039795
macro_f1 = 0.8380830219986303
weighted_f1 = 0.837852858007624
regression_loss = 0.3095647096633911
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00037703703703703705, 7.540740740740742e-05]
-------------------------------------------------
Done 704 Batches of 1085
class_loss = 0.6451963782310486
macro_f1 = 0.8383399252731385
weighted_f1 = 0.838135704599043
regression_loss = 0.25260064005851746
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0003762962962962963, 7.525925925925926e-05]
-------------------------------------------------
Done 736 Batches of 1085
class_loss = 0.28046900033950806
macro_f1 = 0.8374328498764576
weighted_f1 = 0.8372969653025653
regression_loss = 0.24817949533462524
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0003755555555555555, 7.511111111111111e-05]
-------------------------------------------------
Done 768 Batches of 1085
class_loss = 0.6425372362136841
macro_f1 = 0.8366730236570848
weighted_f1 = 0.8365604953864582
regression_loss = 0.2457062005996704
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00037481481481481486, 7.496296296296297e-05]
-------------------------------------------------
Done 800 Batches of 1085
class_loss = 0.5542692542076111
macro_f1 = 0.8363559398042151
weighted_f1 = 0.836263311628391
regression_loss = 0.2688077390193939
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0003740740740740741, 7.481481481481481e-05]
-------------------------------------------------
Done 832 Batches of 1085
class_loss = 0.6400410532951355
macro_f1 = 0.8362712842901315
weighted_f1 = 0.8362054941460403
regression_loss = 0.25183963775634766
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0003733333333333334, 7.466666666666667e-05]
-------------------------------------------------
Done 864 Batches of 1085
class_loss = 0.5527640581130981
macro_f1 = 0.836071639870419
weighted_f1 = 0.8359876665147739
regression_loss = 0.2128485143184662
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0003725925925925926, 7.451851851851852e-05]
-------------------------------------------------
Done 896 Batches of 1085
class_loss = 0.5128714442253113
macro_f1 = 0.835812609005442
weighted_f1 = 0.8357285520452871
regression_loss = 0.19821619987487793
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0003718518518518519, 7.437037037037038e-05]
-------------------------------------------------
Done 928 Batches of 1085
class_loss = 0.6300588846206665
macro_f1 = 0.8357457289044439
weighted_f1 = 0.8357334658526213
regression_loss = 0.2336132526397705
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0003711111111111111, 7.422222222222223e-05]
-------------------------------------------------
Done 960 Batches of 1085
class_loss = 0.4685335159301758
macro_f1 = 0.8354021195075205
weighted_f1 = 0.8354525864540483
regression_loss = 0.2883489727973938
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00037037037037037035, 7.407407407407407e-05]
-------------------------------------------------
Done 992 Batches of 1085
class_loss = 0.5332067012786865
macro_f1 = 0.8355675215435857
weighted_f1 = 0.8355896449118995
regression_loss = 0.2557522654533386
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00036962962962962964, 7.392592592592593e-05]
-------------------------------------------------
Done 1024 Batches of 1085
class_loss = 0.5650960206985474
macro_f1 = 0.8357790031906983
weighted_f1 = 0.8357468807024011
regression_loss = 0.25945812463760376
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00036888888888888887, 7.377777777777778e-05]
-------------------------------------------------
Done 1056 Batches of 1085
class_loss = 0.47649842500686646
macro_f1 = 0.8357665053235127
weighted_f1 = 0.8357675984534143
regression_loss = 0.2286538928747177
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00036814814814814815, 7.362962962962964e-05]
-------------------------------------------------
Done 1085 Batches of 1085
class_loss = 0.6247967481613159
macro_f1 = 0.8355875555613357
weighted_f1 = 0.8355781009299489
regression_loss = 0.25700584053993225
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00036740740740740744, 7.348148148148149e-05]
1.1954597234725952
0.7768076062202454
1.0249125957489014
0.8326408863067627
0.8507128953933716
0.7093657851219177
0.9246594905853271
0.845737099647522
0.8416230082511902
0.9782273769378662
0.9193307161331177
0.8742991089820862
0.9533100128173828
1.0293970108032227
0.8811758160591125
1.0150811672210693
0.8930059671401978
1.1434965133666992
0.9643088579177856
0.7277175784111023
0.8672801852226257
0.8599464297294617
1.000874638557434
0.8130478858947754
0.9503105878829956
0.969367265701294
0.9827232360839844
0.7568612098693848
0.8960911631584167
1.1014808416366577
0.9766203761100769
0.8183863759040833
0.7217469215393066
1.0060771703720093
0.9023478627204895
0.9376325011253357
0.8620474338531494
1.1152360439300537
0.8740341067314148
1.115012526512146
0.6079920530319214
0.8728974461555481
0.7978663444519043
0.9415802359580994
1.1733486652374268
0.8416679501533508
0.8808771967887878
0.9367404580116272
0.9543409943580627
0.9283199906349182
0.8481428623199463
0.711651086807251
0.9615218639373779
0.9901922941207886
1.1558648347854614
0.7999719977378845
1.0828148126602173
0.6567724943161011
0.7680984139442444
0.7110119462013245
1.1347981691360474
0.9404700398445129
0.93401700258255
1.00265634059906
Epoch 5, training metrics:
--- CLASSIFICATION METRICS --- 
F1 scores: [Macro=0.836, Weighted=0.836] 
Balanced Accuracy = 0.837
Average loss = 0.55014
Epoch 5, validation metrics:
--- CLASSIFICATION METRICS --- 
F1 scores: [Macro=0.742, Weighted=0.742] 
Balanced Accuracy = 0.743
Average loss = 0.91472
avg_train_loss 0.5501446544849378
avg_val_loss 0.9147188980132341
-------------------------------------------------
Done 32 Batches of 1085
class_loss = 0.27160197496414185
macro_f1 = 0.8827558945238552
weighted_f1 = 0.8827390782047191
regression_loss = 0.21478848159313202
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00036666666666666667, 7.333333333333333e-05]
-------------------------------------------------
Done 64 Batches of 1085
class_loss = 0.3611234724521637
macro_f1 = 0.8890675184689849
weighted_f1 = 0.8891952766280138
regression_loss = 0.18662911653518677
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00036592592592592596, 7.318518518518519e-05]
-------------------------------------------------
Done 96 Batches of 1085
class_loss = 0.35988926887512207
macro_f1 = 0.8905548314684186
weighted_f1 = 0.8907664251187064
regression_loss = 0.32354480028152466
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0003651851851851852, 7.303703703703704e-05]
-------------------------------------------------
Done 128 Batches of 1085
class_loss = 0.3516986072063446
macro_f1 = 0.8916495130891662
weighted_f1 = 0.8918556638294217
regression_loss = 0.18275345861911774
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00036444444444444447, 7.28888888888889e-05]
-------------------------------------------------
Done 160 Batches of 1085
class_loss = 0.3568550646305084
macro_f1 = 0.8896375910496259
weighted_f1 = 0.8898417879787606
regression_loss = 0.2918092608451843
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0003637037037037037, 7.274074074074074e-05]
-------------------------------------------------
Done 192 Batches of 1085
class_loss = 0.4203301668167114
macro_f1 = 0.8897148992948767
weighted_f1 = 0.8898336118235778
regression_loss = 0.23179565370082855
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.000362962962962963, 7.25925925925926e-05]
-------------------------------------------------
Done 224 Batches of 1085
class_loss = 0.280552476644516
macro_f1 = 0.8876653765902246
weighted_f1 = 0.8876605238167686
regression_loss = 0.30778276920318604
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0003622222222222222, 7.244444444444445e-05]
-------------------------------------------------
Done 256 Batches of 1085
class_loss = 0.28805553913116455
macro_f1 = 0.8875153305112967
weighted_f1 = 0.8875533559572715
regression_loss = 0.2050052285194397
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00036148148148148145, 7.22962962962963e-05]
-------------------------------------------------
Done 288 Batches of 1085
class_loss = 0.45920437574386597
macro_f1 = 0.8869082225233746
weighted_f1 = 0.8870268611732779
regression_loss = 0.2204672247171402
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00036074074074074074, 7.214814814814816e-05]
-------------------------------------------------
Done 320 Batches of 1085
class_loss = 0.3723774254322052
macro_f1 = 0.8857146605915688
weighted_f1 = 0.8857112426651836
regression_loss = 0.2737494707107544
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00035999999999999997, 7.2e-05]
-------------------------------------------------
Done 352 Batches of 1085
class_loss = 0.27499839663505554
macro_f1 = 0.8862990190647191
weighted_f1 = 0.8861354052880046
regression_loss = 0.2334803193807602
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0003592592592592593, 7.185185185185186e-05]
-------------------------------------------------
Done 384 Batches of 1085
class_loss = 0.19127771258354187
macro_f1 = 0.886500490271137
weighted_f1 = 0.886350622953326
regression_loss = 0.21368446946144104
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00035851851851851854, 7.170370370370371e-05]
-------------------------------------------------
Done 416 Batches of 1085
class_loss = 0.34425246715545654
macro_f1 = 0.8866144308908843
weighted_f1 = 0.8865824672294325
regression_loss = 0.21335570514202118
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00035777777777777777, 7.155555555555555e-05]
-------------------------------------------------
Done 448 Batches of 1085
class_loss = 0.3187214434146881
macro_f1 = 0.8868774508143963
weighted_f1 = 0.886776675161294
regression_loss = 0.24774931371212006
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00035703703703703706, 7.140740740740741e-05]
-------------------------------------------------
Done 480 Batches of 1085
class_loss = 0.3407362997531891
macro_f1 = 0.8869476785592445
weighted_f1 = 0.8869445072317781
regression_loss = 0.2686476707458496
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0003562962962962963, 7.125925925925926e-05]
-------------------------------------------------
Done 512 Batches of 1085
class_loss = 0.3546777069568634
macro_f1 = 0.88693333616611
weighted_f1 = 0.8869554101907638
regression_loss = 0.2152702510356903
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00035555555555555557, 7.111111111111112e-05]
-------------------------------------------------
Done 544 Batches of 1085
class_loss = 0.2528230845928192
macro_f1 = 0.8865348635183447
weighted_f1 = 0.8865413037447595
regression_loss = 0.1853896826505661
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0003548148148148148, 7.096296296296297e-05]
-------------------------------------------------
Done 576 Batches of 1085
class_loss = 0.4197552800178528
macro_f1 = 0.8868862431485383
weighted_f1 = 0.8869393038700534
regression_loss = 0.2340591549873352
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0003540740740740741, 7.081481481481483e-05]
-------------------------------------------------
Done 608 Batches of 1085
class_loss = 0.5102952718734741
macro_f1 = 0.8868430472585163
weighted_f1 = 0.8868347228814816
regression_loss = 0.22326701879501343
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0003533333333333333, 7.066666666666667e-05]
-------------------------------------------------
Done 640 Batches of 1085
class_loss = 0.366027295589447
macro_f1 = 0.8866464271573639
weighted_f1 = 0.886655318381182
regression_loss = 0.3428259491920471
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00035259259259259255, 7.051851851851852e-05]
-------------------------------------------------
Done 672 Batches of 1085
class_loss = 0.39220133423805237
macro_f1 = 0.8866556312500394
weighted_f1 = 0.8867032112681074
regression_loss = 0.2591283321380615
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0003518518518518519, 7.037037037037038e-05]
-------------------------------------------------
Done 704 Batches of 1085
class_loss = 0.22916874289512634
macro_f1 = 0.8866789806752742
weighted_f1 = 0.8866953147112634
regression_loss = 0.24881725013256073
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0003511111111111111, 7.022222222222222e-05]
-------------------------------------------------
Done 736 Batches of 1085
class_loss = 0.42453470826148987
macro_f1 = 0.8865756479188074
weighted_f1 = 0.8866097143600813
regression_loss = 0.24284623563289642
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0003503703703703704, 7.007407407407408e-05]
-------------------------------------------------
Done 768 Batches of 1085
class_loss = 0.4865509569644928
macro_f1 = 0.8863162618281019
weighted_f1 = 0.8863871684862188
regression_loss = 0.27752721309661865
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00034962962962962964, 6.992592592592593e-05]
-------------------------------------------------
Done 800 Batches of 1085
class_loss = 0.323783278465271
macro_f1 = 0.8860457436808247
weighted_f1 = 0.8860396560755691
regression_loss = 0.21423093974590302
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0003488888888888889, 6.977777777777779e-05]
-------------------------------------------------
Done 832 Batches of 1085
class_loss = 0.4693957269191742
macro_f1 = 0.8861838024030101
weighted_f1 = 0.8862213437402393
regression_loss = 0.18793842196464539
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00034814814814814816, 6.962962962962964e-05]
-------------------------------------------------
Done 864 Batches of 1085
class_loss = 0.2932566702365875
macro_f1 = 0.8861728204805172
weighted_f1 = 0.8861657206375224
regression_loss = 0.2878388464450836
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0003474074074074074, 6.948148148148148e-05]
-------------------------------------------------
Done 896 Batches of 1085
class_loss = 0.28723013401031494
macro_f1 = 0.8862403246803122
weighted_f1 = 0.8862399879141843
regression_loss = 0.26628607511520386
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00034666666666666667, 6.933333333333334e-05]
-------------------------------------------------
Done 928 Batches of 1085
class_loss = 0.3568597733974457
macro_f1 = 0.8863990111976057
weighted_f1 = 0.8863656194356917
regression_loss = 0.1869698166847229
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0003459259259259259, 6.918518518518519e-05]
-------------------------------------------------
Done 960 Batches of 1085
class_loss = 0.49912673234939575
macro_f1 = 0.8863877313745891
weighted_f1 = 0.8863536422546838
regression_loss = 0.25779712200164795
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0003451851851851852, 6.903703703703705e-05]
-------------------------------------------------
Done 992 Batches of 1085
class_loss = 0.3742855489253998
macro_f1 = 0.8860787933740877
weighted_f1 = 0.8860536453707245
regression_loss = 0.18819765746593475
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0003444444444444445, 6.88888888888889e-05]
-------------------------------------------------
Done 1024 Batches of 1085
class_loss = 0.3106314241886139
macro_f1 = 0.8857417988821895
weighted_f1 = 0.8856969257909115
regression_loss = 0.21314075589179993
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0003437037037037037, 6.874074074074074e-05]
-------------------------------------------------
Done 1056 Batches of 1085
class_loss = 0.3207870423793793
macro_f1 = 0.8854773970791449
weighted_f1 = 0.8854522032024985
regression_loss = 0.2506612539291382
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.000342962962962963, 6.85925925925926e-05]
-------------------------------------------------
Done 1085 Batches of 1085
class_loss = 0.20987533032894135
macro_f1 = 0.8852471873090794
weighted_f1 = 0.8852387320950518
regression_loss = 0.3822999596595764
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0003422222222222222, 6.844444444444445e-05]
1.1160194873809814
0.8385964035987854
0.8703000545501709
0.8850582838058472
0.9326909184455872
0.6491066217422485
0.8633815050125122
0.8706268072128296
0.8082013726234436
1.0587118864059448
0.8786006569862366
0.8643850088119507
0.9874182939529419
1.1908503770828247
0.9878188967704773
1.1214017868041992
0.9352633953094482
1.1416661739349365
1.0736340284347534
0.846603274345398
0.8982122540473938
0.9328980445861816
1.123610258102417
0.794277548789978
0.9721898436546326
1.065718412399292
0.9417271018028259
0.9961239099502563
0.9672904014587402
1.0279154777526855
1.126853108406067
0.9702848792076111
0.7521495223045349
1.1465590000152588
0.9676268696784973
1.0961425304412842
0.9862892031669617
1.17276132106781
0.9880895614624023
1.1209211349487305
0.6699089407920837
0.8586944341659546
0.8436753749847412
1.0691472291946411
1.2755866050720215
0.8897596597671509
0.9685357213020325
1.0275934934616089
1.0667102336883545
1.0280592441558838
0.9194142818450928
0.726131021976471
1.0424638986587524
0.995944619178772
1.2599377632141113
0.7225098013877869
1.1628568172454834
0.7166552543640137
0.7840332388877869
0.7687565088272095
1.2579395771026611
1.0636966228485107
1.0596734285354614
1.2539687156677246
Epoch 6, training metrics:
--- CLASSIFICATION METRICS --- 
F1 scores: [Macro=0.885, Weighted=0.885] 
Balanced Accuracy = 0.886
Average loss = 0.39848
Epoch 6, validation metrics:
--- CLASSIFICATION METRICS --- 
F1 scores: [Macro=0.746, Weighted=0.746] 
Balanced Accuracy = 0.747
Average loss = 0.97499
avg_train_loss 0.3984845075052455
avg_val_loss 0.9749941891059279
-------------------------------------------------
Done 32 Batches of 1085
class_loss = 0.24872933328151703
macro_f1 = 0.9153521628031553
weighted_f1 = 0.9143656638504087
regression_loss = 0.3150317072868347
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0003414814814814815, 6.829629629629631e-05]
-------------------------------------------------
Done 64 Batches of 1085
class_loss = 0.21135792136192322
macro_f1 = 0.9216524855045266
weighted_f1 = 0.9210325821705164
regression_loss = 0.25978970527648926
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00034074074074074074, 6.814814814814815e-05]
-------------------------------------------------
Done 96 Batches of 1085
class_loss = 0.23467297852039337
macro_f1 = 0.9267057779589571
weighted_f1 = 0.9268373974573604
regression_loss = 0.20167234539985657
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00034, 6.800000000000001e-05]
-------------------------------------------------
Done 128 Batches of 1085
class_loss = 0.21307942271232605
macro_f1 = 0.9277960599270694
weighted_f1 = 0.9278753449335175
regression_loss = 0.19911450147628784
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00033925925925925926, 6.785185185185186e-05]
-------------------------------------------------
Done 160 Batches of 1085
class_loss = 0.27449366450309753
macro_f1 = 0.9283850884327508
weighted_f1 = 0.9284811261443751
regression_loss = 0.22850793600082397
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0003385185185185185, 6.77037037037037e-05]
-------------------------------------------------
Done 192 Batches of 1085
class_loss = 0.21577188372612
macro_f1 = 0.9281764062007216
weighted_f1 = 0.9282647426001752
regression_loss = 0.22186611592769623
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00033777777777777777, 6.755555555555557e-05]
-------------------------------------------------
Done 224 Batches of 1085
class_loss = 0.19768324494361877
macro_f1 = 0.9279048772425517
weighted_f1 = 0.9280650929577405
regression_loss = 0.25068211555480957
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00033703703703703706, 6.740740740740741e-05]
-------------------------------------------------
Done 256 Batches of 1085
class_loss = 0.3445621132850647
macro_f1 = 0.928155550345196
weighted_f1 = 0.928151364119643
regression_loss = 0.25042909383773804
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00033629629629629634, 6.725925925925927e-05]
-------------------------------------------------
Done 288 Batches of 1085
class_loss = 0.23683445155620575
macro_f1 = 0.9271826848167464
weighted_f1 = 0.9272090880500675
regression_loss = 0.1746155470609665
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0003355555555555556, 6.711111111111112e-05]
-------------------------------------------------
Done 320 Batches of 1085
class_loss = 0.18706373870372772
macro_f1 = 0.9278615017599773
weighted_f1 = 0.9278733830475215
regression_loss = 0.21920844912528992
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0003348148148148148, 6.696296296296296e-05]
-------------------------------------------------
Done 352 Batches of 1085
class_loss = 0.2839144468307495
macro_f1 = 0.9274014035618923
weighted_f1 = 0.9273871681614839
regression_loss = 0.2740176320075989
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0003340740740740741, 6.681481481481482e-05]
-------------------------------------------------
Done 384 Batches of 1085
class_loss = 0.15498259663581848
macro_f1 = 0.9272925570581162
weighted_f1 = 0.9273082920302504
regression_loss = 0.2530258297920227
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0003333333333333333, 6.666666666666667e-05]
-------------------------------------------------
Done 416 Batches of 1085
class_loss = 0.21274417638778687
macro_f1 = 0.9281131054424353
weighted_f1 = 0.9280793624741804
regression_loss = 0.19447389245033264
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0003325925925925926, 6.651851851851853e-05]
-------------------------------------------------
Done 448 Batches of 1085
class_loss = 0.19494079053401947
macro_f1 = 0.9283376815737137
weighted_f1 = 0.9283709647253274
regression_loss = 0.20627373456954956
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00033185185185185184, 6.637037037037038e-05]
-------------------------------------------------
Done 480 Batches of 1085
class_loss = 0.3284577429294586
macro_f1 = 0.9284190098476321
weighted_f1 = 0.9284803370492568
regression_loss = 0.2923057973384857
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0003311111111111111, 6.622222222222224e-05]
-------------------------------------------------
Done 512 Batches of 1085
class_loss = 0.16681551933288574
macro_f1 = 0.9277905249069954
weighted_f1 = 0.9278032612437513
regression_loss = 0.21346500515937805
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00033037037037037036, 6.607407407407408e-05]
-------------------------------------------------
Done 544 Batches of 1085
class_loss = 0.3228733539581299
macro_f1 = 0.9276295014635263
weighted_f1 = 0.9275956640602918
regression_loss = 0.1944827437400818
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0003296296296296296, 6.592592592592593e-05]
-------------------------------------------------
Done 576 Batches of 1085
class_loss = 0.2123781442642212
macro_f1 = 0.9273405160321693
weighted_f1 = 0.9272890553034491
regression_loss = 0.21665693819522858
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0003288888888888889, 6.577777777777779e-05]
-------------------------------------------------
Done 608 Batches of 1085
class_loss = 0.23190872371196747
macro_f1 = 0.9270194729534614
weighted_f1 = 0.9269785583613686
regression_loss = 0.25511765480041504
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00032814814814814816, 6.562962962962963e-05]
-------------------------------------------------
Done 640 Batches of 1085
class_loss = 0.2664467394351959
macro_f1 = 0.9266857481616155
weighted_f1 = 0.9266312910554332
regression_loss = 0.2271149605512619
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00032740740740740744, 6.54814814814815e-05]
-------------------------------------------------
Done 672 Batches of 1085
class_loss = 0.30206096172332764
macro_f1 = 0.9267393223840903
weighted_f1 = 0.9266994947436266
regression_loss = 0.25989222526550293
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0003266666666666667, 6.533333333333334e-05]
-------------------------------------------------
Done 704 Batches of 1085
class_loss = 0.31201493740081787
macro_f1 = 0.9257244285947264
weighted_f1 = 0.9257216786024518
regression_loss = 0.23489424586296082
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00032592592592592596, 6.51851851851852e-05]
-------------------------------------------------
Done 736 Batches of 1085
class_loss = 0.18756651878356934
macro_f1 = 0.925389605211295
weighted_f1 = 0.9254079228480436
regression_loss = 0.14617890119552612
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0003251851851851852, 6.503703703703705e-05]
-------------------------------------------------
Done 768 Batches of 1085
class_loss = 0.1662009358406067
macro_f1 = 0.9249797338896457
weighted_f1 = 0.924994746163267
regression_loss = 0.2366931438446045
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0003244444444444444, 6.488888888888889e-05]
-------------------------------------------------
Done 800 Batches of 1085
class_loss = 0.23760266602039337
macro_f1 = 0.9246123972568837
weighted_f1 = 0.9246027608628895
regression_loss = 0.2842964828014374
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0003237037037037037, 6.474074074074075e-05]
-------------------------------------------------
Done 832 Batches of 1085
class_loss = 0.1995813548564911
macro_f1 = 0.9243372776186508
weighted_f1 = 0.9243134320098442
regression_loss = 0.2054603397846222
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00032296296296296294, 6.45925925925926e-05]
-------------------------------------------------
Done 864 Batches of 1085
class_loss = 0.11863145232200623
macro_f1 = 0.923880859119586
weighted_f1 = 0.9238698221294905
regression_loss = 0.19028693437576294
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0003222222222222222, 6.444444444444446e-05]
-------------------------------------------------
Done 896 Batches of 1085
class_loss = 0.2487809956073761
macro_f1 = 0.9240027030679101
weighted_f1 = 0.923981999888705
regression_loss = 0.23346632719039917
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0003214814814814815, 6.42962962962963e-05]
-------------------------------------------------
Done 928 Batches of 1085
class_loss = 0.2662787139415741
macro_f1 = 0.9234534794376215
weighted_f1 = 0.9234496069101323
regression_loss = 0.2746489346027374
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00032074074074074074, 6.414814814814815e-05]
-------------------------------------------------
Done 960 Batches of 1085
class_loss = 0.3365910053253174
macro_f1 = 0.9233820041088672
weighted_f1 = 0.9233994400928183
regression_loss = 0.2520749866962433
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00032, 6.400000000000001e-05]
-------------------------------------------------
Done 992 Batches of 1085
class_loss = 0.2426951676607132
macro_f1 = 0.9233722190474815
weighted_f1 = 0.9233574923286081
regression_loss = 0.23271846771240234
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00031925925925925926, 6.385185185185186e-05]
-------------------------------------------------
Done 1024 Batches of 1085
class_loss = 0.2838144898414612
macro_f1 = 0.923290267030851
weighted_f1 = 0.9232763792772631
regression_loss = 0.25374239683151245
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00031851851851851854, 6.37037037037037e-05]
-------------------------------------------------
Done 1056 Batches of 1085
class_loss = 0.19009900093078613
macro_f1 = 0.9233781196740102
weighted_f1 = 0.9233795474582961
regression_loss = 0.2662919759750366
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0003177777777777778, 6.355555555555556e-05]
-------------------------------------------------
Done 1085 Batches of 1085
class_loss = 0.16059429943561554
macro_f1 = 0.9231925925100443
weighted_f1 = 0.9231903053935243
regression_loss = 0.3154963552951813
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00031703703703703706, 6.340740740740741e-05]
1.4425194263458252
0.8844689726829529
0.9049405455589294
0.9800283908843994
0.8868681192398071
0.6695681810379028
0.993625283241272
0.9086298942565918
1.0041077136993408
1.2318222522735596
0.9486644268035889
1.1480218172073364
1.2553026676177979
1.1797261238098145
1.0723437070846558
1.1538820266723633
0.9746219515800476
1.2447625398635864
1.1061291694641113
0.9146350622177124
0.9164747595787048
0.9436764717102051
1.2064859867095947
0.9807406663894653
1.0187351703643799
1.1189661026000977
1.0708792209625244
0.883482038974762
1.1076157093048096
1.2147208452224731
1.347419023513794
0.9060897827148438
0.7320432066917419
1.206274151802063
0.9314509630203247
1.1705390214920044
0.9834951758384705
1.243992567062378
1.1225557327270508
1.3699445724487305
0.6834017038345337
0.9571095108985901
0.9894472360610962
1.1096938848495483
1.3966364860534668
0.8641045689582825
1.1549150943756104
1.030127763748169
1.160294771194458
1.1387526988983154
0.9895203113555908
0.7733283638954163
1.1233774423599243
1.0378636121749878
1.4730080366134644
0.8943849205970764
1.1825225353240967
0.6797271966934204
0.7580268383026123
0.9158278703689575
1.3573726415634155
1.0737711191177368
1.126013994216919
1.3926489353179932
Epoch 7, training metrics:
--- CLASSIFICATION METRICS --- 
F1 scores: [Macro=0.923, Weighted=0.923] 
Balanced Accuracy = 0.923
Average loss = 0.28192
Epoch 7, validation metrics:
--- CLASSIFICATION METRICS --- 
F1 scores: [Macro=0.748, Weighted=0.748] 
Balanced Accuracy = 0.747
Average loss = 1.05691
avg_train_loss 0.28191790588989785
avg_val_loss 1.0569082340225577
slurmstepd: error: *** JOB 15131543 ON nid005098 CANCELLED AT 2025-12-02T23:01:55 ***
