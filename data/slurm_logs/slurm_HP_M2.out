/pfs/lustrep2/projappl/project_465002413/ChessGPT/chessgpt_multitask.py:33: DtypeWarning: Columns (3,5) have mixed types. Specify dtype option on import or set low_memory=False.
  df = pd.read_csv(csv_path)
/pfs/lustrep2/projappl/project_465002413/ChessGPT/chessgpt_multitask.py:80: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.
The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.

For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.


  df_white_black['white_elo'].fillna(df_white_black['white_elo'].mean(), inplace=True)
/pfs/lustrep2/projappl/project_465002413/ChessGPT/chessgpt_multitask.py:81: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.
The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.

For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.


  df_white_black['black_elo'].fillna(df_white_black['black_elo'].mean(), inplace=True)
Total: 20,853
Train: 17,725 (85.00%)
Val:   1,042 (5.00%)
Test:  2,086 (10.00%)
|-|-|-|-|-|mean_elo: 2663.78369140625|-|-|-|-|
|-|-|-|-|-|std_elo: 110.49032592773438|-|-|-|-|
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.34s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  2.83s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.35s/it]
HYPERPARAM: 
 shared_dim1=512, 
 shared_dim2=256, 
 head_dim=128, 
 dropout=0.1
HYPERPARAM: tokenizer, max_length=256
139
Unfroze last transformer layer.
HYPERPARAM: lrs: 5e-4  &  1e-4
HYPERPARAM: weight_decays: 0.00001  &  0.00001
Using device: cuda
HYPERPARAM: alpha: 0.2
Done 32 Batches of 139
class_loss = 2.9974782466888428
macro_f1 = 0.017737160811725564
weighted_f1 = 0.01745548377328625
regression_loss = 0.33020979166030884
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [6.25e-05, 1.25e-05]
Done 64 Batches of 139
class_loss = 2.9994382858276367
macro_f1 = 0.017894757627987767
weighted_f1 = 0.018163559034619157
regression_loss = 0.3587549328804016
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.000125, 2.5e-05]
Done 96 Batches of 139
class_loss = 2.9957435131073
macro_f1 = 0.020286875751082907
weighted_f1 = 0.020382731908110595
regression_loss = 0.3499077558517456
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0001875, 3.7500000000000003e-05]
Done 128 Batches of 139
class_loss = 2.990490674972534
macro_f1 = 0.026730355228521817
weighted_f1 = 0.0267967713257871
regression_loss = 0.2753947675228119
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00025, 5e-05]
Done 139 Batches of 139
class_loss = 2.995352268218994
macro_f1 = 0.029034727251279934
weighted_f1 = 0.02904920109424416
regression_loss = 0.33174148201942444
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0003125, 6.25e-05]
3.0698719024658203
3.066636323928833
3.0696048736572266
3.048578977584839
3.060631513595581
3.0444483757019043
3.0480828285217285
3.054702043533325
3.051995277404785
avg_train_loss 3.0676437744991385
avg_val_loss 3.0571724573771157
Done 32 Batches of 139
class_loss = 2.9988200664520264
macro_f1 = 0.031009866768643913
weighted_f1 = 0.03151552314274451
regression_loss = 0.37993091344833374
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.000375, 7.500000000000001e-05]
Done 64 Batches of 139
class_loss = 2.969942569732666
macro_f1 = 0.05451635893214294
weighted_f1 = 0.05524053191030979
regression_loss = 0.37634366750717163
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0004375, 8.75e-05]
Done 96 Batches of 139
class_loss = 2.9556844234466553
macro_f1 = 0.06784491350446165
weighted_f1 = 0.06845399925019816
regression_loss = 0.45018213987350464
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0005, 0.0001]
Done 128 Batches of 139
class_loss = 2.8953685760498047
macro_f1 = 0.08290410543695305
weighted_f1 = 0.08293319335490773
regression_loss = 0.4157538414001465
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0004997993579454254, 9.995987158908508e-05]
Done 139 Batches of 139
class_loss = 2.7887067794799805
macro_f1 = 0.08614717320397111
weighted_f1 = 0.08613023271880597
regression_loss = 0.35092195868492126
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0004995987158908507, 9.991974317817014e-05]
2.86736798286438
2.932664394378662
2.8844940662384033
2.815394401550293
2.8220725059509277
2.853024959564209
2.8201239109039307
2.8745386600494385
2.7437286376953125
avg_train_loss 3.0148502699762796
avg_val_loss 2.8459343910217285
Done 32 Batches of 139
class_loss = 2.783470392227173
macro_f1 = 0.07205690274398217
weighted_f1 = 0.07211042849319736
regression_loss = 0.376596599817276
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0004993980738362761, 9.987961476725522e-05]
Done 64 Batches of 139
class_loss = 2.767638921737671
macro_f1 = 0.08402343012128698
weighted_f1 = 0.08466838079413656
regression_loss = 0.45187270641326904
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0004991974317817015, 9.98394863563403e-05]
Done 96 Batches of 139
class_loss = 2.646216630935669
macro_f1 = 0.09658800878030627
weighted_f1 = 0.0969074868518322
regression_loss = 0.3249499797821045
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0004989967897271268, 9.979935794542536e-05]
Done 128 Batches of 139
class_loss = 2.61776065826416
macro_f1 = 0.1116730751570492
weighted_f1 = 0.11161156293381212
regression_loss = 0.41059523820877075
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0004987961476725522, 9.975922953451044e-05]
Done 139 Batches of 139
class_loss = 2.6057395935058594
macro_f1 = 0.11324121189366565
weighted_f1 = 0.11322066055256656
regression_loss = 0.33458152413368225
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0004985955056179776, 9.971910112359551e-05]
2.7159957885742188
2.6822891235351562
2.7337183952331543
2.662261486053467
2.631415367126465
2.6658475399017334
2.5881338119506836
2.6735589504241943
2.404994249343872
avg_train_loss 2.8390564524012505
avg_val_loss 2.6398016346825495
Done 32 Batches of 139
class_loss = 2.5353212356567383
macro_f1 = 0.14558159667548845
weighted_f1 = 0.14834136451620158
regression_loss = 0.3449937105178833
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.000498394863563403, 9.967897271268059e-05]
Done 64 Batches of 139
class_loss = 2.516230344772339
macro_f1 = 0.15192706582973697
weighted_f1 = 0.15215617111859883
regression_loss = 0.36150771379470825
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0004981942215088283, 9.963884430176565e-05]
Done 96 Batches of 139
class_loss = 2.4202113151550293
macro_f1 = 0.16491317798486607
weighted_f1 = 0.16481070845678925
regression_loss = 0.41532835364341736
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0004979935794542536, 9.959871589085073e-05]
Done 128 Batches of 139
class_loss = 2.3918564319610596
macro_f1 = 0.17603991982252737
weighted_f1 = 0.1760981142091895
regression_loss = 0.3054153621196747
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.000497792937399679, 9.955858747993579e-05]
Done 139 Batches of 139
class_loss = 2.487020492553711
macro_f1 = 0.1785026103924565
weighted_f1 = 0.17847708191248743
regression_loss = 0.38497117161750793
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0004975922953451043, 9.951845906902088e-05]
2.540268898010254
2.5870115756988525
2.6121063232421875
2.5245890617370605
2.491039991378784
2.5159318447113037
2.4774396419525146
2.5132105350494385
2.3999416828155518
avg_train_loss 2.608790334180105
avg_val_loss 2.5179488393995495
Done 32 Batches of 139
class_loss = 2.4602978229522705
macro_f1 = 0.22263198014955088
weighted_f1 = 0.22355533096549354
regression_loss = 0.4124358296394348
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0004973916532905297, 9.947833065810594e-05]
Done 64 Batches of 139
class_loss = 2.3612301349639893
macro_f1 = 0.22944986705584675
weighted_f1 = 0.2289376235343231
regression_loss = 0.3036770820617676
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0004971910112359551, 9.943820224719102e-05]
Done 96 Batches of 139
class_loss = 2.4468772411346436
macro_f1 = 0.2316789630242419
weighted_f1 = 0.23076538874420402
regression_loss = 0.3891761898994446
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0004969903691813805, 9.939807383627608e-05]
Done 128 Batches of 139
class_loss = 2.268181324005127
macro_f1 = 0.2395106586521405
weighted_f1 = 0.23920249526947557
regression_loss = 0.41008293628692627
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0004967897271268058, 9.935794542536117e-05]
Done 139 Batches of 139
class_loss = 2.296494722366333
macro_f1 = 0.24219885250844336
weighted_f1 = 0.24200809565491396
regression_loss = 0.4005165100097656
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0004965890850722311, 9.931781701444623e-05]
2.396250009536743
2.5144197940826416
2.3384151458740234
2.2867355346679688
2.271237373352051
2.3819079399108887
2.253032684326172
2.289412498474121
2.136827230453491
avg_train_loss 2.4461528431597372
avg_val_loss 2.3186931345197888
Traceback (most recent call last):
  File "/pfs/lustrep2/projappl/project_465002413/ChessGPT/chessgpt_multitask.py", line 649, in <module>
    model.load_state_dict(torch.load(r"/best_models/best_model4.pth"))  # I load the model that performed better on validation
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/miniconda3/envs/pytorch/lib/python3.12/site-packages/torch/serialization.py", line 1479, in load
    with _open_file_like(f, "rb") as opened_file:
         ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/miniconda3/envs/pytorch/lib/python3.12/site-packages/torch/serialization.py", line 759, in _open_file_like
    return _open_file(name_or_buffer, mode)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/miniconda3/envs/pytorch/lib/python3.12/site-packages/torch/serialization.py", line 740, in __init__
    super().__init__(open(name, mode))
                     ^^^^^^^^^^^^^^^^
FileNotFoundError: [Errno 2] No such file or directory: '/best_models/best_model2.pth'
