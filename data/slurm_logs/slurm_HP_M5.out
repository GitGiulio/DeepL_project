/pfs/lustrep2/projappl/project_465002413/ChessGPT/chessgpt_multitask.py:33: DtypeWarning: Columns (3,5) have mixed types. Specify dtype option on import or set low_memory=False.
  df = pd.read_csv(csv_path)
/pfs/lustrep2/projappl/project_465002413/ChessGPT/chessgpt_multitask.py:80: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.
The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.

For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.


  df_white_black['white_elo'].fillna(df_white_black['white_elo'].mean(), inplace=True)
/pfs/lustrep2/projappl/project_465002413/ChessGPT/chessgpt_multitask.py:81: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.
The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.

For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.


  df_white_black['black_elo'].fillna(df_white_black['black_elo'].mean(), inplace=True)
Total: 20,853
Train: 17,725 (85.00%)
Val:   1,042 (5.00%)
Test:  2,086 (10.00%)
|-|-|-|-|-|mean_elo: 2663.78369140625|-|-|-|-|
|-|-|-|-|-|std_elo: 110.49032592773438|-|-|-|-|

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.25s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  2.83s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.34s/it]
HYPERPARAM: 
 shared_dim1=1024, 
 shared_dim2=512, 
 head_dim=256, 
 dropout=0.0
HYPERPARAM: tokenizer, max_length=128
139
Unfroze last transformer layer.
HYPERPARAM: lrs: 5e-4  &  1e-4
HYPERPARAM: weight_decays: 0.00001  &  0.00001
Using device: cuda
HYPERPARAM: alpha: 0.2
Done 32 Batches of 139
class_loss = 2.9980320930480957
macro_f1 = 0.004523580365736285
weighted_f1 = 0.004286457201403335
regression_loss = 0.325667142868042
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [6.25e-05, 1.25e-05]
Done 64 Batches of 139
class_loss = 2.997694730758667
macro_f1 = 0.004860783388390751
weighted_f1 = 0.004966871914724676
regression_loss = 0.3774903118610382
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.000125, 2.5e-05]
Done 96 Batches of 139
class_loss = 2.9937005043029785
macro_f1 = 0.005193385869991375
weighted_f1 = 0.0051454546814578206
regression_loss = 0.36021947860717773
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0001875, 3.7500000000000003e-05]
Done 128 Batches of 139
class_loss = 2.9908018112182617
macro_f1 = 0.014840469376755039
weighted_f1 = 0.014845888687701174
regression_loss = 0.4314316511154175
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00025, 5e-05]
Done 139 Batches of 139
class_loss = 2.9846293926239014
macro_f1 = 0.025000726867984113
weighted_f1 = 0.025035776486191357
regression_loss = 0.31083446741104126
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0003125, 6.25e-05]
3.0521388053894043
3.056549310684204
3.049781084060669
3.0296154022216797
3.0414741039276123
3.0177013874053955
3.0218522548675537
3.031724691390991
3.013432502746582
avg_train_loss 3.064341672032857
avg_val_loss 3.0349188380771213
Done 32 Batches of 139
class_loss = 2.9588661193847656
macro_f1 = 0.08921684138303403
weighted_f1 = 0.08932647564041227
regression_loss = 0.34985506534576416
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.000375, 7.500000000000001e-05]
Done 64 Batches of 139
class_loss = 2.920109272003174
macro_f1 = 0.09624821102655835
weighted_f1 = 0.0956585670636683
regression_loss = 0.27243345975875854
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0004375, 8.75e-05]
Done 96 Batches of 139
class_loss = 2.855030059814453
macro_f1 = 0.11793080248374324
weighted_f1 = 0.11719809130977457
regression_loss = 0.3210047483444214
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0005, 0.0001]
Done 128 Batches of 139
class_loss = 2.750011444091797
macro_f1 = 0.1173272878054649
weighted_f1 = 0.11715959368989529
regression_loss = 0.3820229768753052
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0004997993579454254, 9.995987158908508e-05]
Done 139 Batches of 139
class_loss = 2.855027437210083
macro_f1 = 0.11867386432841798
weighted_f1 = 0.1186171796191601
regression_loss = 0.21993513405323029
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0004995987158908507, 9.991974317817014e-05]
2.8407840728759766
2.8753559589385986
2.8346762657165527
2.8733150959014893
2.7497382164001465
2.7267186641693115
2.763943672180176
2.7736902236938477
2.5557186603546143
avg_train_loss 2.9410838308951837
avg_val_loss 2.7771045366923013
Done 32 Batches of 139
class_loss = 2.7447409629821777
macro_f1 = 0.14104898759117748
weighted_f1 = 0.13819812389170205
regression_loss = 0.35293373465538025
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0004993980738362761, 9.987961476725522e-05]
Done 64 Batches of 139
class_loss = 2.6019656658172607
macro_f1 = 0.13753873237653771
weighted_f1 = 0.13620467667900435
regression_loss = 0.34206852316856384
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0004991974317817015, 9.98394863563403e-05]
Done 96 Batches of 139
class_loss = 2.747490406036377
macro_f1 = 0.1418276052909221
weighted_f1 = 0.14146814422222828
regression_loss = 0.3675021529197693
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0004989967897271268, 9.979935794542536e-05]
Done 128 Batches of 139
class_loss = 2.4875950813293457
macro_f1 = 0.15273943521346936
weighted_f1 = 0.1527731913193922
regression_loss = 0.3757396340370178
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0004987961476725522, 9.975922953451044e-05]
Done 139 Batches of 139
class_loss = 2.4783778190612793
macro_f1 = 0.1565910221413867
weighted_f1 = 0.1565290482868821
regression_loss = 0.4099055826663971
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0004985955056179776, 9.971910112359551e-05]
2.6599490642547607
2.517076253890991
2.6772947311401367
2.5634474754333496
2.5765414237976074
2.5291225910186768
2.4827332496643066
2.477261543273926
2.5043435096740723
avg_train_loss 2.7015954607682264
avg_val_loss 2.5541966491275363
Done 32 Batches of 139
class_loss = 2.5974338054656982
macro_f1 = 0.1744852715721027
weighted_f1 = 0.177353239705188
regression_loss = 0.3361539840698242
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.000498394863563403, 9.967897271268059e-05]
Done 64 Batches of 139
class_loss = 2.5659496784210205
macro_f1 = 0.1950674394638697
weighted_f1 = 0.19449186594533757
regression_loss = 0.27480363845825195
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0004981942215088283, 9.963884430176565e-05]
Done 96 Batches of 139
class_loss = 2.441289186477661
macro_f1 = 0.20998268470987078
weighted_f1 = 0.21121171008340112
regression_loss = 0.3358401358127594
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0004979935794542536, 9.959871589085073e-05]
Done 128 Batches of 139
class_loss = 2.408306837081909
macro_f1 = 0.22110464240703695
weighted_f1 = 0.22150419844095953
regression_loss = 0.32301560044288635
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.000497792937399679, 9.955858747993579e-05]
Done 139 Batches of 139
class_loss = 2.4216275215148926
macro_f1 = 0.22315089178369946
weighted_f1 = 0.22310559772746097
regression_loss = 0.5239304304122925
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0004975922953451043, 9.951845906902088e-05]
2.4433979988098145
2.4561572074890137
2.4036993980407715
2.336820602416992
2.3636770248413086
2.3750953674316406
2.2966535091400146
2.297358751296997
2.2515082359313965
avg_train_loss 2.4701376475876184
avg_val_loss 2.358263121710883
Done 32 Batches of 139
class_loss = 2.2667787075042725
macro_f1 = 0.265413250733015
weighted_f1 = 0.267425000374749
regression_loss = 0.2821909189224243
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0004973916532905297, 9.947833065810594e-05]
Done 64 Batches of 139
class_loss = 2.002056121826172
macro_f1 = 0.2838584627855519
weighted_f1 = 0.2843134736341476
regression_loss = 0.3244142532348633
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0004971910112359551, 9.943820224719102e-05]
Done 96 Batches of 139
class_loss = 2.204505681991577
macro_f1 = 0.28201925001962846
weighted_f1 = 0.28102866059757464
regression_loss = 0.38282889127731323
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0004969903691813805, 9.939807383627608e-05]
Done 128 Batches of 139
class_loss = 2.2594492435455322
macro_f1 = 0.28845104927691617
weighted_f1 = 0.28833841026964363
regression_loss = 0.3459585905075073
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0004967897271268058, 9.935794542536117e-05]
Done 139 Batches of 139
class_loss = 1.990704894065857
macro_f1 = 0.2913673559003848
weighted_f1 = 0.2912921176916869
regression_loss = 0.21643637120723724
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0004965890850722311, 9.931781701444623e-05]
2.1884219646453857
2.331127405166626
2.2416765689849854
2.3336193561553955
2.1561522483825684
2.2863895893096924
2.0791382789611816
2.1087253093719482
2.053205966949463
avg_train_loss 2.2471031739557388
avg_val_loss 2.1976062986585827
2.352736711502075
2.188138484954834
2.1315758228302
1.9872616529464722
2.0314979553222656
2.275738000869751
2.2770018577575684
2.3940389156341553
2.2873003482818604
2.1514415740966797
1.946935772895813
2.0872642993927
1.9525648355484009
2.1183063983917236
2.1704728603363037
2.021228313446045
1.9670342206954956
