/pfs/lustrep2/projappl/project_465002413/ChessGPT/chessgpt_multitask.py:40: DtypeWarning: Columns (3,5) have mixed types. Specify dtype option on import or set low_memory=False.
  df = pd.read_csv(csv_path)
/pfs/lustrep2/projappl/project_465002413/ChessGPT/chessgpt_multitask.py:86: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.
The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.

For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.


  df_white_black['white_elo'].fillna(df_white_black['white_elo'].mean(), inplace=True)
/pfs/lustrep2/projappl/project_465002413/ChessGPT/chessgpt_multitask.py:87: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.
The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.

For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.


  df_white_black['black_elo'].fillna(df_white_black['black_elo'].mean(), inplace=True)
Total: 163,249
Train: 138,761 (85.00%)
Val:   8,162 (5.00%)
Test:  16,326 (10.00%)
|-|-|-|-|-|mean_elo: 2663.914794921875|-|-|-|-|
|-|-|-|-|-|std_elo: 111.10905456542969|-|-|-|-|

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.16s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  2.76s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.27s/it]
HYPERPARAM: 
 shared_dim1=1024, 
 shared_dim2=512, 
 head_dim=256, 
 dropout=0.1
HYPERPARAM: tokenizer, max_length=256
1085
Unfroze last transformer layer.
HYPERPARAM: lrs: 5e-4  &  1e-4
HYPERPARAM: weight_decays: 0.005 &  0.001
Using device: cuda
-------LOADING MODEL--------------
-------LOADING MODEL DONE---------
HYPERPARAM: alpha: 0.2
-------------------------------------------------
Done 32 Batches of 1085
class_loss = 0.1885954737663269
macro_f1 = 0.9452184281341227
weighted_f1 = 0.9449258812326333
regression_loss = 0.27017077803611755
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [2e-05, 4.000000000000001e-06]
-------------------------------------------------
Done 64 Batches of 1085
class_loss = 0.1221403107047081
macro_f1 = 0.946400288722461
weighted_f1 = 0.9460139251362311
regression_loss = 0.35018259286880493
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [4e-05, 8.000000000000001e-06]
-------------------------------------------------
Done 96 Batches of 1085
class_loss = 0.20082545280456543
macro_f1 = 0.9459221524237693
weighted_f1 = 0.9459184400434976
regression_loss = 0.264914870262146
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [6e-05, 1.2e-05]
-------------------------------------------------
Done 128 Batches of 1085
class_loss = 0.22943073511123657
macro_f1 = 0.9461885057620811
weighted_f1 = 0.946000889738138
regression_loss = 0.2937150001525879
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [8e-05, 1.6000000000000003e-05]
-------------------------------------------------
Done 160 Batches of 1085
class_loss = 0.10209503769874573
macro_f1 = 0.945459956968033
weighted_f1 = 0.9451858104510814
regression_loss = 0.28132620453834534
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0001, 2e-05]
-------------------------------------------------
Done 192 Batches of 1085
class_loss = 0.13163506984710693
macro_f1 = 0.9459080141876907
weighted_f1 = 0.9457240008406179
regression_loss = 0.25644591450691223
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00012, 2.4e-05]
-------------------------------------------------
Done 224 Batches of 1085
class_loss = 0.11283168941736221
macro_f1 = 0.9463663750945269
weighted_f1 = 0.9462308694301764
regression_loss = 0.30142295360565186
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00014000000000000001, 2.8000000000000003e-05]
-------------------------------------------------
Done 256 Batches of 1085
class_loss = 0.2326107919216156
macro_f1 = 0.9469612154517492
weighted_f1 = 0.9469084469554527
regression_loss = 0.24136173725128174
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00016, 3.2000000000000005e-05]
-------------------------------------------------
Done 288 Batches of 1085
class_loss = 0.16442129015922546
macro_f1 = 0.9463033606839831
weighted_f1 = 0.9462736772796603
regression_loss = 0.26069921255111694
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00017999999999999998, 3.6e-05]
-------------------------------------------------
Done 320 Batches of 1085
class_loss = 0.1929573267698288
macro_f1 = 0.9467301945729669
weighted_f1 = 0.9467470208940062
regression_loss = 0.24181899428367615
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0002, 4e-05]
-------------------------------------------------
Done 352 Batches of 1085
class_loss = 0.18226352334022522
macro_f1 = 0.9468433648039072
weighted_f1 = 0.9468529141407249
regression_loss = 0.30034488439559937
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00022, 4.4000000000000006e-05]
-------------------------------------------------
Done 384 Batches of 1085
class_loss = 0.13483528792858124
macro_f1 = 0.9462825197849523
weighted_f1 = 0.9462992365903755
regression_loss = 0.22578898072242737
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00024, 4.8e-05]
-------------------------------------------------
Done 416 Batches of 1085
class_loss = 0.16891264915466309
macro_f1 = 0.9473229671524204
weighted_f1 = 0.9473892055783268
regression_loss = 0.3214013874530792
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00026000000000000003, 5.2000000000000004e-05]
-------------------------------------------------
Done 448 Batches of 1085
class_loss = 0.10204323381185532
macro_f1 = 0.9473416608839781
weighted_f1 = 0.9473971675318429
regression_loss = 0.23305673897266388
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00028000000000000003, 5.6000000000000006e-05]
-------------------------------------------------
Done 480 Batches of 1085
class_loss = 0.17784133553504944
macro_f1 = 0.9475260728581653
weighted_f1 = 0.9475881368291146
regression_loss = 0.22075794637203217
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0003, 6e-05]
-------------------------------------------------
Done 512 Batches of 1085
class_loss = 0.18320444226264954
macro_f1 = 0.9469954179542593
weighted_f1 = 0.9470376925686703
regression_loss = 0.2492348700761795
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00032, 6.400000000000001e-05]
-------------------------------------------------
Done 544 Batches of 1085
class_loss = 0.12002778798341751
macro_f1 = 0.94646699913863
weighted_f1 = 0.9464892308764277
regression_loss = 0.26074135303497314
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00034, 6.800000000000001e-05]
-------------------------------------------------
Done 576 Batches of 1085
class_loss = 0.17113105952739716
macro_f1 = 0.9458540849025399
weighted_f1 = 0.9458669686360247
regression_loss = 0.2703433632850647
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00035999999999999997, 7.2e-05]
-------------------------------------------------
Done 608 Batches of 1085
class_loss = 0.2004299908876419
macro_f1 = 0.9455342093401023
weighted_f1 = 0.9455274272520728
regression_loss = 0.22843022644519806
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00038, 7.6e-05]
-------------------------------------------------
Done 640 Batches of 1085
class_loss = 0.11714651435613632
macro_f1 = 0.9452918054829184
weighted_f1 = 0.9452559262925743
regression_loss = 0.21243968605995178
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0004, 8e-05]
-------------------------------------------------
Done 672 Batches of 1085
class_loss = 0.1903601884841919
macro_f1 = 0.9450388432812865
weighted_f1 = 0.9450190827327637
regression_loss = 0.286230206489563
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00042, 8.4e-05]
-------------------------------------------------
Done 704 Batches of 1085
class_loss = 0.24198627471923828
macro_f1 = 0.9445280802203525
weighted_f1 = 0.9444794218145899
regression_loss = 0.27398258447647095
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00044, 8.800000000000001e-05]
-------------------------------------------------
Done 736 Batches of 1085
class_loss = 0.27065280079841614
macro_f1 = 0.9436159364062379
weighted_f1 = 0.9435674075468612
regression_loss = 0.24293282628059387
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00046, 9.200000000000001e-05]
-------------------------------------------------
Done 768 Batches of 1085
class_loss = 0.28525370359420776
macro_f1 = 0.9433825604430031
weighted_f1 = 0.9433096560433503
regression_loss = 0.25241440534591675
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00048, 9.6e-05]
-------------------------------------------------
Done 800 Batches of 1085
class_loss = 0.23692557215690613
macro_f1 = 0.9426018026781966
weighted_f1 = 0.9425339015615616
regression_loss = 0.3394542336463928
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0005, 0.0001]
-------------------------------------------------
Done 832 Batches of 1085
class_loss = 0.26559510827064514
macro_f1 = 0.9419850049167767
weighted_f1 = 0.9419305755911032
regression_loss = 0.2777518630027771
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0004984615384615384, 9.96923076923077e-05]
-------------------------------------------------
Done 864 Batches of 1085
class_loss = 0.24304117262363434
macro_f1 = 0.9414933388075359
weighted_f1 = 0.9414559702103028
regression_loss = 0.21921588480472565
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0004969230769230769, 9.938461538461539e-05]
-------------------------------------------------
Done 896 Batches of 1085
class_loss = 0.2301093339920044
macro_f1 = 0.9412469108851991
weighted_f1 = 0.9412235777301257
regression_loss = 0.22714896500110626
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0004953846153846154, 9.907692307692308e-05]
-------------------------------------------------
Done 928 Batches of 1085
class_loss = 0.2545996606349945
macro_f1 = 0.9406738134485175
weighted_f1 = 0.9406546085761969
regression_loss = 0.2754814624786377
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0004938461538461538, 9.876923076923077e-05]
-------------------------------------------------
Done 960 Batches of 1085
class_loss = 0.3273594379425049
macro_f1 = 0.9400543613456198
weighted_f1 = 0.9400448175261522
regression_loss = 0.29058313369750977
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0004923076923076924, 9.846153846153848e-05]
-------------------------------------------------
Done 992 Batches of 1085
class_loss = 0.14858679473400116
macro_f1 = 0.9393196906638315
weighted_f1 = 0.9393113968217318
regression_loss = 0.27765953540802
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0004907692307692308, 9.815384615384616e-05]
-------------------------------------------------
Done 1024 Batches of 1085
class_loss = 0.18470779061317444
macro_f1 = 0.9385735341005818
weighted_f1 = 0.9385657552760673
regression_loss = 0.19074013829231262
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0004892307692307693, 9.784615384615386e-05]
-------------------------------------------------
Done 1056 Batches of 1085
class_loss = 0.19543246924877167
macro_f1 = 0.9382967552699768
weighted_f1 = 0.9382972708152174
regression_loss = 0.27175915241241455
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0004876923076923077, 9.753846153846154e-05]
-------------------------------------------------
Done 1085 Batches of 1085
class_loss = 0.16534334421157837
macro_f1 = 0.9377733513932569
weighted_f1 = 0.9377737825949384
regression_loss = 0.10778605192899704
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0004861538461538462, 9.723076923076924e-05]
1.3501793146133423
1.031071424484253
1.059863567352295
1.0801899433135986
1.2780001163482666
0.7845947742462158
1.1542060375213623
1.195475697517395
1.0023460388183594
1.2167576551437378
1.114896297454834
1.0127263069152832
1.170793890953064
1.4352751970291138
1.1774364709854126
1.3881254196166992
1.2011830806732178
1.352309226989746
1.2413756847381592
1.119720458984375
0.9567350745201111
1.058280110359192
1.3328733444213867
0.9188359975814819
1.1220775842666626
1.396325707435608
1.1165193319320679
0.9987744092941284
0.9831812381744385
1.2218711376190186
1.4819574356079102
1.1298515796661377
0.874383270740509
1.4643298387527466
1.1709996461868286
1.2848997116088867
1.1027675867080688
1.3986752033233643
1.172731876373291
1.3222700357437134
0.746768593788147
1.0117123126983643
0.970342218875885
1.2027487754821777
1.611955165863037
1.0576773881912231
1.2266746759414673
1.0987181663513184
1.0939711332321167
1.308454990386963
1.1702020168304443
0.9054327011108398
1.1926774978637695
1.0886168479919434
1.4902937412261963
0.9698602557182312
1.3068137168884277
0.819082498550415
0.8720896244049072
1.0576672554016113
1.4729653596878052
1.2294538021087646
1.2022422552108765
1.4149731397628784
Epoch 8, training metrics:
--- CLASSIFICATION METRICS --- 
F1 scores: [Macro=0.938, Weighted=0.938] 
Balanced Accuracy = 0.938
Average loss = 0.24470
Epoch 8, validation metrics:
--- CLASSIFICATION METRICS --- 
F1 scores: [Macro=0.740, Weighted=0.740] 
Balanced Accuracy = 0.741
Average loss = 1.16244
avg_train_loss 0.24470212819389484
avg_val_loss 1.1624415758997202
-------------------------------------------------
Done 32 Batches of 1085
class_loss = 0.272450715303421
macro_f1 = 0.9461966233750875
weighted_f1 = 0.9463599847977118
regression_loss = 0.2985270917415619
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0004846153846153846, 9.692307692307692e-05]
-------------------------------------------------
Done 64 Batches of 1085
class_loss = 0.1367286890745163
macro_f1 = 0.9498310860454975
weighted_f1 = 0.9500493936698203
regression_loss = 0.245834618806839
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0004830769230769231, 9.661538461538462e-05]
-------------------------------------------------
Done 96 Batches of 1085
class_loss = 0.1506393998861313
macro_f1 = 0.9509598279203317
weighted_f1 = 0.9512166387815869
regression_loss = 0.28131216764450073
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0004815384615384615, 9.63076923076923e-05]
-------------------------------------------------
Done 128 Batches of 1085
class_loss = 0.1068384051322937
macro_f1 = 0.9515972582975172
weighted_f1 = 0.9517769513178631
regression_loss = 0.23588332533836365
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00048, 9.6e-05]
-------------------------------------------------
Done 160 Batches of 1085
class_loss = 0.15591102838516235
macro_f1 = 0.951677634933529
weighted_f1 = 0.9518246016701895
regression_loss = 0.2208908349275589
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00047846153846153844, 9.569230769230769e-05]
-------------------------------------------------
Done 192 Batches of 1085
class_loss = 0.09884622693061829
macro_f1 = 0.952229099137867
weighted_f1 = 0.9523564968699686
regression_loss = 0.2984774708747864
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.000476923076923077, 9.53846153846154e-05]
-------------------------------------------------
Done 224 Batches of 1085
class_loss = 0.12685057520866394
macro_f1 = 0.951364418710656
weighted_f1 = 0.9515225424336914
regression_loss = 0.3220731019973755
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0004753846153846154, 9.507692307692308e-05]
-------------------------------------------------
Done 256 Batches of 1085
class_loss = 0.1958930641412735
macro_f1 = 0.9502838820417375
weighted_f1 = 0.9504109888285227
regression_loss = 0.20467311143875122
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00047384615384615385, 9.476923076923078e-05]
-------------------------------------------------
Done 288 Batches of 1085
class_loss = 0.12511657178401947
macro_f1 = 0.9494525821654138
weighted_f1 = 0.949613293647978
regression_loss = 0.23126354813575745
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00047230769230769234, 9.446153846153846e-05]
-------------------------------------------------
Done 320 Batches of 1085
class_loss = 0.23906061053276062
macro_f1 = 0.9492334767070023
weighted_f1 = 0.9494063002356381
regression_loss = 0.2695074677467346
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00047076923076923077, 9.415384615384616e-05]
-------------------------------------------------
Done 352 Batches of 1085
class_loss = 0.11738689243793488
macro_f1 = 0.9487353795143877
weighted_f1 = 0.9488650003986274
regression_loss = 0.3095542788505554
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00046923076923076926, 9.384615384615386e-05]
-------------------------------------------------
Done 384 Batches of 1085
class_loss = 0.14164042472839355
macro_f1 = 0.9484526766872008
weighted_f1 = 0.9485611031925438
regression_loss = 0.22790439426898956
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0004676923076923077, 9.353846153846154e-05]
-------------------------------------------------
Done 416 Batches of 1085
class_loss = 0.07948053628206253
macro_f1 = 0.9480834948605972
weighted_f1 = 0.9481781000763335
regression_loss = 0.2399062216281891
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0004661538461538462, 9.323076923076924e-05]
-------------------------------------------------
Done 448 Batches of 1085
class_loss = 0.21925930678844452
macro_f1 = 0.9478957263778895
weighted_f1 = 0.9479728506230083
regression_loss = 0.19405390322208405
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0004646153846153846, 9.292307692307692e-05]
-------------------------------------------------
Done 480 Batches of 1085
class_loss = 0.10307592153549194
macro_f1 = 0.9475997793550596
weighted_f1 = 0.9476742409809238
regression_loss = 0.2273302525281906
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00046307692307692304, 9.261538461538462e-05]
-------------------------------------------------
Done 512 Batches of 1085
class_loss = 0.11941392719745636
macro_f1 = 0.9471308015581927
weighted_f1 = 0.9472145193681243
regression_loss = 0.24488390982151031
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0004615384615384616, 9.230769230769232e-05]
-------------------------------------------------
Done 544 Batches of 1085
class_loss = 0.172056183218956
macro_f1 = 0.9463353908076085
weighted_f1 = 0.9464385919841329
regression_loss = 0.2592153549194336
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00046, 9.200000000000001e-05]
-------------------------------------------------
Done 576 Batches of 1085
class_loss = 0.12426834553480148
macro_f1 = 0.9461520520957795
weighted_f1 = 0.9462239887385397
regression_loss = 0.29739436507225037
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0004584615384615385, 9.16923076923077e-05]
-------------------------------------------------
Done 608 Batches of 1085
class_loss = 0.20372912287712097
macro_f1 = 0.9461077426092283
weighted_f1 = 0.9461605727245447
regression_loss = 0.22832226753234863
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00045692307692307693, 9.13846153846154e-05]
-------------------------------------------------
Done 640 Batches of 1085
class_loss = 0.2524190843105316
macro_f1 = 0.9464545740455304
weighted_f1 = 0.9465157142869582
regression_loss = 0.22276808321475983
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0004553846153846154, 9.107692307692308e-05]
-------------------------------------------------
Done 672 Batches of 1085
class_loss = 0.1690601110458374
macro_f1 = 0.9457889223419924
weighted_f1 = 0.9458604811108227
regression_loss = 0.22252202033996582
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00045384615384615385, 9.076923076923078e-05]
-------------------------------------------------
Done 704 Batches of 1085
class_loss = 0.20608073472976685
macro_f1 = 0.9452573834082113
weighted_f1 = 0.945304338653202
regression_loss = 0.24385115504264832
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0004523076923076923, 9.046153846153846e-05]
-------------------------------------------------
Done 736 Batches of 1085
class_loss = 0.22368761897087097
macro_f1 = 0.9449700665194051
weighted_f1 = 0.9450301830564143
regression_loss = 0.2188040018081665
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00045076923076923077, 9.015384615384616e-05]
-------------------------------------------------
Done 768 Batches of 1085
class_loss = 0.1975080519914627
macro_f1 = 0.9450216098977746
weighted_f1 = 0.9450403647061094
regression_loss = 0.22813111543655396
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0004492307692307692, 8.984615384615384e-05]
-------------------------------------------------
Done 800 Batches of 1085
class_loss = 0.16433127224445343
macro_f1 = 0.9445261512032367
weighted_f1 = 0.944535478771554
regression_loss = 0.19293248653411865
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0004476923076923077, 8.953846153846154e-05]
-------------------------------------------------
Done 832 Batches of 1085
class_loss = 0.23686863481998444
macro_f1 = 0.9442085040875243
weighted_f1 = 0.9442121469025386
regression_loss = 0.24792146682739258
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0004461538461538462, 8.923076923076924e-05]
-------------------------------------------------
Done 864 Batches of 1085
class_loss = 0.28747087717056274
macro_f1 = 0.9437930132088594
weighted_f1 = 0.9438049785096629
regression_loss = 0.2263883352279663
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00044461538461538466, 8.892307692307693e-05]
-------------------------------------------------
Done 896 Batches of 1085
class_loss = 0.31558018922805786
macro_f1 = 0.9438135601227181
weighted_f1 = 0.943817765855437
regression_loss = 0.2243148684501648
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0004430769230769231, 8.861538461538462e-05]
-------------------------------------------------
Done 928 Batches of 1085
class_loss = 0.2590855658054352
macro_f1 = 0.9436648812794373
weighted_f1 = 0.9436628256151759
regression_loss = 0.24115389585494995
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00044153846153846153, 8.830769230769231e-05]
-------------------------------------------------
Done 960 Batches of 1085
class_loss = 0.24816356599330902
macro_f1 = 0.9432753871551318
weighted_f1 = 0.9432902872636704
regression_loss = 0.24918991327285767
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00044, 8.800000000000001e-05]
-------------------------------------------------
Done 992 Batches of 1085
class_loss = 0.1604268103837967
macro_f1 = 0.9429533679807289
weighted_f1 = 0.9429563249665879
regression_loss = 0.2976655960083008
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00043846153846153845, 8.76923076923077e-05]
-------------------------------------------------
Done 1024 Batches of 1085
class_loss = 0.12165838479995728
macro_f1 = 0.942704221242025
weighted_f1 = 0.9426944472541022
regression_loss = 0.209520623087883
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00043692307692307693, 8.738461538461539e-05]
-------------------------------------------------
Done 1056 Batches of 1085
class_loss = 0.15247930586338043
macro_f1 = 0.9427687248312211
weighted_f1 = 0.9427483957951787
regression_loss = 0.2354663759469986
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00043538461538461537, 8.707692307692308e-05]
-------------------------------------------------
Done 1085 Batches of 1085
class_loss = 0.02083538845181465
macro_f1 = 0.9425735470366092
weighted_f1 = 0.9425631805699569
regression_loss = 0.1741029918193817
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00043384615384615385, 8.676923076923077e-05]
1.575850486755371
0.8986497521400452
1.020155668258667
0.8866798281669617
1.1213077306747437
0.7375307679176331
1.0464863777160645
1.1664758920669556
1.0304887294769287
1.3626959323883057
1.0172353982925415
1.1624586582183838
1.1646617650985718
1.448938250541687
1.2112518548965454
1.2274739742279053
1.2925792932510376
1.1716761589050293
1.1793771982192993
1.0160365104675293
1.0389740467071533
0.9311294555664062
1.3872125148773193
1.0437793731689453
1.151013731956482
1.2550878524780273
1.2098345756530762
1.1563434600830078
1.157734751701355
1.2870852947235107
1.3431874513626099
1.0984165668487549
0.76774001121521
1.524713158607483
1.2205203771591187
1.4145183563232422
1.0753628015518188
1.5563913583755493
1.307426929473877
1.3805047273635864
0.7772150635719299
0.8010858297348022
1.1242917776107788
1.237217903137207
1.5911214351654053
1.1279181241989136
1.3152916431427002
1.1750129461288452
1.2280139923095703
1.2521241903305054
1.155236840248108
0.7300301790237427
1.2881934642791748
0.9215857982635498
1.482400894165039
1.0503828525543213
1.3722608089447021
0.742121696472168
1.0040897130966187
1.172314167022705
1.5903584957122803
1.237591028213501
1.211521029472351
1.4486972093582153
Epoch 9, training metrics:
--- CLASSIFICATION METRICS --- 
F1 scores: [Macro=0.943, Weighted=0.943] 
Balanced Accuracy = 0.943
Average loss = 0.22608
Epoch 9, validation metrics:
--- CLASSIFICATION METRICS --- 
F1 scores: [Macro=0.752, Weighted=0.752] 
Balanced Accuracy = 0.751
Average loss = 1.17314
avg_train_loss 0.22608494013547897
avg_val_loss 1.1731416266411543
-------------------------------------------------
Done 32 Batches of 1085
class_loss = 0.07469522207975388
macro_f1 = 0.9634258046212324
weighted_f1 = 0.963598486880829
regression_loss = 0.23182788491249084
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0004323076923076923, 8.646153846153846e-05]
-------------------------------------------------
Done 64 Batches of 1085
class_loss = 0.08071319013834
macro_f1 = 0.9639735175486089
weighted_f1 = 0.9639132591813696
regression_loss = 0.26851797103881836
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00043076923076923083, 8.615384615384617e-05]
-------------------------------------------------
Done 96 Batches of 1085
class_loss = 0.09833211451768875
macro_f1 = 0.9623447273879462
weighted_f1 = 0.9623367964760662
regression_loss = 0.22592055797576904
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00042923076923076926, 8.584615384615385e-05]
-------------------------------------------------
Done 128 Batches of 1085
class_loss = 0.05698426440358162
macro_f1 = 0.9642377913375684
weighted_f1 = 0.9641904289252556
regression_loss = 0.23600569367408752
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0004276923076923077, 8.553846153846155e-05]
-------------------------------------------------
Done 160 Batches of 1085
class_loss = 0.07392557710409164
macro_f1 = 0.9642973444140928
weighted_f1 = 0.9643464992474508
regression_loss = 0.23614606261253357
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0004261538461538462, 8.523076923076923e-05]
-------------------------------------------------
Done 192 Batches of 1085
class_loss = 0.14791461825370789
macro_f1 = 0.9642820940852607
weighted_f1 = 0.9643534983611854
regression_loss = 0.2300124615430832
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0004246153846153846, 8.492307692307693e-05]
-------------------------------------------------
Done 224 Batches of 1085
class_loss = 0.09425369650125504
macro_f1 = 0.9645024806681637
weighted_f1 = 0.9645976890520587
regression_loss = 0.20110371708869934
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0004230769230769231, 8.461538461538461e-05]
-------------------------------------------------
Done 256 Batches of 1085
class_loss = 0.07326676696538925
macro_f1 = 0.9647548802115476
weighted_f1 = 0.9647737843034181
regression_loss = 0.18646620213985443
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00042153846153846153, 8.430769230769231e-05]
-------------------------------------------------
Done 288 Batches of 1085
class_loss = 0.1662701964378357
macro_f1 = 0.9648578550950436
weighted_f1 = 0.9648760782646016
regression_loss = 0.2938525676727295
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00042, 8.4e-05]
-------------------------------------------------
Done 320 Batches of 1085
class_loss = 0.06448842585086823
macro_f1 = 0.965095330655461
weighted_f1 = 0.9650965916549703
regression_loss = 0.21183358132839203
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00041846153846153845, 8.369230769230769e-05]
-------------------------------------------------
Done 352 Batches of 1085
class_loss = 0.1156240701675415
macro_f1 = 0.9649405113931525
weighted_f1 = 0.964922426641084
regression_loss = 0.2287580668926239
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0004169230769230769, 8.338461538461538e-05]
-------------------------------------------------
Done 384 Batches of 1085
class_loss = 0.07314951717853546
macro_f1 = 0.9646486103307987
weighted_f1 = 0.9646667070316293
regression_loss = 0.2505425214767456
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0004153846153846154, 8.307692307692309e-05]
-------------------------------------------------
Done 416 Batches of 1085
class_loss = 0.1670081615447998
macro_f1 = 0.9648712024457449
weighted_f1 = 0.9649204546783196
regression_loss = 0.20570631325244904
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00041384615384615386, 8.276923076923077e-05]
-------------------------------------------------
Done 448 Batches of 1085
class_loss = 0.1403902769088745
macro_f1 = 0.964175837441372
weighted_f1 = 0.9642130167158482
regression_loss = 0.2615088224411011
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00041230769230769234, 8.246153846153847e-05]
-------------------------------------------------
Done 480 Batches of 1085
class_loss = 0.06544695794582367
macro_f1 = 0.963806988091956
weighted_f1 = 0.9638493803496002
regression_loss = 0.19825869798660278
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0004107692307692308, 8.215384615384615e-05]
-------------------------------------------------
Done 512 Batches of 1085
class_loss = 0.14468291401863098
macro_f1 = 0.9637600541073891
weighted_f1 = 0.9638166431822506
regression_loss = 0.23160134255886078
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00040923076923076926, 8.184615384615385e-05]
-------------------------------------------------
Done 544 Batches of 1085
class_loss = 0.13388271629810333
macro_f1 = 0.963260812959313
weighted_f1 = 0.9632960362592476
regression_loss = 0.22297443449497223
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0004076923076923077, 8.153846153846155e-05]
-------------------------------------------------
Done 576 Batches of 1085
class_loss = 0.10149376839399338
macro_f1 = 0.9632565340139845
weighted_f1 = 0.9632860918931455
regression_loss = 0.19093766808509827
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0004061538461538461, 8.123076923076923e-05]
-------------------------------------------------
Done 608 Batches of 1085
class_loss = 0.1049569621682167
macro_f1 = 0.9630869652125446
weighted_f1 = 0.9631071936966558
regression_loss = 0.28189292550086975
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0004046153846153846, 8.092307692307693e-05]
-------------------------------------------------
Done 640 Batches of 1085
class_loss = 0.26502540707588196
macro_f1 = 0.9627217961794639
weighted_f1 = 0.9627386844695136
regression_loss = 0.20290309190750122
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00040307692307692305, 8.061538461538461e-05]
-------------------------------------------------
Done 672 Batches of 1085
class_loss = 0.11878831684589386
macro_f1 = 0.9624051749447193
weighted_f1 = 0.9624196252384294
regression_loss = 0.20752930641174316
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00040153846153846153, 8.030769230769231e-05]
-------------------------------------------------
Done 704 Batches of 1085
class_loss = 0.08102146536111832
macro_f1 = 0.9622992636365051
weighted_f1 = 0.9623198793173255
regression_loss = 0.26348286867141724
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0004, 8e-05]
-------------------------------------------------
Done 736 Batches of 1085
class_loss = 0.19935184717178345
macro_f1 = 0.9622518266663793
weighted_f1 = 0.9622592956817987
regression_loss = 0.21524852514266968
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0003984615384615385, 7.96923076923077e-05]
-------------------------------------------------
Done 768 Batches of 1085
class_loss = 0.11275531351566315
macro_f1 = 0.9618342933645003
weighted_f1 = 0.9618341801764773
regression_loss = 0.23306140303611755
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00039692307692307694, 7.938461538461539e-05]
-------------------------------------------------
Done 800 Batches of 1085
class_loss = 0.10713732242584229
macro_f1 = 0.9617012319218452
weighted_f1 = 0.9617090732834757
regression_loss = 0.23909199237823486
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0003953846153846154, 7.907692307692309e-05]
-------------------------------------------------
Done 832 Batches of 1085
class_loss = 0.07246596366167068
macro_f1 = 0.9616122621678219
weighted_f1 = 0.9616213409381731
regression_loss = 0.22187069058418274
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00039384615384615386, 7.876923076923077e-05]
-------------------------------------------------
Done 864 Batches of 1085
class_loss = 0.1011417955160141
macro_f1 = 0.9617207702493573
weighted_f1 = 0.9617400939968235
regression_loss = 0.2159622609615326
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0003923076923076923, 7.846153846153847e-05]
-------------------------------------------------
Done 896 Batches of 1085
class_loss = 0.09129641950130463
macro_f1 = 0.961412095005994
weighted_f1 = 0.9614335700225974
regression_loss = 0.18897613883018494
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0003907692307692308, 7.815384615384615e-05]
-------------------------------------------------
Done 928 Batches of 1085
class_loss = 0.06167412921786308
macro_f1 = 0.9613328691320768
weighted_f1 = 0.9613498085371107
regression_loss = 0.2881210446357727
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0003892307692307692, 7.784615384615385e-05]
-------------------------------------------------
Done 960 Batches of 1085
class_loss = 0.15456105768680573
macro_f1 = 0.9612398249519686
weighted_f1 = 0.9612518227797898
regression_loss = 0.1835900992155075
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0003876923076923077, 7.753846153846153e-05]
-------------------------------------------------
Done 992 Batches of 1085
class_loss = 0.0930776447057724
macro_f1 = 0.9612115159061421
weighted_f1 = 0.9612182454725761
regression_loss = 0.2168741226196289
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0003861538461538462, 7.723076923076924e-05]
-------------------------------------------------
Done 1024 Batches of 1085
class_loss = 0.1545913964509964
macro_f1 = 0.9609219290177821
weighted_f1 = 0.9609267632975694
regression_loss = 0.2999451160430908
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00038461538461538467, 7.692307692307693e-05]
-------------------------------------------------
Done 1056 Batches of 1085
class_loss = 0.10205307602882385
macro_f1 = 0.9607356172607204
weighted_f1 = 0.9607397905673252
regression_loss = 0.3224930167198181
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0003830769230769231, 7.661538461538462e-05]
-------------------------------------------------
Done 1085 Batches of 1085
class_loss = 0.08117654919624329
macro_f1 = 0.960570022771862
weighted_f1 = 0.9605727001052075
regression_loss = 0.09070082753896713
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00038153846153846153, 7.630769230769231e-05]
1.6054500341415405
1.0921508073806763
1.1426594257354736
1.3554813861846924
1.3085476160049438
0.8451473116874695
1.1182091236114502
1.5001782178878784
1.286358118057251
1.4728809595108032
1.1951016187667847
1.2276660203933716
1.2580608129501343
1.4369398355484009
1.3807172775268555
1.5844488143920898
1.2424750328063965
1.4396932125091553
1.2081347703933716
1.094316005706787
0.9883528351783752
1.1992079019546509
1.465339183807373
1.2450271844863892
1.241268515586853
1.504353404045105
1.3662680387496948
0.9922739267349243
1.3583062887191772
1.4129822254180908
1.4735499620437622
1.2687056064605713
0.781888484954834
1.5022125244140625
1.4052534103393555
1.3662526607513428
1.2522482872009277
1.4870673418045044
1.3108887672424316
1.3871716260910034
0.9104266166687012
1.1439361572265625
1.2646057605743408
1.4660632610321045
1.6301279067993164
1.2570139169692993
1.3588899374008179
1.2675697803497314
1.2037357091903687
1.340637445449829
1.2348920106887817
1.0230976343154907
1.3706529140472412
1.2287691831588745
1.7730721235275269
1.2265862226486206
1.5202734470367432
1.0508246421813965
0.8741586804389954
1.0020170211791992
1.622880220413208
1.3618439435958862
1.304479718208313
1.6741496324539185
Epoch 10, training metrics:
--- CLASSIFICATION METRICS --- 
F1 scores: [Macro=0.961, Weighted=0.961] 
Balanced Accuracy = 0.961
Average loss = 0.17045
Epoch 10, validation metrics:
--- CLASSIFICATION METRICS --- 
F1 scores: [Macro=0.748, Weighted=0.748] 
Balanced Accuracy = 0.748
Average loss = 1.29553
avg_train_loss 0.17045345920983548
avg_val_loss 1.2955302884802222
-------------------------------------------------
Done 32 Batches of 1085
class_loss = 0.05756129324436188
macro_f1 = 0.9741991238635024
weighted_f1 = 0.9743070107505852
regression_loss = 0.29132187366485596
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00038, 7.6e-05]
-------------------------------------------------
Done 64 Batches of 1085
class_loss = 0.05331858620047569
macro_f1 = 0.9743096830753368
weighted_f1 = 0.9743377072144944
regression_loss = 0.23902921378612518
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00037846153846153845, 7.56923076923077e-05]
-------------------------------------------------
Done 96 Batches of 1085
class_loss = 0.02175469882786274
macro_f1 = 0.9734886223030628
weighted_f1 = 0.9735124826775693
regression_loss = 0.21033798158168793
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00037692307692307694, 7.538461538461539e-05]
-------------------------------------------------
Done 128 Batches of 1085
class_loss = 0.06698732823133469
macro_f1 = 0.9755415323518941
weighted_f1 = 0.9755148545430721
regression_loss = 0.23839977383613586
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00037538461538461537, 7.507692307692308e-05]
-------------------------------------------------
Done 160 Batches of 1085
class_loss = 0.10203155130147934
macro_f1 = 0.9751995912770909
weighted_f1 = 0.9751817186350628
regression_loss = 0.24996976554393768
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00037384615384615386, 7.476923076923077e-05]
-------------------------------------------------
Done 192 Batches of 1085
class_loss = 0.09996336698532104
macro_f1 = 0.9754419592197721
weighted_f1 = 0.9754508498332728
regression_loss = 0.20694419741630554
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0003723076923076923, 7.446153846153846e-05]
-------------------------------------------------
Done 224 Batches of 1085
class_loss = 0.08967364579439163
macro_f1 = 0.9753403854127871
weighted_f1 = 0.9753607259645742
regression_loss = 0.2630998492240906
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0003707692307692308, 7.415384615384616e-05]
-------------------------------------------------
Done 256 Batches of 1085
class_loss = 0.07253023982048035
macro_f1 = 0.975639063666876
weighted_f1 = 0.9756620030138793
regression_loss = 0.22059768438339233
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00036923076923076927, 7.384615384615386e-05]
-------------------------------------------------
Done 288 Batches of 1085
class_loss = 0.0738295242190361
macro_f1 = 0.9760966302972761
weighted_f1 = 0.9761429898559377
regression_loss = 0.16890519857406616
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0003676923076923077, 7.353846153846154e-05]
-------------------------------------------------
Done 320 Batches of 1085
class_loss = 0.07879547029733658
macro_f1 = 0.9755938312494077
weighted_f1 = 0.9756528530384954
regression_loss = 0.22958879172801971
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0003661538461538462, 7.323076923076924e-05]
-------------------------------------------------
Done 352 Batches of 1085
class_loss = 0.11864293366670609
macro_f1 = 0.9753482364344656
weighted_f1 = 0.97538327993306
regression_loss = 0.2029639333486557
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0003646153846153846, 7.292307692307692e-05]
-------------------------------------------------
Done 384 Batches of 1085
class_loss = 0.12307923287153244
macro_f1 = 0.9753576773416178
weighted_f1 = 0.9753979711418623
regression_loss = 0.22586584091186523
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0003630769230769231, 7.261538461538462e-05]
-------------------------------------------------
Done 416 Batches of 1085
class_loss = 0.09738152474164963
macro_f1 = 0.9742140128604546
weighted_f1 = 0.97425766364486
regression_loss = 0.23199760913848877
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00036153846153846154, 7.23076923076923e-05]
-------------------------------------------------
Done 448 Batches of 1085
class_loss = 0.01824740692973137
macro_f1 = 0.9741550254386941
weighted_f1 = 0.97419253321299
regression_loss = 0.21015392243862152
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00035999999999999997, 7.2e-05]
-------------------------------------------------
Done 480 Batches of 1085
class_loss = 0.09513191878795624
macro_f1 = 0.9741023900194659
weighted_f1 = 0.9741255086451573
regression_loss = 0.17607393860816956
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00035846153846153846, 7.169230769230769e-05]
-------------------------------------------------
Done 512 Batches of 1085
class_loss = 0.10935709625482559
macro_f1 = 0.9739200804879289
weighted_f1 = 0.9739453508004661
regression_loss = 0.24021649360656738
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0003569230769230769, 7.138461538461538e-05]
-------------------------------------------------
Done 544 Batches of 1085
class_loss = 0.07006058096885681
macro_f1 = 0.973741701514437
weighted_f1 = 0.9737521942352292
regression_loss = 0.2564816176891327
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00035538461538461543, 7.107692307692308e-05]
-------------------------------------------------
Done 576 Batches of 1085
class_loss = 0.1504904329776764
macro_f1 = 0.9738556986847404
weighted_f1 = 0.9738530021222319
regression_loss = 0.19954684376716614
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00035384615384615386, 7.076923076923078e-05]
-------------------------------------------------
Done 608 Batches of 1085
class_loss = 0.049884896725416183
macro_f1 = 0.9737510019362683
weighted_f1 = 0.9737618738171954
regression_loss = 0.1923525333404541
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00035230769230769235, 7.046153846153846e-05]
-------------------------------------------------
Done 640 Batches of 1085
class_loss = 0.04801589995622635
macro_f1 = 0.973905484968672
weighted_f1 = 0.9739144662252467
regression_loss = 0.2354293316602707
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0003507692307692308, 7.015384615384616e-05]
-------------------------------------------------
Done 672 Batches of 1085
class_loss = 0.11735422909259796
macro_f1 = 0.9737833094547955
weighted_f1 = 0.9737848892712283
regression_loss = 0.2532537579536438
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00034923076923076927, 6.984615384615386e-05]
-------------------------------------------------
Done 704 Batches of 1085
class_loss = 0.1904139220714569
macro_f1 = 0.973551009605972
weighted_f1 = 0.973554218685888
regression_loss = 0.2960297763347626
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0003476923076923077, 6.953846153846154e-05]
-------------------------------------------------
Done 736 Batches of 1085
class_loss = 0.10777106881141663
macro_f1 = 0.9734449335775777
weighted_f1 = 0.9734496895043808
regression_loss = 0.2787708044052124
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00034615384615384613, 6.923076923076924e-05]
-------------------------------------------------
Done 768 Batches of 1085
class_loss = 0.13721491396427155
macro_f1 = 0.9730306621638036
weighted_f1 = 0.9730390665128582
regression_loss = 0.2566806674003601
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0003446153846153846, 6.892307692307692e-05]
-------------------------------------------------
Done 800 Batches of 1085
class_loss = 0.1079406589269638
macro_f1 = 0.972763963424172
weighted_f1 = 0.9727691437547826
regression_loss = 0.20798751711845398
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00034307692307692305, 6.861538461538462e-05]
-------------------------------------------------
Done 832 Batches of 1085
class_loss = 0.07717235386371613
macro_f1 = 0.972523217357061
weighted_f1 = 0.9725202280130498
regression_loss = 0.3243119418621063
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00034153846153846154, 6.83076923076923e-05]
-------------------------------------------------
Done 864 Batches of 1085
class_loss = 0.08507587760686874
macro_f1 = 0.9724171284125479
weighted_f1 = 0.9724168549394843
regression_loss = 0.21264664828777313
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00034, 6.800000000000001e-05]
-------------------------------------------------
Done 896 Batches of 1085
class_loss = 0.0952572226524353
macro_f1 = 0.9723588078229908
weighted_f1 = 0.972346871996592
regression_loss = 0.21849818527698517
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0003384615384615385, 6.76923076923077e-05]
-------------------------------------------------
Done 928 Batches of 1085
class_loss = 0.1276473104953766
macro_f1 = 0.9721065014969502
weighted_f1 = 0.9720956148347557
regression_loss = 0.19618147611618042
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00033692307692307694, 6.73846153846154e-05]
-------------------------------------------------
Done 960 Batches of 1085
class_loss = 0.10125521570444107
macro_f1 = 0.9719752054322048
weighted_f1 = 0.9719665010033546
regression_loss = 0.2360808104276657
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0003353846153846154, 6.707692307692308e-05]
-------------------------------------------------
Done 992 Batches of 1085
class_loss = 0.08348052203655243
macro_f1 = 0.9719399311195673
weighted_f1 = 0.9719413512187042
regression_loss = 0.23588624596595764
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00033384615384615386, 6.676923076923078e-05]
-------------------------------------------------
Done 1024 Batches of 1085
class_loss = 0.12487092614173889
macro_f1 = 0.971910542215601
weighted_f1 = 0.9719089701778477
regression_loss = 0.24722832441329956
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0003323076923076923, 6.646153846153846e-05]
-------------------------------------------------
Done 1056 Batches of 1085
class_loss = 0.12691019475460052
macro_f1 = 0.971845957470555
weighted_f1 = 0.9718433468732591
regression_loss = 0.24452942609786987
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0003307692307692308, 6.615384615384616e-05]
-------------------------------------------------
Done 1085 Batches of 1085
class_loss = 0.057214073836803436
macro_f1 = 0.9717657320612743
weighted_f1 = 0.9717612225408825
regression_loss = 0.3386023938655853
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0003292307692307692, 6.584615384615384e-05]
1.639255404472351
1.1706511974334717
1.081634759902954
1.3829479217529297
1.0630689859390259
0.865680992603302
1.3934482336044312
1.6462193727493286
1.2241554260253906
1.6562658548355103
1.0991086959838867
1.2186777591705322
1.3838454484939575
1.6912707090377808
1.225606918334961
1.3693766593933105
1.2835800647735596
1.5878790616989136
1.1477059125900269
1.1085822582244873
1.1233853101730347
1.1636040210723877
1.6858842372894287
1.1455034017562866
1.336000680923462
1.7030302286148071
1.3430598974227905
1.1476845741271973
1.3091188669204712
1.563963532447815
1.7339216470718384
1.3571803569793701
0.8899260759353638
1.7037899494171143
1.4498616456985474
1.5669665336608887
1.1140257120132446
1.6717580556869507
1.4254239797592163
1.560843825340271
0.7512475252151489
1.0236014127731323
1.1186844110488892
1.5106401443481445
1.7567939758300781
1.1862781047821045
1.3035058975219727
1.4915308952331543
1.3833277225494385
1.550594687461853
1.4462330341339111
0.9326556324958801
1.5403213500976562
1.278472661972046
1.9387978315353394
1.1439248323440552
1.663278341293335
0.9181990623474121
0.8517829775810242
1.2060630321502686
1.7798831462860107
1.2972333431243896
1.3645573854446411
1.7065993547439575
Epoch 11, training metrics:
--- CLASSIFICATION METRICS --- 
F1 scores: [Macro=0.972, Weighted=0.972] 
Balanced Accuracy = 0.972
Average loss = 0.13696
Epoch 11, validation metrics:
--- CLASSIFICATION METRICS --- 
F1 scores: [Macro=0.752, Weighted=0.752] 
Balanced Accuracy = 0.752
Average loss = 1.34966
avg_train_loss 0.13696320667038864
avg_val_loss 1.3496578270569444
-------------------------------------------------
Done 32 Batches of 1085
class_loss = 0.08190138638019562
macro_f1 = 0.9828151526497612
weighted_f1 = 0.9828628015263271
regression_loss = 0.21046072244644165
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0003276923076923077, 6.553846153846154e-05]
-------------------------------------------------
Done 64 Batches of 1085
class_loss = 0.04063721001148224
macro_f1 = 0.9824506959306494
weighted_f1 = 0.9823887365333617
regression_loss = 0.2437419593334198
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00032615384615384613, 6.523076923076923e-05]
-------------------------------------------------
Done 96 Batches of 1085
class_loss = 0.06279707700014114
macro_f1 = 0.9823595844748777
weighted_f1 = 0.9823914865765062
regression_loss = 0.24541452527046204
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0003246153846153846, 6.492307692307693e-05]
-------------------------------------------------
Done 128 Batches of 1085
class_loss = 0.17529232800006866
macro_f1 = 0.9819353715052686
weighted_f1 = 0.9819022979673432
regression_loss = 0.2337174415588379
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0003230769230769231, 6.461538461538462e-05]
-------------------------------------------------
Done 160 Batches of 1085
class_loss = 0.05721781402826309
macro_f1 = 0.9818019505864971
weighted_f1 = 0.9817146574114975
regression_loss = 0.27588486671447754
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00032153846153846154, 6.430769230769231e-05]
-------------------------------------------------
Done 192 Batches of 1085
class_loss = 0.049885064363479614
macro_f1 = 0.9815114807261696
weighted_f1 = 0.9814226080442601
regression_loss = 0.2088598757982254
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00032, 6.400000000000001e-05]
-------------------------------------------------
Done 224 Batches of 1085
class_loss = 0.09466934949159622
macro_f1 = 0.981250673446822
weighted_f1 = 0.9811786313591221
regression_loss = 0.2270510494709015
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00031846153846153846, 6.36923076923077e-05]
-------------------------------------------------
Done 256 Batches of 1085
class_loss = 0.06906218081712723
macro_f1 = 0.9811963430599604
weighted_f1 = 0.9811493960791124
regression_loss = 0.2699452042579651
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00031692307692307695, 6.338461538461539e-05]
-------------------------------------------------
Done 288 Batches of 1085
class_loss = 0.029407214373350143
macro_f1 = 0.9811441515100903
weighted_f1 = 0.9811036856658157
regression_loss = 0.19436126947402954
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0003153846153846154, 6.307692307692308e-05]
-------------------------------------------------
Done 320 Batches of 1085
class_loss = 0.07073875516653061
macro_f1 = 0.9808380101921887
weighted_f1 = 0.9807971975929579
regression_loss = 0.23264779150485992
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0003138461538461538, 6.276923076923077e-05]
-------------------------------------------------
Done 352 Batches of 1085
class_loss = 0.03851430490612984
macro_f1 = 0.9804796186048715
weighted_f1 = 0.9804554370699715
regression_loss = 0.189458429813385
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0003123076923076923, 6.246153846153846e-05]
-------------------------------------------------
Done 384 Batches of 1085
class_loss = 0.028796140104532242
macro_f1 = 0.9804043365311574
weighted_f1 = 0.9803729912578713
regression_loss = 0.21838951110839844
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00031076923076923073, 6.215384615384615e-05]
-------------------------------------------------
Done 416 Batches of 1085
class_loss = 0.04163268581032753
macro_f1 = 0.9806509601610658
weighted_f1 = 0.980623074544527
regression_loss = 0.1796744465827942
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00030923076923076927, 6.184615384615385e-05]
-------------------------------------------------
Done 448 Batches of 1085
class_loss = 0.09986011683940887
macro_f1 = 0.980684774907265
weighted_f1 = 0.9806626862690275
regression_loss = 0.20029599964618683
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0003076923076923077, 6.153846153846155e-05]
-------------------------------------------------
Done 480 Batches of 1085
class_loss = 0.05621318891644478
macro_f1 = 0.9799130087498629
weighted_f1 = 0.9799025432034777
regression_loss = 0.2503167986869812
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0003061538461538462, 6.123076923076923e-05]
-------------------------------------------------
Done 512 Batches of 1085
class_loss = 0.048907335847616196
macro_f1 = 0.979902236492736
weighted_f1 = 0.9798947168558826
regression_loss = 0.22346031665802002
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0003046153846153846, 6.092307692307693e-05]
-------------------------------------------------
Done 544 Batches of 1085
class_loss = 0.05088723823428154
macro_f1 = 0.9798949818778364
weighted_f1 = 0.9798847171092419
regression_loss = 0.2119702696800232
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0003030769230769231, 6.061538461538462e-05]
-------------------------------------------------
Done 576 Batches of 1085
class_loss = 0.07867461442947388
macro_f1 = 0.9799160406743395
weighted_f1 = 0.9799156017038915
regression_loss = 0.20928126573562622
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00030153846153846154, 6.030769230769231e-05]
-------------------------------------------------
Done 608 Batches of 1085
class_loss = 0.0666203424334526
macro_f1 = 0.9799971509568408
weighted_f1 = 0.9799957477197077
regression_loss = 0.264662504196167
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0003, 6e-05]
-------------------------------------------------
Done 640 Batches of 1085
class_loss = 0.07591528445482254
macro_f1 = 0.9800979674421143
weighted_f1 = 0.9801050580057096
regression_loss = 0.2242990881204605
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00029846153846153846, 5.969230769230769e-05]
-------------------------------------------------
Done 672 Batches of 1085
class_loss = 0.04834093898534775
macro_f1 = 0.9798848685932476
weighted_f1 = 0.9798898513553056
regression_loss = 0.2385442852973938
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0002969230769230769, 5.938461538461538e-05]
-------------------------------------------------
Done 704 Batches of 1085
class_loss = 0.09138145297765732
macro_f1 = 0.9799608779880884
weighted_f1 = 0.9799821168293547
regression_loss = 0.20477555692195892
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00029538461538461543, 5.907692307692309e-05]
-------------------------------------------------
Done 736 Batches of 1085
class_loss = 0.016675448045134544
macro_f1 = 0.9798914594025281
weighted_f1 = 0.9799178998909334
regression_loss = 0.22265540063381195
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00029384615384615387, 5.876923076923078e-05]
-------------------------------------------------
Done 768 Batches of 1085
class_loss = 0.12003450840711594
macro_f1 = 0.9796577397168214
weighted_f1 = 0.9796861648813027
regression_loss = 0.19750940799713135
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00029230769230769235, 5.846153846153847e-05]
-------------------------------------------------
Done 800 Batches of 1085
class_loss = 0.09495195746421814
macro_f1 = 0.9795817997493668
weighted_f1 = 0.9796005732769405
regression_loss = 0.205852210521698
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0002907692307692308, 5.815384615384616e-05]
-------------------------------------------------
Done 832 Batches of 1085
class_loss = 0.02859899029135704
macro_f1 = 0.979500035794986
weighted_f1 = 0.9795127435375396
regression_loss = 0.31385520100593567
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0002892307692307692, 5.784615384615385e-05]
-------------------------------------------------
Done 864 Batches of 1085
class_loss = 0.034969206899404526
macro_f1 = 0.9794014682088301
weighted_f1 = 0.9794123200582404
regression_loss = 0.21751272678375244
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0002876923076923077, 5.753846153846154e-05]
-------------------------------------------------
Done 896 Batches of 1085
class_loss = 0.15233591198921204
macro_f1 = 0.979295665717116
weighted_f1 = 0.9793087205992426
regression_loss = 0.2259858101606369
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00028615384615384614, 5.723076923076923e-05]
-------------------------------------------------
Done 928 Batches of 1085
class_loss = 0.01884888857603073
macro_f1 = 0.9792698517045053
weighted_f1 = 0.9792728684069592
regression_loss = 0.2428346872329712
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0002846153846153846, 5.692307692307692e-05]
-------------------------------------------------
Done 960 Batches of 1085
class_loss = 0.05694155767560005
macro_f1 = 0.9792817171751305
weighted_f1 = 0.9792801128227823
regression_loss = 0.2103089690208435
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00028307692307692306, 5.661538461538461e-05]
-------------------------------------------------
Done 992 Batches of 1085
class_loss = 0.03643682971596718
macro_f1 = 0.9792964232346101
weighted_f1 = 0.9792958721707767
regression_loss = 0.21984396874904633
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00028153846153846154, 5.630769230769231e-05]
-------------------------------------------------
Done 1024 Batches of 1085
class_loss = 0.08768163621425629
macro_f1 = 0.9792176723547511
weighted_f1 = 0.9792183017556958
regression_loss = 0.26367294788360596
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00028000000000000003, 5.6000000000000006e-05]
-------------------------------------------------
Done 1056 Batches of 1085
class_loss = 0.08339600265026093
macro_f1 = 0.9791604973810097
weighted_f1 = 0.9791592589737261
regression_loss = 0.1902138590812683
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00027846153846153846, 5.5692307692307696e-05]
-------------------------------------------------
Done 1085 Batches of 1085
class_loss = 0.019436217844486237
macro_f1 = 0.9790547221507389
weighted_f1 = 0.9790521553749286
regression_loss = 0.18611495196819305
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00027692307692307695, 5.538461538461539e-05]
1.6662497520446777
1.0942364931106567
1.1984813213348389
1.2653424739837646
1.654134750366211
0.72408527135849
1.357304573059082
1.6821322441101074
1.33780837059021
1.729004144668579
1.3086166381835938
1.4962663650512695
1.361214280128479
1.5652073621749878
1.368730902671814
1.5138084888458252
1.4535771608352661
1.5614049434661865
1.5424522161483765
1.1724066734313965
1.107939600944519
1.1801421642303467
1.7379800081253052
1.1876038312911987
1.3706141710281372
1.7438254356384277
1.2910587787628174
1.3065906763076782
1.487633466720581
1.422303557395935
1.434799313545227
1.2378497123718262
0.9253100752830505
1.7878477573394775
1.4431129693984985
1.5141453742980957
1.2694209814071655
1.7454060316085815
1.4163689613342285
1.6238036155700684
0.961117684841156
1.0848982334136963
1.2265262603759766
1.4446766376495361
1.9694643020629883
1.217301845550537
1.3358367681503296
1.5066231489181519
1.422428846359253
1.599765658378601
1.2415271997451782
1.0494554042816162
1.6068037748336792
1.3144328594207764
1.8653430938720703
1.3954805135726929
1.581739068031311
1.0563538074493408
1.054835319519043
1.3628904819488525
1.807324767112732
1.4697173833847046
1.4597141742706299
1.6850119829177856
Epoch 12, training metrics:
--- CLASSIFICATION METRICS --- 
F1 scores: [Macro=0.979, Weighted=0.979] 
Balanced Accuracy = 0.979
Average loss = 0.11374
Epoch 12, validation metrics:
--- CLASSIFICATION METRICS --- 
F1 scores: [Macro=0.749, Weighted=0.749] 
Balanced Accuracy = 0.751
Average loss = 1.40634
avg_train_loss 0.11374487265395129
avg_val_loss 1.4063357831910253
-------------------------------------------------
Done 32 Batches of 1085
class_loss = 0.05429372563958168
macro_f1 = 0.9847456412748375
weighted_f1 = 0.9848815577858446
regression_loss = 0.21887566149234772
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0002753846153846154, 5.5076923076923084e-05]
-------------------------------------------------
Done 64 Batches of 1085
class_loss = 0.04172062873840332
macro_f1 = 0.986069363998839
weighted_f1 = 0.986112592987811
regression_loss = 0.19276607036590576
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00027384615384615387, 5.4769230769230775e-05]
-------------------------------------------------
Done 96 Batches of 1085
class_loss = 0.08834836632013321
macro_f1 = 0.9864389514404799
weighted_f1 = 0.986514976557391
regression_loss = 0.18540143966674805
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0002723076923076923, 5.4461538461538466e-05]
-------------------------------------------------
Done 128 Batches of 1085
class_loss = 0.05838221311569214
macro_f1 = 0.9871377725806514
weighted_f1 = 0.9872075002633444
regression_loss = 0.17710363864898682
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0002707692307692308, 5.4153846153846156e-05]
-------------------------------------------------
Done 160 Batches of 1085
class_loss = 0.040839750319719315
macro_f1 = 0.9873887807631186
weighted_f1 = 0.9874224059364465
regression_loss = 0.2406606674194336
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0002692307692307692, 5.384615384615385e-05]
-------------------------------------------------
Done 192 Batches of 1085
class_loss = 0.0696801096200943
macro_f1 = 0.9880499403284257
weighted_f1 = 0.9880968773391257
regression_loss = 0.17459742724895477
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0002676923076923077, 5.353846153846154e-05]
-------------------------------------------------
Done 224 Batches of 1085
class_loss = 0.0590209998190403
macro_f1 = 0.9876715740061502
weighted_f1 = 0.9877024497665857
regression_loss = 0.2406269609928131
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00026615384615384614, 5.323076923076923e-05]
-------------------------------------------------
Done 256 Batches of 1085
class_loss = 0.05085562542080879
macro_f1 = 0.987342095297285
weighted_f1 = 0.9873760427275488
regression_loss = 0.2490319013595581
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0002646153846153846, 5.292307692307693e-05]
-------------------------------------------------
Done 288 Batches of 1085
class_loss = 0.060810383409261703
macro_f1 = 0.987199317835817
weighted_f1 = 0.9872319118679315
regression_loss = 0.20356929302215576
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0002630769230769231, 5.261538461538462e-05]
-------------------------------------------------
Done 320 Batches of 1085
class_loss = 0.05075051635503769
macro_f1 = 0.9871527223349579
weighted_f1 = 0.9871664742092533
regression_loss = 0.197525292634964
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00026153846153846154, 5.230769230769231e-05]
-------------------------------------------------
Done 352 Batches of 1085
class_loss = 0.07107741385698318
macro_f1 = 0.9870345749462489
weighted_f1 = 0.9870449975990013
regression_loss = 0.271642804145813
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00026000000000000003, 5.2000000000000004e-05]
-------------------------------------------------
Done 384 Batches of 1085
class_loss = 0.05435918644070625
macro_f1 = 0.9870737732849098
weighted_f1 = 0.9870870712559867
regression_loss = 0.22338953614234924
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00025846153846153846, 5.1692307692307694e-05]
-------------------------------------------------
Done 416 Batches of 1085
class_loss = 0.07479123771190643
macro_f1 = 0.9870908917287121
weighted_f1 = 0.9871044437345031
regression_loss = 0.18177902698516846
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00025692307692307695, 5.1384615384615385e-05]
-------------------------------------------------
Done 448 Batches of 1085
class_loss = 0.040953949093818665
macro_f1 = 0.9871850144067935
weighted_f1 = 0.9872055886414484
regression_loss = 0.19291828572750092
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0002553846153846154, 5.1076923076923075e-05]
-------------------------------------------------
Done 480 Batches of 1085
class_loss = 0.10498353838920593
macro_f1 = 0.9869933421604612
weighted_f1 = 0.9870170506496462
regression_loss = 0.2668974995613098
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0002538461538461538, 5.0769230769230766e-05]
-------------------------------------------------
Done 512 Batches of 1085
class_loss = 0.058453019708395004
macro_f1 = 0.9869758073634015
weighted_f1 = 0.9869890515479333
regression_loss = 0.27216485142707825
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0002523076923076923, 5.046153846153846e-05]
-------------------------------------------------
Done 544 Batches of 1085
class_loss = 0.015046896412968636
macro_f1 = 0.9869121585022054
weighted_f1 = 0.986921053763924
regression_loss = 0.21082091331481934
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00025076923076923073, 5.0153846153846154e-05]
-------------------------------------------------
Done 576 Batches of 1085
class_loss = 0.03975353017449379
macro_f1 = 0.9868200606857368
weighted_f1 = 0.986834161309887
regression_loss = 0.3069484829902649
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0002492307692307692, 4.984615384615385e-05]
-------------------------------------------------
Done 608 Batches of 1085
class_loss = 0.03215524181723595
macro_f1 = 0.9866480452053553
weighted_f1 = 0.9866530675313042
regression_loss = 0.21368853747844696
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0002476923076923077, 4.953846153846154e-05]
-------------------------------------------------
Done 640 Batches of 1085
class_loss = 0.037742216140031815
macro_f1 = 0.986704312720492
weighted_f1 = 0.9867103172285822
regression_loss = 0.20509016513824463
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0002461538461538462, 4.923076923076924e-05]
-------------------------------------------------
Done 672 Batches of 1085
class_loss = 0.03902214765548706
macro_f1 = 0.9866519111817631
weighted_f1 = 0.9866570766343754
regression_loss = 0.22367073595523834
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00024461538461538463, 4.892307692307693e-05]
-------------------------------------------------
Done 704 Batches of 1085
class_loss = 0.11488828808069229
macro_f1 = 0.9864036240828143
weighted_f1 = 0.9864080744976281
regression_loss = 0.25839346647262573
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0002430769230769231, 4.861538461538462e-05]
-------------------------------------------------
Done 736 Batches of 1085
class_loss = 0.014452334493398666
macro_f1 = 0.9864931201674205
weighted_f1 = 0.9865000989020016
regression_loss = 0.25725191831588745
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00024153846153846155, 4.830769230769231e-05]
-------------------------------------------------
Done 768 Batches of 1085
class_loss = 0.026212027296423912
macro_f1 = 0.9864645380348749
weighted_f1 = 0.9864727653398807
regression_loss = 0.20370668172836304
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00024, 4.8e-05]
-------------------------------------------------
Done 800 Batches of 1085
class_loss = 0.0796162486076355
macro_f1 = 0.9865015513044553
weighted_f1 = 0.9865061304571204
regression_loss = 0.17607945203781128
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0002384615384615385, 4.76923076923077e-05]
-------------------------------------------------
Done 832 Batches of 1085
class_loss = 0.05210147425532341
macro_f1 = 0.9865611742230309
weighted_f1 = 0.986565295711417
regression_loss = 0.2103230357170105
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00023692307692307693, 4.738461538461539e-05]
-------------------------------------------------
Done 864 Batches of 1085
class_loss = 0.07040490955114365
macro_f1 = 0.9865163250629401
weighted_f1 = 0.986520232687814
regression_loss = 0.2079888880252838
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00023538461538461538, 4.707692307692308e-05]
-------------------------------------------------
Done 896 Batches of 1085
class_loss = 0.0771317332983017
macro_f1 = 0.9865268560188338
weighted_f1 = 0.9865307998609557
regression_loss = 0.1893172413110733
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00023384615384615384, 4.676923076923077e-05]
-------------------------------------------------
Done 928 Batches of 1085
class_loss = 0.022335859015583992
macro_f1 = 0.9864397204479026
weighted_f1 = 0.9864395759117277
regression_loss = 0.20490577816963196
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0002323076923076923, 4.646153846153846e-05]
-------------------------------------------------
Done 960 Batches of 1085
class_loss = 0.018723227083683014
macro_f1 = 0.9864229922569642
weighted_f1 = 0.986427498369462
regression_loss = 0.21956494450569153
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0002307692307692308, 4.615384615384616e-05]
-------------------------------------------------
Done 992 Batches of 1085
class_loss = 0.011378278024494648
macro_f1 = 0.9863968757262681
weighted_f1 = 0.9864005090802462
regression_loss = 0.2926786541938782
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00022923076923076925, 4.584615384615385e-05]
-------------------------------------------------
Done 1024 Batches of 1085
class_loss = 0.03370663896203041
macro_f1 = 0.9864272232638724
weighted_f1 = 0.9864287612256362
regression_loss = 0.2110723853111267
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0002276923076923077, 4.553846153846154e-05]
-------------------------------------------------
Done 1056 Batches of 1085
class_loss = 0.01624387688934803
macro_f1 = 0.9864057602560814
weighted_f1 = 0.9864036660380368
regression_loss = 0.29755115509033203
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00022615384615384614, 4.523076923076923e-05]
-------------------------------------------------
Done 1085 Batches of 1085
class_loss = 0.005709121935069561
macro_f1 = 0.9862662025500416
weighted_f1 = 0.986262851519222
regression_loss = 0.14544470608234406
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0002246153846153846, 4.492307692307692e-05]
1.644293189048767
1.179080605506897
1.2704452276229858
1.4555431604385376
1.655663013458252
0.9568065404891968
1.2857980728149414
1.890076994895935
1.2628880739212036
1.795135259628296
1.2617623805999756
1.6790019273757935
1.460227370262146
1.7265080213546753
1.6050595045089722
1.648421287536621
1.6031419038772583
1.6079719066619873
1.4433382749557495
1.322047472000122
1.3136308193206787
1.221529245376587
1.7001399993896484
1.304565668106079
1.2111388444900513
1.8106613159179688
1.4974459409713745
1.284042477607727
1.6151399612426758
1.4567570686340332
1.5841070413589478
1.377197027206421
1.0120103359222412
1.856787085533142
1.6490973234176636
1.7707006931304932
1.4797964096069336
1.6313462257385254
1.3451040983200073
1.717971920967102
1.092013955116272
1.045365333557129
1.193489909172058
1.6183865070343018
1.8054351806640625
1.157346487045288
1.5301519632339478
1.5168266296386719
1.5789350271224976
1.62625253200531
1.4233707189559937
1.0826352834701538
1.7113665342330933
1.2472807168960571
2.009089946746826
1.4935364723205566
1.6661655902862549
1.1723603010177612
1.1107996702194214
1.3239543437957764
1.869940996170044
1.6427818536758423
1.6115134954452515
1.7446452379226685
Epoch 13, training metrics:
--- CLASSIFICATION METRICS --- 
F1 scores: [Macro=0.986, Weighted=0.986] 
Balanced Accuracy = 0.986
Average loss = 0.09171
Epoch 13, validation metrics:
--- CLASSIFICATION METRICS --- 
F1 scores: [Macro=0.752, Weighted=0.752] 
Balanced Accuracy = 0.753
Average loss = 1.48228
avg_train_loss 0.0917120082003455
avg_val_loss 1.482281630858779
-------------------------------------------------
Done 32 Batches of 1085
class_loss = 0.03540083020925522
macro_f1 = 0.9920260107897738
weighted_f1 = 0.9919334314705578
regression_loss = 0.19567805528640747
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0002230769230769231, 4.461538461538462e-05]
-------------------------------------------------
Done 64 Batches of 1085
class_loss = 0.04426373168826103
macro_f1 = 0.991843987385962
weighted_f1 = 0.9918196049821473
regression_loss = 0.2204408496618271
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00022153846153846155, 4.430769230769231e-05]
-------------------------------------------------
Done 96 Batches of 1085
class_loss = 0.022973278537392616
macro_f1 = 0.9911251502045104
weighted_f1 = 0.9911213271710599
regression_loss = 0.227340430021286
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00022, 4.4000000000000006e-05]
-------------------------------------------------
Done 128 Batches of 1085
class_loss = 0.0461781844496727
macro_f1 = 0.991358712828893
weighted_f1 = 0.9913877310867589
regression_loss = 0.283103883266449
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00021846153846153847, 4.3692307692307696e-05]
-------------------------------------------------
Done 160 Batches of 1085
class_loss = 0.010660206899046898
macro_f1 = 0.9914903028372244
weighted_f1 = 0.9914978429517253
regression_loss = 0.24788305163383484
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00021692307692307693, 4.338461538461539e-05]
-------------------------------------------------
Done 192 Batches of 1085
class_loss = 0.03694263473153114
macro_f1 = 0.9916028450828863
weighted_f1 = 0.9916126005654096
regression_loss = 0.18742337822914124
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00021538461538461541, 4.3076923076923084e-05]
-------------------------------------------------
Done 224 Batches of 1085
class_loss = 0.052405551075935364
macro_f1 = 0.9914298359338263
weighted_f1 = 0.9914504356629145
regression_loss = 0.21723806858062744
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00021384615384615385, 4.2769230769230775e-05]
-------------------------------------------------
Done 256 Batches of 1085
class_loss = 0.008324244059622288
macro_f1 = 0.9917118603810267
weighted_f1 = 0.9917264947564949
regression_loss = 0.2715352177619934
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0002123076923076923, 4.2461538461538465e-05]
-------------------------------------------------
Done 288 Batches of 1085
class_loss = 0.03604000061750412
macro_f1 = 0.9919879608623526
weighted_f1 = 0.9919949534288806
regression_loss = 0.2849075198173523
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00021076923076923077, 4.2153846153846156e-05]
-------------------------------------------------
Done 320 Batches of 1085
class_loss = 0.02504257671535015
macro_f1 = 0.991859070902691
weighted_f1 = 0.9918673056372044
regression_loss = 0.24797827005386353
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00020923076923076922, 4.1846153846153846e-05]
-------------------------------------------------
Done 352 Batches of 1085
class_loss = 0.06042062118649483
macro_f1 = 0.9918626780465765
weighted_f1 = 0.9918746657001629
regression_loss = 0.22320833802223206
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0002076923076923077, 4.1538461538461544e-05]
-------------------------------------------------
Done 384 Batches of 1085
class_loss = 0.009732174687087536
macro_f1 = 0.9917674542346873
weighted_f1 = 0.9917787394531521
regression_loss = 0.2197868824005127
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00020615384615384617, 4.1230769230769234e-05]
-------------------------------------------------
Done 416 Batches of 1085
class_loss = 0.06373421102762222
macro_f1 = 0.9918730835761533
weighted_f1 = 0.9918858444737433
regression_loss = 0.19595210254192352
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00020461538461538463, 4.0923076923076925e-05]
-------------------------------------------------
Done 448 Batches of 1085
class_loss = 0.02001139149069786
macro_f1 = 0.9917401611463348
weighted_f1 = 0.9917503481271348
regression_loss = 0.18848314881324768
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00020307692307692306, 4.0615384615384615e-05]
-------------------------------------------------
Done 480 Batches of 1085
class_loss = 0.006926604546606541
macro_f1 = 0.9917705695406316
weighted_f1 = 0.9917794087662549
regression_loss = 0.21572116017341614
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00020153846153846152, 4.0307692307692306e-05]
-------------------------------------------------
Done 512 Batches of 1085
class_loss = 0.011546332389116287
macro_f1 = 0.9917978990491205
weighted_f1 = 0.9918048175138241
regression_loss = 0.22569222748279572
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0002, 4e-05]
-------------------------------------------------
Done 544 Batches of 1085
class_loss = 0.04500902071595192
macro_f1 = 0.9918959693499593
weighted_f1 = 0.9918990113915753
regression_loss = 0.25580623745918274
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00019846153846153847, 3.9692307692307694e-05]
-------------------------------------------------
Done 576 Batches of 1085
class_loss = 0.01963709481060505
macro_f1 = 0.9917745111643835
weighted_f1 = 0.9917790444540068
regression_loss = 0.23194295167922974
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00019692307692307693, 3.9384615384615384e-05]
-------------------------------------------------
Done 608 Batches of 1085
class_loss = 0.021057749167084694
macro_f1 = 0.9918465272558008
weighted_f1 = 0.9918520146163821
regression_loss = 0.1734182983636856
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0001953846153846154, 3.9076923076923075e-05]
-------------------------------------------------
Done 640 Batches of 1085
class_loss = 0.01079657580703497
macro_f1 = 0.9918791296876532
weighted_f1 = 0.9918812523154461
regression_loss = 0.19172580540180206
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00019384615384615385, 3.8769230769230766e-05]
-------------------------------------------------
Done 672 Batches of 1085
class_loss = 0.012254924513399601
macro_f1 = 0.9918016499845488
weighted_f1 = 0.9918025894155775
regression_loss = 0.22344456613063812
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00019230769230769233, 3.846153846153846e-05]
-------------------------------------------------
Done 704 Batches of 1085
class_loss = 0.00221743481233716
macro_f1 = 0.9918166669748967
weighted_f1 = 0.9918201804750675
regression_loss = 0.20922645926475525
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00019076923076923077, 3.8153846153846153e-05]
-------------------------------------------------
Done 736 Batches of 1085
class_loss = 0.08657218515872955
macro_f1 = 0.9917153930037028
weighted_f1 = 0.9917191876659232
regression_loss = 0.26259908080101013
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00018923076923076923, 3.784615384615385e-05]
-------------------------------------------------
Done 768 Batches of 1085
class_loss = 0.04472135007381439
macro_f1 = 0.9917137870258077
weighted_f1 = 0.9917181804540693
regression_loss = 0.17375940084457397
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00018769230769230769, 3.753846153846154e-05]
-------------------------------------------------
Done 800 Batches of 1085
class_loss = 0.03232845291495323
macro_f1 = 0.9917617484417527
weighted_f1 = 0.9917664204114148
regression_loss = 0.184247225522995
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00018615384615384615, 3.723076923076923e-05]
-------------------------------------------------
Done 832 Batches of 1085
class_loss = 0.007155043072998524
macro_f1 = 0.9917334214487618
weighted_f1 = 0.9917356603897383
regression_loss = 0.20821401476860046
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00018461538461538463, 3.692307692307693e-05]
-------------------------------------------------
Done 864 Batches of 1085
class_loss = 0.026074500754475594
macro_f1 = 0.9916417977910467
weighted_f1 = 0.9916437481999121
regression_loss = 0.19537946581840515
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0001830769230769231, 3.661538461538462e-05]
-------------------------------------------------
Done 896 Batches of 1085
class_loss = 0.008349594660103321
macro_f1 = 0.99160284786731
weighted_f1 = 0.9916020330379556
regression_loss = 0.23868077993392944
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00018153846153846155, 3.630769230769231e-05]
-------------------------------------------------
Done 928 Batches of 1085
class_loss = 0.005564818158745766
macro_f1 = 0.9916828290613033
weighted_f1 = 0.9916810002863615
regression_loss = 0.19727995991706848
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00017999999999999998, 3.6e-05]
-------------------------------------------------
Done 960 Batches of 1085
class_loss = 0.042443934828042984
macro_f1 = 0.9917181975799201
weighted_f1 = 0.9917140781709245
regression_loss = 0.24531565606594086
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00017846153846153844, 3.569230769230769e-05]
-------------------------------------------------
Done 992 Batches of 1085
class_loss = 0.02744000218808651
macro_f1 = 0.9917649215641079
weighted_f1 = 0.9917608782615372
regression_loss = 0.17988410592079163
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00017692307692307693, 3.538461538461539e-05]
-------------------------------------------------
Done 1024 Batches of 1085
class_loss = 0.011047028936445713
macro_f1 = 0.9916555472692627
weighted_f1 = 0.9916520854934397
regression_loss = 0.14783549308776855
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0001753846153846154, 3.507692307692308e-05]
-------------------------------------------------
Done 1056 Batches of 1085
class_loss = 0.03780370578169823
macro_f1 = 0.9917498312959754
weighted_f1 = 0.9917497649166045
regression_loss = 0.22804361581802368
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00017384615384615385, 3.476923076923077e-05]
-------------------------------------------------
Done 1085 Batches of 1085
class_loss = 0.013832238502800465
macro_f1 = 0.9917185988485357
weighted_f1 = 0.9917181838207786
regression_loss = 0.1448061764240265
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0001723076923076923, 3.446153846153846e-05]
1.8515101671218872
1.2013617753982544
1.2192966938018799
1.599629521369934
1.7192316055297852
0.9994301795959473
1.3210843801498413
2.0668535232543945
1.4290080070495605
2.038421630859375
1.2440853118896484
1.5588549375534058
1.5752017498016357
1.9268635511398315
1.6052324771881104
2.0008316040039062
1.7052496671676636
1.7119643688201904
1.502229928970337
1.341152310371399
1.2637338638305664
1.2129408121109009
1.7249014377593994
1.3432034254074097
1.3394553661346436
1.8746161460876465
1.6961768865585327
1.5255022048950195
1.675479769706726
1.7985448837280273
1.8441972732543945
1.444410800933838
1.0455124378204346
1.7585819959640503
1.7711859941482544
1.751693844795227
1.5439915657043457
1.8564355373382568
1.5812896490097046
1.8894329071044922
0.9839150309562683
1.2003347873687744
1.3277596235275269
1.6640552282333374
2.054274559020996
1.5278093814849854
1.5260144472122192
1.697935700416565
1.558929443359375
1.5967239141464233
1.4240796566009521
1.1453338861465454
1.7981430292129517
1.4079221487045288
2.171698808670044
1.4089243412017822
1.7575733661651611
1.1827386617660522
1.2055583000183105
1.3553506135940552
2.0501132011413574
1.7908741235733032
1.709923505783081
1.7632725238800049
Epoch 14, training metrics:
--- CLASSIFICATION METRICS --- 
F1 scores: [Macro=0.992, Weighted=0.992] 
Balanced Accuracy = 0.992
Average loss = 0.07174
Epoch 14, validation metrics:
--- CLASSIFICATION METRICS --- 
F1 scores: [Macro=0.759, Weighted=0.759] 
Balanced Accuracy = 0.760
Average loss = 1.57606
avg_train_loss 0.07174043555482192
avg_val_loss 1.5760631011798978
-------------------------------------------------
Done 32 Batches of 1085
class_loss = 0.010158923454582691
macro_f1 = 0.9959113293611888
weighted_f1 = 0.9959671200700219
regression_loss = 0.19794593751430511
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00017076923076923077, 3.415384615384615e-05]
-------------------------------------------------
Done 64 Batches of 1085
class_loss = 0.012032759375870228
macro_f1 = 0.9960461189613076
weighted_f1 = 0.9960292260348552
regression_loss = 0.25750115513801575
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00016923076923076926, 3.384615384615385e-05]
-------------------------------------------------
Done 96 Batches of 1085
class_loss = 0.018905844539403915
macro_f1 = 0.9960661996673567
weighted_f1 = 0.9960507997291373
regression_loss = 0.22289766371250153
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0001676923076923077, 3.353846153846154e-05]
-------------------------------------------------
Done 128 Batches of 1085
class_loss = 0.02022075466811657
macro_f1 = 0.9953255344508418
weighted_f1 = 0.9953234021905185
regression_loss = 0.20339950919151306
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00016615384615384615, 3.323076923076923e-05]
-------------------------------------------------
Done 160 Batches of 1085
class_loss = 0.004473904613405466
macro_f1 = 0.9957158306071886
weighted_f1 = 0.9957244307594276
regression_loss = 0.2699497640132904
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0001646153846153846, 3.292307692307692e-05]
-------------------------------------------------
Done 192 Batches of 1085
class_loss = 0.0045315357856452465
macro_f1 = 0.9956557961564197
weighted_f1 = 0.9956641685859697
regression_loss = 0.18204526603221893
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00016307692307692307, 3.261538461538462e-05]
-------------------------------------------------
Done 224 Batches of 1085
class_loss = 0.0033169675152748823
macro_f1 = 0.9954352611620644
weighted_f1 = 0.9954451792845113
regression_loss = 0.26740214228630066
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00016153846153846155, 3.230769230769231e-05]
-------------------------------------------------
Done 256 Batches of 1085
class_loss = 0.024020859971642494
macro_f1 = 0.9953924272587547
weighted_f1 = 0.9954042084550655
regression_loss = 0.30714279413223267
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00016, 3.2000000000000005e-05]
-------------------------------------------------
Done 288 Batches of 1085
class_loss = 0.03924403712153435
macro_f1 = 0.9952230441901684
weighted_f1 = 0.9952361856054921
regression_loss = 0.24549748003482819
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00015846153846153847, 3.1692307692307696e-05]
-------------------------------------------------
Done 320 Batches of 1085
class_loss = 0.00594695471227169
macro_f1 = 0.9953370188659075
weighted_f1 = 0.9953466069509562
regression_loss = 0.20722229778766632
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0001569230769230769, 3.1384615384615386e-05]
-------------------------------------------------
Done 352 Batches of 1085
class_loss = 0.014135501347482204
macro_f1 = 0.9954750901397926
weighted_f1 = 0.995481509170132
regression_loss = 0.20040355622768402
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00015538461538461536, 3.107692307692308e-05]
-------------------------------------------------
Done 384 Batches of 1085
class_loss = 0.03464216738939285
macro_f1 = 0.9956098860739777
weighted_f1 = 0.9956142205489337
regression_loss = 0.22910922765731812
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00015384615384615385, 3.0769230769230774e-05]
-------------------------------------------------
Done 416 Batches of 1085
class_loss = 0.02601894550025463
macro_f1 = 0.9956278267640357
weighted_f1 = 0.995632305035704
regression_loss = 0.24203011393547058
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0001523076923076923, 3.0461538461538465e-05]
-------------------------------------------------
Done 448 Batches of 1085
class_loss = 0.008076279424130917
macro_f1 = 0.9956082616151125
weighted_f1 = 0.995612884311424
regression_loss = 0.2585136890411377
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00015076923076923077, 3.0153846153846155e-05]
-------------------------------------------------
Done 480 Batches of 1085
class_loss = 0.06515505909919739
macro_f1 = 0.9956261604526369
weighted_f1 = 0.9956287626566359
regression_loss = 0.24345698952674866
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00014923076923076923, 2.9846153846153846e-05]
-------------------------------------------------
Done 512 Batches of 1085
class_loss = 0.006428605876863003
macro_f1 = 0.9954719256194009
weighted_f1 = 0.9954745581939425
regression_loss = 0.20719370245933533
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00014769230769230772, 2.9538461538461543e-05]
-------------------------------------------------
Done 544 Batches of 1085
class_loss = 0.00712205283343792
macro_f1 = 0.9953915218378594
weighted_f1 = 0.9953958574100392
regression_loss = 0.21671083569526672
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00014615384615384618, 2.9230769230769234e-05]
-------------------------------------------------
Done 576 Batches of 1085
class_loss = 0.0031199667137116194
macro_f1 = 0.9953806953905764
weighted_f1 = 0.995380226679674
regression_loss = 0.22238017618656158
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0001446153846153846, 2.8923076923076925e-05]
-------------------------------------------------
Done 608 Batches of 1085
class_loss = 0.008041879162192345
macro_f1 = 0.9953798385045166
weighted_f1 = 0.9953791537258098
regression_loss = 0.25705307722091675
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00014307692307692307, 2.8615384615384615e-05]
-------------------------------------------------
Done 640 Batches of 1085
class_loss = 0.008073410019278526
macro_f1 = 0.9953922897154005
weighted_f1 = 0.9953904961201502
regression_loss = 0.20874103903770447
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00014153846153846153, 2.8307692307692306e-05]
-------------------------------------------------
Done 672 Batches of 1085
class_loss = 0.016223270446062088
macro_f1 = 0.9954478810937575
weighted_f1 = 0.9954473977347208
regression_loss = 0.2501309812068939
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00014000000000000001, 2.8000000000000003e-05]
-------------------------------------------------
Done 704 Batches of 1085
class_loss = 0.0029411432333290577
macro_f1 = 0.995464696545833
weighted_f1 = 0.9954656988015788
regression_loss = 0.20970860123634338
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00013846153846153847, 2.7692307692307694e-05]
-------------------------------------------------
Done 736 Batches of 1085
class_loss = 0.04612318426370621
macro_f1 = 0.9955138597992722
weighted_f1 = 0.9955144122090936
regression_loss = 0.15972459316253662
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00013692307692307693, 2.7384615384615387e-05]
-------------------------------------------------
Done 768 Batches of 1085
class_loss = 0.0025692840572446585
macro_f1 = 0.9955889043015789
weighted_f1 = 0.9955895960821829
regression_loss = 0.18822208046913147
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0001353846153846154, 2.7076923076923078e-05]
-------------------------------------------------
Done 800 Batches of 1085
class_loss = 0.05622938275337219
macro_f1 = 0.995638837039366
weighted_f1 = 0.9956390908304387
regression_loss = 0.18095427751541138
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00013384615384615385, 2.676923076923077e-05]
-------------------------------------------------
Done 832 Batches of 1085
class_loss = 0.001485122018493712
macro_f1 = 0.9956390766492564
weighted_f1 = 0.9956377872494581
regression_loss = 0.18294231593608856
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0001323076923076923, 2.6461538461538466e-05]
-------------------------------------------------
Done 864 Batches of 1085
class_loss = 0.016343336552381516
macro_f1 = 0.9956465746898571
weighted_f1 = 0.995645593301061
regression_loss = 0.18247276544570923
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00013076923076923077, 2.6153846153846157e-05]
-------------------------------------------------
Done 896 Batches of 1085
class_loss = 0.0017019431106746197
macro_f1 = 0.9956075942481528
weighted_f1 = 0.9956092175312601
regression_loss = 0.1927092969417572
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00012923076923076923, 2.5846153846153847e-05]
-------------------------------------------------
Done 928 Batches of 1085
class_loss = 0.004371200688183308
macro_f1 = 0.9956166151804314
weighted_f1 = 0.9956174917406647
regression_loss = 0.2158472239971161
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0001276923076923077, 2.5538461538461538e-05]
-------------------------------------------------
Done 960 Batches of 1085
class_loss = 0.0649290531873703
macro_f1 = 0.9956014231578768
weighted_f1 = 0.9956008137846033
regression_loss = 0.2574939727783203
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00012615384615384615, 2.523076923076923e-05]
-------------------------------------------------
Done 992 Batches of 1085
class_loss = 0.027997886762022972
macro_f1 = 0.9955776256122533
weighted_f1 = 0.9955772917881281
regression_loss = 0.28420400619506836
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0001246153846153846, 2.4923076923076926e-05]
-------------------------------------------------
Done 1024 Batches of 1085
class_loss = 0.015287143178284168
macro_f1 = 0.9955333031364793
weighted_f1 = 0.9955323708772096
regression_loss = 0.1989520639181137
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0001230769230769231, 2.461538461538462e-05]
-------------------------------------------------
Done 1056 Batches of 1085
class_loss = 0.028208689764142036
macro_f1 = 0.9955567195304482
weighted_f1 = 0.9955568302231195
regression_loss = 0.26201412081718445
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00012153846153846154, 2.430769230769231e-05]
-------------------------------------------------
Done 1085 Batches of 1085
class_loss = 0.012432003393769264
macro_f1 = 0.9955594415152305
weighted_f1 = 0.9955604009643857
regression_loss = 0.30511799454689026
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00012, 2.4e-05]
1.9337782859802246
1.2696374654769897
1.4019105434417725
1.562726378440857
1.6887538433074951
0.9868131279945374
1.4974886178970337
2.062997817993164
1.615525245666504
2.1133809089660645
1.3973815441131592
1.7358564138412476
1.6632485389709473
1.9419523477554321
1.7255138158798218
1.8963770866394043
1.6331937313079834
1.7954233884811401
1.6410560607910156
1.315750241279602
1.3184703588485718
1.305875539779663
1.8808095455169678
1.2933518886566162
1.4415230751037598
2.033999443054199
1.7393122911453247
1.6353445053100586
1.7902764081954956
1.8209532499313354
1.9663517475128174
1.2807780504226685
1.0677257776260376
2.1256680488586426
2.001744031906128
1.7694156169891357
1.5306490659713745
1.8639917373657227
1.7721972465515137
1.908983826637268
1.048664927482605
1.2944300174713135
1.3945890665054321
1.7788468599319458
2.194922924041748
1.558167576789856
1.7577805519104004
1.8044140338897705
1.5280683040618896
1.7030943632125854
1.5912697315216064
1.0847810506820679
1.8930368423461914
1.3860844373703003
2.347106456756592
1.5052688121795654
1.9218486547470093
1.3163334131240845
1.2035855054855347
1.4314196109771729
2.1833529472351074
1.8135501146316528
1.7569286823272705
1.9066722393035889
Epoch 15, training metrics:
--- CLASSIFICATION METRICS --- 
F1 scores: [Macro=0.996, Weighted=0.996] 
Balanced Accuracy = 0.996
Average loss = 0.05901
Epoch 15, validation metrics:
--- CLASSIFICATION METRICS --- 
F1 scores: [Macro=0.761, Weighted=0.761] 
Balanced Accuracy = 0.762
Average loss = 1.65360
avg_train_loss 0.05900681087326619
avg_val_loss 1.6536000622436404
-------------------------------------------------
Done 32 Batches of 1085
class_loss = 0.003359796479344368
macro_f1 = 0.9974807267713046
weighted_f1 = 0.9974815532768722
regression_loss = 0.17994987964630127
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00011846153846153846, 2.3692307692307695e-05]
-------------------------------------------------
Done 64 Batches of 1085
class_loss = 0.003078588517382741
macro_f1 = 0.9976349863121706
weighted_f1 = 0.9976432790848327
regression_loss = 0.2315814197063446
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00011692307692307692, 2.3384615384615385e-05]
-------------------------------------------------
Done 96 Batches of 1085
class_loss = 0.006401334423571825
macro_f1 = 0.9981897468700943
weighted_f1 = 0.9981905211927459
regression_loss = 0.16748470067977905
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0001153846153846154, 2.307692307692308e-05]
-------------------------------------------------
Done 128 Batches of 1085
class_loss = 0.003395786276087165
macro_f1 = 0.9982339015924199
weighted_f1 = 0.9982157717957821
regression_loss = 0.22121462225914001
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00011384615384615385, 2.276923076923077e-05]
-------------------------------------------------
Done 160 Batches of 1085
class_loss = 0.006728558335453272
macro_f1 = 0.9980942857374238
weighted_f1 = 0.9980835457360662
regression_loss = 0.2116875946521759
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0001123076923076923, 2.246153846153846e-05]
-------------------------------------------------
Done 192 Batches of 1085
class_loss = 0.005164568778127432
macro_f1 = 0.9978371094826597
weighted_f1 = 0.9978319380758199
regression_loss = 0.14626184105873108
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00011076923076923077, 2.2153846153846154e-05]
-------------------------------------------------
Done 224 Batches of 1085
class_loss = 0.00214157672598958
macro_f1 = 0.9978353821700698
weighted_f1 = 0.9978277109187006
regression_loss = 0.23344437777996063
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00010923076923076923, 2.1846153846153848e-05]
-------------------------------------------------
Done 256 Batches of 1085
class_loss = 0.0016523762606084347
macro_f1 = 0.9979519019192766
weighted_f1 = 0.9979470321001147
regression_loss = 0.19723805785179138
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00010769230769230771, 2.1538461538461542e-05]
-------------------------------------------------
Done 288 Batches of 1085
class_loss = 0.0019504677038639784
macro_f1 = 0.9980506275620712
weighted_f1 = 0.9980398607268027
regression_loss = 0.20209547877311707
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00010615384615384615, 2.1230769230769233e-05]
-------------------------------------------------
Done 320 Batches of 1085
class_loss = 0.0019240608671680093
macro_f1 = 0.9980466898226268
weighted_f1 = 0.9980404943198263
regression_loss = 0.26643043756484985
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00010461538461538461, 2.0923076923076923e-05]
-------------------------------------------------
Done 352 Batches of 1085
class_loss = 0.002477646339684725
macro_f1 = 0.998023083188641
weighted_f1 = 0.9980187553040993
regression_loss = 0.20082324743270874
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00010307692307692309, 2.0615384615384617e-05]
-------------------------------------------------
Done 384 Batches of 1085
class_loss = 0.00200719409622252
macro_f1 = 0.9980656467793831
weighted_f1 = 0.9980618932493175
regression_loss = 0.26415249705314636
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.00010153846153846153, 2.0307692307692308e-05]
-------------------------------------------------
Done 416 Batches of 1085
class_loss = 0.0029444328974932432
macro_f1 = 0.9981204130513003
weighted_f1 = 0.998117265251546
regression_loss = 0.17385423183441162
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0001, 2e-05]
-------------------------------------------------
Done 448 Batches of 1085
class_loss = 0.011306419968605042
macro_f1 = 0.9981129501818146
weighted_f1 = 0.9981121729884594
regression_loss = 0.23687580227851868
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [9.846153846153846e-05, 1.9692307692307692e-05]
-------------------------------------------------
Done 480 Batches of 1085
class_loss = 0.0041535478085279465
macro_f1 = 0.9981899942049524
weighted_f1 = 0.9981893490442052
regression_loss = 0.19474320113658905
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [9.692307692307692e-05, 1.9384615384615383e-05]
-------------------------------------------------
Done 512 Batches of 1085
class_loss = 0.007298680488020182
macro_f1 = 0.9981972876062647
weighted_f1 = 0.9981957086635729
regression_loss = 0.17946380376815796
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [9.538461538461538e-05, 1.9076923076923077e-05]
-------------------------------------------------
Done 544 Batches of 1085
class_loss = 0.0060364562086761
macro_f1 = 0.9981724130255636
weighted_f1 = 0.9981726215573212
regression_loss = 0.20655430853366852
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [9.384615384615384e-05, 1.876923076923077e-05]
-------------------------------------------------
Done 576 Batches of 1085
class_loss = 0.004790919367223978
macro_f1 = 0.9981937401349675
weighted_f1 = 0.9981928195613823
regression_loss = 0.20143567025661469
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [9.230769230769232e-05, 1.8461538461538465e-05]
-------------------------------------------------
Done 608 Batches of 1085
class_loss = 0.001032884931191802
macro_f1 = 0.9981969423485509
weighted_f1 = 0.9981979945293846
regression_loss = 0.22296631336212158
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [9.076923076923078e-05, 1.8153846153846155e-05]
-------------------------------------------------
Done 640 Batches of 1085
class_loss = 0.02663581818342209
macro_f1 = 0.9982126886210183
weighted_f1 = 0.9982148976203075
regression_loss = 0.20969876646995544
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [8.923076923076922e-05, 1.7846153846153846e-05]
-------------------------------------------------
Done 672 Batches of 1085
class_loss = 0.0009230005671270192
macro_f1 = 0.9981923139292007
weighted_f1 = 0.9981952372557275
regression_loss = 0.1815965175628662
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [8.76923076923077e-05, 1.753846153846154e-05]
-------------------------------------------------
Done 704 Batches of 1085
class_loss = 0.0010313353268429637
macro_f1 = 0.9981405620739
weighted_f1 = 0.9981439849340502
regression_loss = 0.2003118395805359
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [8.615384615384615e-05, 1.723076923076923e-05]
-------------------------------------------------
Done 736 Batches of 1085
class_loss = 0.0014415549812838435
macro_f1 = 0.9981784439112333
weighted_f1 = 0.9981822866256933
regression_loss = 0.25517281889915466
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [8.461538461538463e-05, 1.6923076923076924e-05]
-------------------------------------------------
Done 768 Batches of 1085
class_loss = 0.009419099427759647
macro_f1 = 0.9981946879123778
weighted_f1 = 0.9981970187826096
regression_loss = 0.21407854557037354
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [8.307692307692307e-05, 1.6615384615384615e-05]
-------------------------------------------------
Done 800 Batches of 1085
class_loss = 0.0018590455874800682
macro_f1 = 0.9981703553128277
weighted_f1 = 0.998171450244745
regression_loss = 0.2022828459739685
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [8.153846153846153e-05, 1.630769230769231e-05]
-------------------------------------------------
Done 832 Batches of 1085
class_loss = 0.003873382927849889
macro_f1 = 0.9981451528056754
weighted_f1 = 0.9981478677000388
regression_loss = 0.18680205941200256
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [8e-05, 1.6000000000000003e-05]
-------------------------------------------------
Done 864 Batches of 1085
class_loss = 0.003150798147544265
macro_f1 = 0.9981155798569482
weighted_f1 = 0.998116966988459
regression_loss = 0.1837109923362732
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [7.846153846153845e-05, 1.5692307692307693e-05]
-------------------------------------------------
Done 896 Batches of 1085
class_loss = 0.004505818709731102
macro_f1 = 0.9981054114523322
weighted_f1 = 0.9981057157398086
regression_loss = 0.2088865041732788
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [7.692307692307693e-05, 1.5384615384615387e-05]
-------------------------------------------------
Done 928 Batches of 1085
class_loss = 0.0005930077168159187
macro_f1 = 0.9980613801051161
weighted_f1 = 0.9980615056655102
regression_loss = 0.19561120867729187
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [7.538461538461539e-05, 1.5076923076923078e-05]
-------------------------------------------------
Done 960 Batches of 1085
class_loss = 0.07715492695569992
macro_f1 = 0.9980369488457959
weighted_f1 = 0.9980365790396571
regression_loss = 0.22312575578689575
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [7.384615384615386e-05, 1.4769230769230772e-05]
-------------------------------------------------
Done 992 Batches of 1085
class_loss = 0.000721826683729887
macro_f1 = 0.9980457032179594
weighted_f1 = 0.9980447669737774
regression_loss = 0.19723168015480042
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [7.23076923076923e-05, 1.4461538461538462e-05]
-------------------------------------------------
Done 1024 Batches of 1085
class_loss = 0.005289978347718716
macro_f1 = 0.9980448962648646
weighted_f1 = 0.9980448080195135
regression_loss = 0.2375449538230896
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [7.076923076923076e-05, 1.4153846153846153e-05]
-------------------------------------------------
Done 1056 Batches of 1085
class_loss = 0.004746311344206333
macro_f1 = 0.9980381236179854
weighted_f1 = 0.9980374426592888
regression_loss = 0.20031161606311798
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [6.923076923076924e-05, 1.3846153846153847e-05]
-------------------------------------------------
Done 1085 Batches of 1085
class_loss = 0.006959607824683189
macro_f1 = 0.9980104327428485
weighted_f1 = 0.9980106695810311
regression_loss = 0.17218729853630066
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [6.76923076923077e-05, 1.3538461538461539e-05]
2.109844446182251
1.369194507598877
1.450221300125122
1.7000046968460083
1.7762598991394043
0.9859166145324707
1.4409765005111694
2.207547426223755
1.624754786491394
2.048011302947998
1.4256969690322876
1.6932271718978882
1.7302172183990479
1.9356073141098022
1.7361769676208496
1.9606013298034668
1.8431228399276733
1.8620927333831787
1.6868468523025513
1.3716442584991455
1.3000380992889404
1.3144667148590088
1.8708906173706055
1.2815351486206055
1.5118532180786133
2.09224271774292
1.7384121417999268
1.6185590028762817
1.890210747718811
1.8395582437515259
1.9120590686798096
1.468038558959961
1.061808466911316
2.0488882064819336
1.994652271270752
1.9001778364181519
1.559065341949463
2.0099217891693115
1.6066795587539673
1.9866312742233276
1.0753669738769531
1.205791711807251
1.4584300518035889
1.8077127933502197
2.233370780944824
1.4854366779327393
1.6866533756256104
1.7619075775146484
1.646517038345337
1.8127055168151855
1.5397945642471313
1.2068839073181152
1.945940375328064
1.4170149564743042
2.399055004119873
1.5685168504714966
1.970447301864624
1.3822129964828491
1.2320059537887573
1.5117851495742798
2.2237343788146973
1.8649190664291382
1.8977224826812744
1.9783674478530884
Epoch 16, training metrics:
--- CLASSIFICATION METRICS --- 
F1 scores: [Macro=0.998, Weighted=0.998] 
Balanced Accuracy = 0.998
Average loss = 0.05012
Epoch 16, validation metrics:
--- CLASSIFICATION METRICS --- 
F1 scores: [Macro=0.765, Weighted=0.765] 
Balanced Accuracy = 0.765
Average loss = 1.69181
avg_train_loss 0.0501196838331662
avg_val_loss 1.6918116733431816
-------------------------------------------------
Done 32 Batches of 1085
class_loss = 0.001405020128004253
macro_f1 = 0.9987281712297789
weighted_f1 = 0.9987397987590246
regression_loss = 0.25870460271835327
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [6.615384615384616e-05, 1.3230769230769233e-05]
-------------------------------------------------
Done 64 Batches of 1085
class_loss = 0.0037878265138715506
macro_f1 = 0.998617322606292
weighted_f1 = 0.9986360123576521
regression_loss = 0.2309720367193222
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [6.461538461538462e-05, 1.2923076923076924e-05]
-------------------------------------------------
Done 96 Batches of 1085
class_loss = 0.0015093706315383315
macro_f1 = 0.9988366978295471
weighted_f1 = 0.998848748016198
regression_loss = 0.1536351442337036
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [6.307692307692308e-05, 1.2615384615384616e-05]
-------------------------------------------------
Done 128 Batches of 1085
class_loss = 0.004977238830178976
macro_f1 = 0.999139846508295
weighted_f1 = 0.9991388155436415
regression_loss = 0.2213071882724762
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [6.153846153846155e-05, 1.230769230769231e-05]
-------------------------------------------------
Done 160 Batches of 1085
class_loss = 0.0036625470966100693
macro_f1 = 0.9990202053978516
weighted_f1 = 0.9990173373774377
regression_loss = 0.27148130536079407
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [6e-05, 1.2e-05]
-------------------------------------------------
Done 192 Batches of 1085
class_loss = 0.003053063992410898
macro_f1 = 0.9990589328575068
weighted_f1 = 0.9990592581790687
regression_loss = 0.2392633706331253
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [5.846153846153846e-05, 1.1692307692307693e-05]
-------------------------------------------------
Done 224 Batches of 1085
class_loss = 0.002328804461285472
macro_f1 = 0.9990559161529923
weighted_f1 = 0.9990541311628097
regression_loss = 0.24681054055690765
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [5.692307692307693e-05, 1.1384615384615385e-05]
-------------------------------------------------
Done 256 Batches of 1085
class_loss = 0.0004460057243704796
macro_f1 = 0.9990794101132476
weighted_f1 = 0.9990808442425093
regression_loss = 0.2161637246608734
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [5.538461538461539e-05, 1.1076923076923077e-05]
-------------------------------------------------
Done 288 Batches of 1085
class_loss = 0.0014905179850757122
macro_f1 = 0.9989922478339212
weighted_f1 = 0.9989928113309962
regression_loss = 0.22136950492858887
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [5.3846153846153853e-05, 1.0769230769230771e-05]
-------------------------------------------------
Done 320 Batches of 1085
class_loss = 0.0013823631452396512
macro_f1 = 0.9990445493442129
weighted_f1 = 0.9990448425962488
regression_loss = 0.21696138381958008
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [5.2307692307692306e-05, 1.0461538461538462e-05]
-------------------------------------------------
Done 352 Batches of 1085
class_loss = 0.004129106644541025
macro_f1 = 0.9990431534464447
weighted_f1 = 0.9990428898307445
regression_loss = 0.19813275337219238
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [5.0769230769230766e-05, 1.0153846153846154e-05]
-------------------------------------------------
Done 384 Batches of 1085
class_loss = 0.0026631061919033527
macro_f1 = 0.9989801033660303
weighted_f1 = 0.9989800615686554
regression_loss = 0.178786963224411
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [4.923076923076923e-05, 9.846153846153846e-06]
-------------------------------------------------
Done 416 Batches of 1085
class_loss = 0.0012559021124616265
macro_f1 = 0.9989281138136503
weighted_f1 = 0.9989269344222826
regression_loss = 0.174674391746521
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [4.769230769230769e-05, 9.538461538461538e-06]
-------------------------------------------------
Done 448 Batches of 1085
class_loss = 0.0013911781134083867
macro_f1 = 0.9989527437395196
weighted_f1 = 0.9989513268127141
regression_loss = 0.18426017463207245
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [4.615384615384616e-05, 9.230769230769232e-06]
-------------------------------------------------
Done 480 Batches of 1085
class_loss = 0.004664797335863113
macro_f1 = 0.9989593839804058
weighted_f1 = 0.9989561371871672
regression_loss = 0.21914567053318024
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [4.461538461538461e-05, 8.923076923076923e-06]
-------------------------------------------------
Done 512 Batches of 1085
class_loss = 0.0003259869117755443
macro_f1 = 0.9989327252179226
weighted_f1 = 0.9989297746097803
regression_loss = 0.21187016367912292
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [4.307692307692308e-05, 8.615384615384615e-06]
-------------------------------------------------
Done 544 Batches of 1085
class_loss = 0.0012492482783272862
macro_f1 = 0.9989541004500386
weighted_f1 = 0.9989496788505692
regression_loss = 0.24095672369003296
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [4.153846153846154e-05, 8.307692307692307e-06]
-------------------------------------------------
Done 576 Batches of 1085
class_loss = 0.0012845422606915236
macro_f1 = 0.9989829934751772
weighted_f1 = 0.9989809569095595
regression_loss = 0.19993628561496735
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [4e-05, 8.000000000000001e-06]
-------------------------------------------------
Done 608 Batches of 1085
class_loss = 0.0044920784421265125
macro_f1 = 0.9989599091611681
weighted_f1 = 0.9989574448954452
regression_loss = 0.19551801681518555
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [3.846153846153846e-05, 7.692307692307694e-06]
-------------------------------------------------
Done 640 Batches of 1085
class_loss = 0.0021068977657705545
macro_f1 = 0.9990114516376792
weighted_f1 = 0.9990096547899537
regression_loss = 0.2439521849155426
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [3.692307692307693e-05, 7.384615384615386e-06]
-------------------------------------------------
Done 672 Batches of 1085
class_loss = 0.0008145671454258263
macro_f1 = 0.998977809660856
weighted_f1 = 0.9989753811966484
regression_loss = 0.23006156086921692
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [3.538461538461538e-05, 7.076923076923076e-06]
-------------------------------------------------
Done 704 Batches of 1085
class_loss = 0.0017760060727596283
macro_f1 = 0.9989916418309974
weighted_f1 = 0.9989886808501832
regression_loss = 0.2083713412284851
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [3.384615384615385e-05, 6.7692307692307695e-06]
-------------------------------------------------
Done 736 Batches of 1085
class_loss = 0.003625465091317892
macro_f1 = 0.9989834340285253
weighted_f1 = 0.9989795709473887
regression_loss = 0.18689268827438354
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [3.230769230769231e-05, 6.461538461538462e-06]
-------------------------------------------------
Done 768 Batches of 1085
class_loss = 0.00581660121679306
macro_f1 = 0.9989745645720524
weighted_f1 = 0.9989712135799524
regression_loss = 0.21220870316028595
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [3.0769230769230774e-05, 6.153846153846155e-06]
-------------------------------------------------
Done 800 Batches of 1085
class_loss = 0.007585556246340275
macro_f1 = 0.9990064034978545
weighted_f1 = 0.999002644373116
regression_loss = 0.2793425917625427
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [2.923076923076923e-05, 5.846153846153846e-06]
-------------------------------------------------
Done 832 Batches of 1085
class_loss = 0.0017061653779819608
macro_f1 = 0.9989516746902096
weighted_f1 = 0.9989470100696606
regression_loss = 0.23428216576576233
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [2.7692307692307694e-05, 5.5384615384615385e-06]
-------------------------------------------------
Done 864 Batches of 1085
class_loss = 0.0002378163335379213
macro_f1 = 0.9989266517551364
weighted_f1 = 0.9989226806989977
regression_loss = 0.2056364119052887
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [2.6153846153846153e-05, 5.230769230769231e-06]
-------------------------------------------------
Done 896 Batches of 1085
class_loss = 0.0004836130829062313
macro_f1 = 0.9989209361626781
weighted_f1 = 0.9989175538801967
regression_loss = 0.23443615436553955
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [2.4615384615384616e-05, 4.923076923076923e-06]
-------------------------------------------------
Done 928 Batches of 1085
class_loss = 0.0008999201818369329
macro_f1 = 0.9989258703591988
weighted_f1 = 0.9989212131441134
regression_loss = 0.20268148183822632
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [2.307692307692308e-05, 4.615384615384616e-06]
-------------------------------------------------
Done 960 Batches of 1085
class_loss = 0.003658320987597108
macro_f1 = 0.9989271301274318
weighted_f1 = 0.9989246291580403
regression_loss = 0.22569668292999268
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [2.153846153846154e-05, 4.3076923076923076e-06]
-------------------------------------------------
Done 992 Batches of 1085
class_loss = 0.0014422826934605837
macro_f1 = 0.998906006785002
weighted_f1 = 0.9989041683974148
regression_loss = 0.1541738510131836
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [2e-05, 4.000000000000001e-06]
-------------------------------------------------
Done 1024 Batches of 1085
class_loss = 0.0013297765981405973
macro_f1 = 0.9989172000854563
weighted_f1 = 0.9989155414704078
regression_loss = 0.21650700271129608
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [1.8461538461538465e-05, 3.692307692307693e-06]
-------------------------------------------------
Done 1056 Batches of 1085
class_loss = 0.001210870104841888
macro_f1 = 0.9989188946035483
weighted_f1 = 0.9989188247515106
regression_loss = 0.22402739524841309
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [1.6923076923076924e-05, 3.3846153846153848e-06]
-------------------------------------------------
Done 1085 Batches of 1085
class_loss = 4.434273068909533e-05
macro_f1 = 0.9989038885659938
weighted_f1 = 0.9989045024160504
regression_loss = 0.16741983592510223
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [1.5384615384615387e-05, 3.0769230769230774e-06]
2.163482189178467
1.3649448156356812
1.437063455581665
1.7147186994552612
1.7974027395248413
1.0265929698944092
1.4570527076721191
2.256540060043335
1.6285052299499512
2.137880325317383
1.4377671480178833
1.723681926727295
1.7199217081069946
1.9827088117599487
1.7376693487167358
2.0003843307495117
1.81719172000885
1.8794262409210205
1.6722126007080078
1.3693628311157227
1.3418043851852417
1.3599450588226318
1.879636526107788
1.3132160902023315
1.4945552349090576
2.0652523040771484
1.7704955339431763
1.6397703886032104
1.8615919351577759
1.8509676456451416
1.9696917533874512
1.5299681425094604
1.0588791370391846
2.105208396911621
2.0393450260162354
1.9481080770492554
1.6251074075698853
2.0679707527160645
1.6127948760986328
2.029541492462158
1.103555679321289
1.2725942134857178
1.4330341815948486
1.8457099199295044
2.2800493240356445
1.511529803276062
1.7362250089645386
1.829883098602295
1.6978076696395874
1.8452742099761963
1.579375982284546
1.2071433067321777
1.980890154838562
1.47568941116333
2.3968114852905273
1.563360571861267
1.983726978302002
1.405580997467041
1.2273787260055542
1.4985162019729614
2.204634666442871
1.8720488548278809
1.8945059776306152
2.040938377380371
Epoch 17, training metrics:
--- CLASSIFICATION METRICS --- 
F1 scores: [Macro=0.999, Weighted=0.999] 
Balanced Accuracy = 0.999
Average loss = 0.04662
Epoch 17, validation metrics:
--- CLASSIFICATION METRICS --- 
F1 scores: [Macro=0.765, Weighted=0.765] 
Balanced Accuracy = 0.766
Average loss = 1.71523
avg_train_loss 0.04662129134969777
avg_val_loss 1.7152285128831863
-------------------------------------------------
Done 32 Batches of 1085
class_loss = 0.0055113742128014565
macro_f1 = 0.999513174997046
weighted_f1 = 0.99949583671618
regression_loss = 0.23426535725593567
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [1.3846153846153847e-05, 2.7692307692307693e-06]
-------------------------------------------------
Done 64 Batches of 1085
class_loss = 0.00034991619759239256
macro_f1 = 0.9996437089727083
weighted_f1 = 0.9996279118006828
regression_loss = 0.21472644805908203
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [1.2307692307692308e-05, 2.4615384615384615e-06]
-------------------------------------------------
Done 96 Batches of 1085
class_loss = 0.002500756410881877
macro_f1 = 0.9995980698321721
weighted_f1 = 0.9995887940915319
regression_loss = 0.24696268141269684
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [1.076923076923077e-05, 2.1538461538461538e-06]
-------------------------------------------------
Done 128 Batches of 1085
class_loss = 0.0010564333060756326
macro_f1 = 0.9996403751554418
weighted_f1 = 0.9996308984189199
regression_loss = 0.21668194234371185
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [9.230769230769232e-06, 1.8461538461538465e-06]
-------------------------------------------------
Done 160 Batches of 1085
class_loss = 0.0016062124632298946
macro_f1 = 0.9994634113998112
weighted_f1 = 0.9994594145325564
regression_loss = 0.20283618569374084
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [7.692307692307694e-06, 1.5384615384615387e-06]
-------------------------------------------------
Done 192 Batches of 1085
class_loss = 0.004029660485684872
macro_f1 = 0.9995147413213836
weighted_f1 = 0.9995090957333816
regression_loss = 0.21216270327568054
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [6.153846153846154e-06, 1.2307692307692308e-06]
-------------------------------------------------
Done 224 Batches of 1085
class_loss = 0.0007353677647188306
macro_f1 = 0.9995099773656506
weighted_f1 = 0.9995094545019493
regression_loss = 0.22170212864875793
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [4.615384615384616e-06, 9.230769230769232e-07]
-------------------------------------------------
Done 256 Batches of 1085
class_loss = 0.0027761966921389103
macro_f1 = 0.999571383525564
weighted_f1 = 0.9995710208348789
regression_loss = 0.1663888543844223
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [3.076923076923077e-06, 6.153846153846154e-07]
-------------------------------------------------
Done 288 Batches of 1085
class_loss = 0.0013783209724351764
macro_f1 = 0.9996186935776705
weighted_f1 = 0.9996188570554493
regression_loss = 0.21039840579032898
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [1.5384615384615385e-06, 3.076923076923077e-07]
-------------------------------------------------
Done 320 Batches of 1085
class_loss = 0.002036223653703928
macro_f1 = 0.9995844622271985
weighted_f1 = 0.9995836172981962
regression_loss = 0.23979300260543823
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0, 0.0]
-------------------------------------------------
Done 352 Batches of 1085
class_loss = 0.0009636976174078882
macro_f1 = 0.9995335802635985
weighted_f1 = 0.9995325392347301
regression_loss = 0.22547684609889984
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0, 0.0]
-------------------------------------------------
Done 384 Batches of 1085
class_loss = 0.001301799900829792
macro_f1 = 0.999511315037753
weighted_f1 = 0.9995104261643103
regression_loss = 0.2587798833847046
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0, 0.0]
-------------------------------------------------
Done 416 Batches of 1085
class_loss = 0.002680256962776184
macro_f1 = 0.9994585214200228
weighted_f1 = 0.9994540515307629
regression_loss = 0.2016620635986328
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0, 0.0]
-------------------------------------------------
Done 448 Batches of 1085
class_loss = 0.0035134099889546633
macro_f1 = 0.999461289545725
weighted_f1 = 0.9994581805280432
regression_loss = 0.176949143409729
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0, 0.0]
-------------------------------------------------
Done 480 Batches of 1085
class_loss = 0.001807861728593707
macro_f1 = 0.9994313637563892
weighted_f1 = 0.9994291400102453
regression_loss = 0.2041715532541275
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0, 0.0]
-------------------------------------------------
Done 512 Batches of 1085
class_loss = 0.009223097935318947
macro_f1 = 0.9994200501021752
weighted_f1 = 0.9994190235586669
regression_loss = 0.2106052041053772
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0, 0.0]
-------------------------------------------------
Done 544 Batches of 1085
class_loss = 0.0007626748993061483
macro_f1 = 0.9994099047936309
weighted_f1 = 0.9994100987620865
regression_loss = 0.18517252802848816
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0, 0.0]
-------------------------------------------------
Done 576 Batches of 1085
class_loss = 0.002112169750034809
macro_f1 = 0.9994441971136064
weighted_f1 = 0.9994429285594887
regression_loss = 0.2235722541809082
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0, 0.0]
-------------------------------------------------
Done 608 Batches of 1085
class_loss = 0.0018865374149754643
macro_f1 = 0.9994596101560675
weighted_f1 = 0.9994594276478173
regression_loss = 0.29382815957069397
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0, 0.0]
-------------------------------------------------
Done 640 Batches of 1085
class_loss = 0.001304317032918334
macro_f1 = 0.9994498811887269
weighted_f1 = 0.9994498162330995
regression_loss = 0.2253245860338211
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0, 0.0]
-------------------------------------------------
Done 672 Batches of 1085
class_loss = 0.005590897053480148
macro_f1 = 0.9994513584854786
weighted_f1 = 0.9994527686097042
regression_loss = 0.29470086097717285
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0, 0.0]
-------------------------------------------------
Done 704 Batches of 1085
class_loss = 0.0009286621934734285
macro_f1 = 0.9994556977983013
weighted_f1 = 0.9994554547518102
regression_loss = 0.19587618112564087
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0, 0.0]
-------------------------------------------------
Done 736 Batches of 1085
class_loss = 0.0007086526602506638
macro_f1 = 0.9994361486280132
weighted_f1 = 0.9994366458437232
regression_loss = 0.19375699758529663
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0, 0.0]
-------------------------------------------------
Done 768 Batches of 1085
class_loss = 0.0003718123189173639
macro_f1 = 0.9994298439731392
weighted_f1 = 0.9994295924018033
regression_loss = 0.18528735637664795
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0, 0.0]
-------------------------------------------------
Done 800 Batches of 1085
class_loss = 0.00609489344060421
macro_f1 = 0.9993831043782826
weighted_f1 = 0.9993839762167004
regression_loss = 0.21550729870796204
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0, 0.0]
-------------------------------------------------
Done 832 Batches of 1085
class_loss = 0.0028049631509929895
macro_f1 = 0.9993793529054752
weighted_f1 = 0.9993794966899371
regression_loss = 0.20537331700325012
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0, 0.0]
-------------------------------------------------
Done 864 Batches of 1085
class_loss = 0.0012527918443083763
macro_f1 = 0.9993574230171192
weighted_f1 = 0.9993572398988596
regression_loss = 0.18165476620197296
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0, 0.0]
-------------------------------------------------
Done 896 Batches of 1085
class_loss = 0.0005511229392141104
macro_f1 = 0.9993535576868078
weighted_f1 = 0.9993540340391152
regression_loss = 0.23054298758506775
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0, 0.0]
-------------------------------------------------
Done 928 Batches of 1085
class_loss = 0.0054702796041965485
macro_f1 = 0.9993593503951379
weighted_f1 = 0.9993594751864958
regression_loss = 0.32425886392593384
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0, 0.0]
-------------------------------------------------
Done 960 Batches of 1085
class_loss = 0.001259452779777348
macro_f1 = 0.9993636914784426
weighted_f1 = 0.9993645714071829
regression_loss = 0.18782415986061096
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0, 0.0]
-------------------------------------------------
Done 992 Batches of 1085
class_loss = 0.0016200562240555882
macro_f1 = 0.9993841768995205
weighted_f1 = 0.9993850898217875
regression_loss = 0.20833823084831238
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0, 0.0]
-------------------------------------------------
Done 1024 Batches of 1085
class_loss = 0.0012457645498216152
macro_f1 = 0.9993729361640356
weighted_f1 = 0.9993737771803899
regression_loss = 0.20563365519046783
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0, 0.0]
-------------------------------------------------
Done 1056 Batches of 1085
class_loss = 0.0017618098063394427
macro_f1 = 0.9993847880189921
weighted_f1 = 0.9993853672734285
regression_loss = 0.14643290638923645
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0, 0.0]
-------------------------------------------------
Done 1085 Batches of 1085
class_loss = 0.0010941713117063046
macro_f1 = 0.9993793331392103
weighted_f1 = 0.9993801900744131
regression_loss = 0.8744171261787415
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0, 0.0]
2.1614866256713867
1.3723583221435547
1.4300719499588013
1.7127768993377686
1.7917486429214478
1.0242109298706055
1.4658472537994385
2.2532172203063965
1.6241028308868408
2.136603355407715
1.440637469291687
1.7337868213653564
1.717027187347412
1.9867236614227295
1.7449692487716675
1.9997292757034302
1.82331383228302
1.8743451833724976
1.674891471862793
1.3667933940887451
1.3502452373504639
1.3655083179473877
1.8830972909927368
1.3051464557647705
1.5032628774642944
2.0712106227874756
1.765882134437561
1.6350594758987427
1.8601688146591187
1.8494277000427246
1.9746932983398438
1.5332252979278564
1.0617541074752808
2.105497121810913
2.0388681888580322
1.9419009685516357
1.6292680501937866
2.060401678085327
1.614991307258606
2.0332565307617188
1.1026535034179688
1.2652976512908936
1.4385313987731934
1.8423683643341064
2.2762856483459473
1.5154918432235718
1.7405670881271362
1.823279619216919
1.6925411224365234
1.8450589179992676
1.578986644744873
1.1948158740997314
1.9845433235168457
1.4752599000930786
2.3987011909484863
1.5661948919296265
1.977334976196289
1.4114538431167603
1.2225407361984253
1.5068910121917725
2.203507423400879
1.8862743377685547
1.8878169059753418
2.032787322998047
Epoch 18, training metrics:
--- CLASSIFICATION METRICS --- 
F1 scores: [Macro=0.999, Weighted=0.999] 
Balanced Accuracy = 0.999
Average loss = 0.04566
Epoch 18, validation metrics:
--- CLASSIFICATION METRICS --- 
F1 scores: [Macro=0.765, Weighted=0.765] 
Balanced Accuracy = 0.766
Average loss = 1.71542
avg_train_loss 0.04565875739683204
avg_val_loss 1.7154170405119658
-------------------------------------------------
Done 32 Batches of 1085
class_loss = 0.0032221542205661535
macro_f1 = 0.9992517416369495
weighted_f1 = 0.9992439275287958
regression_loss = 0.2952490448951721
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0, 0.0]
-------------------------------------------------
Done 64 Batches of 1085
class_loss = 0.0032252285163849592
macro_f1 = 0.9996262189470547
weighted_f1 = 0.9996279590714815
regression_loss = 0.28927648067474365
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0, 0.0]
-------------------------------------------------
Done 96 Batches of 1085
class_loss = 0.003113526850938797
macro_f1 = 0.9995893300758771
weighted_f1 = 0.9995888771798347
regression_loss = 0.19491571187973022
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0, 0.0]
-------------------------------------------------
Done 128 Batches of 1085
class_loss = 0.0011602962622419
macro_f1 = 0.9995715908056717
weighted_f1 = 0.9995694249069261
regression_loss = 0.26591652631759644
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0, 0.0]
-------------------------------------------------
Done 160 Batches of 1085
class_loss = 0.00084926892304793
macro_f1 = 0.999264145190416
weighted_f1 = 0.9992630473228935
regression_loss = 0.24114176630973816
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0, 0.0]
-------------------------------------------------
Done 192 Batches of 1085
class_loss = 0.0004937637131661177
macro_f1 = 0.9993051411914523
weighted_f1 = 0.9993046465602933
regression_loss = 0.16504263877868652
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0, 0.0]
-------------------------------------------------
Done 224 Batches of 1085
class_loss = 0.0005249339155852795
macro_f1 = 0.9992640386894129
weighted_f1 = 0.9992642915528749
regression_loss = 0.1993509829044342
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0, 0.0]
-------------------------------------------------
Done 256 Batches of 1085
class_loss = 0.0037159735802561045
macro_f1 = 0.9992950977840463
weighted_f1 = 0.9992953325660955
regression_loss = 0.2142004370689392
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0, 0.0]
-------------------------------------------------
Done 288 Batches of 1085
class_loss = 0.0012993535492569208
macro_f1 = 0.9993185937276866
weighted_f1 = 0.9993194522335741
regression_loss = 0.21851485967636108
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0, 0.0]
-------------------------------------------------
Done 320 Batches of 1085
class_loss = 0.0011205981718376279
macro_f1 = 0.9993388418544514
weighted_f1 = 0.9993387349631774
regression_loss = 0.18543466925621033
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0, 0.0]
-------------------------------------------------
Done 352 Batches of 1085
class_loss = 0.014833727851510048
macro_f1 = 0.9993536632491375
weighted_f1 = 0.9993544984371626
regression_loss = 0.20603033900260925
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0, 0.0]
-------------------------------------------------
Done 384 Batches of 1085
class_loss = 0.011752188205718994
macro_f1 = 0.9993260195423359
weighted_f1 = 0.9993268349889913
regression_loss = 0.20906338095664978
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0, 0.0]
-------------------------------------------------
Done 416 Batches of 1085
class_loss = 0.0002630121889524162
macro_f1 = 0.999322399976857
weighted_f1 = 0.9993222713420001
regression_loss = 0.19860109686851501
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0, 0.0]
-------------------------------------------------
Done 448 Batches of 1085
class_loss = 0.006004304625093937
macro_f1 = 0.9992828160022755
weighted_f1 = 0.99928339610764
regression_loss = 0.1829105019569397
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0, 0.0]
-------------------------------------------------
Done 480 Batches of 1085
class_loss = 0.0006972998380661011
macro_f1 = 0.9993158724009552
weighted_f1 = 0.9993149616215855
regression_loss = 0.2345210611820221
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0, 0.0]
-------------------------------------------------
Done 512 Batches of 1085
class_loss = 0.0013226218288764358
macro_f1 = 0.9992825234202449
weighted_f1 = 0.9992814239188008
regression_loss = 0.21659892797470093
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0, 0.0]
-------------------------------------------------
Done 544 Batches of 1085
class_loss = 0.000468322541564703
macro_f1 = 0.9993103186683495
weighted_f1 = 0.9993093824138715
regression_loss = 0.18719401955604553
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0, 0.0]
-------------------------------------------------
Done 576 Batches of 1085
class_loss = 0.0009737447835505009
macro_f1 = 0.9992663769003636
weighted_f1 = 0.9992663082427834
regression_loss = 0.2681274712085724
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0, 0.0]
-------------------------------------------------
Done 608 Batches of 1085
class_loss = 0.004967177752405405
macro_f1 = 0.9992790476512589
weighted_f1 = 0.999279226606599
regression_loss = 0.24146056175231934
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0, 0.0]
-------------------------------------------------
Done 640 Batches of 1085
class_loss = 0.0012085485504940152
macro_f1 = 0.999303775627129
weighted_f1 = 0.9993030957323887
regression_loss = 0.19298699498176575
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0, 0.0]
-------------------------------------------------
Done 672 Batches of 1085
class_loss = 0.0010549664730206132
macro_f1 = 0.999314186437417
weighted_f1 = 0.9993130422458089
regression_loss = 0.16906826198101044
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0, 0.0]
-------------------------------------------------
Done 704 Batches of 1085
class_loss = 0.0008930198382586241
macro_f1 = 0.9993003973754633
weighted_f1 = 0.9992998584036571
regression_loss = 0.1538606435060501
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0, 0.0]
-------------------------------------------------
Done 736 Batches of 1085
class_loss = 0.0007842874620109797
macro_f1 = 0.9992996552138452
weighted_f1 = 0.9992984524489569
regression_loss = 0.2318345010280609
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0, 0.0]
-------------------------------------------------
Done 768 Batches of 1085
class_loss = 0.0006523988558910787
macro_f1 = 0.9992859134998939
weighted_f1 = 0.9992869786253422
regression_loss = 0.23314529657363892
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0, 0.0]
-------------------------------------------------
Done 800 Batches of 1085
class_loss = 0.006332831922918558
macro_f1 = 0.9992956480307036
weighted_f1 = 0.9992959783108836
regression_loss = 0.22496464848518372
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0, 0.0]
-------------------------------------------------
Done 832 Batches of 1085
class_loss = 0.004940886981785297
macro_f1 = 0.9992956234713285
weighted_f1 = 0.9992948833953806
regression_loss = 0.24361826479434967
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0, 0.0]
-------------------------------------------------
Done 864 Batches of 1085
class_loss = 0.005308070220053196
macro_f1 = 0.9993035103681416
weighted_f1 = 0.9993029279356674
regression_loss = 0.20688588917255402
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0, 0.0]
-------------------------------------------------
Done 896 Batches of 1085
class_loss = 0.0034536835737526417
macro_f1 = 0.9993014538834964
weighted_f1 = 0.999301665537414
regression_loss = 0.188396155834198
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0, 0.0]
-------------------------------------------------
Done 928 Batches of 1085
class_loss = 0.004407977219671011
macro_f1 = 0.9993084169540245
weighted_f1 = 0.9993089163615198
regression_loss = 0.19955851137638092
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0, 0.0]
-------------------------------------------------
Done 960 Batches of 1085
class_loss = 0.0009794860379770398
macro_f1 = 0.9992981426661997
weighted_f1 = 0.9992993915754786
regression_loss = 0.23460334539413452
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0, 0.0]
-------------------------------------------------
Done 992 Batches of 1085
class_loss = 0.0008508334285579622
macro_f1 = 0.9992806893114023
weighted_f1 = 0.999282600279058
regression_loss = 0.1495269238948822
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0, 0.0]
-------------------------------------------------
Done 1024 Batches of 1085
class_loss = 0.0014589959755539894
macro_f1 = 0.9992885871306546
weighted_f1 = 0.9992897663416588
regression_loss = 0.18858540058135986
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0, 0.0]
-------------------------------------------------
Done 1056 Batches of 1085
class_loss = 0.0017254851991310716
macro_f1 = 0.9992955964449985
weighted_f1 = 0.9992965050816044
regression_loss = 0.17576465010643005
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0, 0.0]
-------------------------------------------------
Done 1085 Batches of 1085
class_loss = 7.56834342610091e-05
macro_f1 = 0.999307425569216
weighted_f1 = 0.9993081183401588
regression_loss = 0.23838253319263458
-------------------------------------------------
Params with no grad after backward: ['base_model.layers.29.input_layernorm.weight', 'base_model.layers.29.input_layernorm.bias', 'base_model.layers.29.post_attention_layernorm.weight', 'base_model.layers.29.post_attention_layernorm.bias', 'base_model.layers.29.attention.query_key_value.weight', 'base_model.layers.29.attention.query_key_value.bias', 'base_model.layers.29.attention.dense.weight', 'base_model.layers.29.attention.dense.bias', 'base_model.layers.29.mlp.dense_h_to_4h.weight', 'base_model.layers.29.mlp.dense_h_to_4h.bias', 'base_model.layers.29.mlp.dense_4h_to_h.weight', 'base_model.layers.29.mlp.dense_4h_to_h.bias', 'base_model.layers.30.input_layernorm.weight', 'base_model.layers.30.input_layernorm.bias', 'base_model.layers.30.post_attention_layernorm.weight', 'base_model.layers.30.post_attention_layernorm.bias', 'base_model.layers.30.attention.query_key_value.weight', 'base_model.layers.30.attention.query_key_value.bias', 'base_model.layers.30.attention.dense.weight', 'base_model.layers.30.attention.dense.bias', 'base_model.layers.30.mlp.dense_h_to_4h.weight', 'base_model.layers.30.mlp.dense_h_to_4h.bias', 'base_model.layers.30.mlp.dense_4h_to_h.weight', 'base_model.layers.30.mlp.dense_4h_to_h.bias', 'base_model.layers.31.input_layernorm.weight', 'base_model.layers.31.input_layernorm.bias', 'base_model.layers.31.post_attention_layernorm.weight', 'base_model.layers.31.post_attention_layernorm.bias', 'base_model.layers.31.attention.query_key_value.weight', 'base_model.layers.31.attention.query_key_value.bias', 'base_model.layers.31.attention.dense.weight', 'base_model.layers.31.attention.dense.bias', 'base_model.layers.31.mlp.dense_h_to_4h.weight', 'base_model.layers.31.mlp.dense_h_to_4h.bias', 'base_model.layers.31.mlp.dense_4h_to_h.weight', 'base_model.layers.31.mlp.dense_4h_to_h.bias', 'base_model.final_layer_norm.weight', 'base_model.final_layer_norm.bias', 'shared.fc1.weight', 'shared.fc1.bias', 'shared.fc2.weight', 'shared.fc2.bias', 'classifier.fc.weight', 'classifier.fc.bias', 'classifier.out.weight', 'classifier.out.bias', 'regressor.fc.weight', 'regressor.fc.bias', 'regressor.out.weight', 'regressor.out.bias']
Optimizer param groups: 50
LRs after scheduler: [0.0, 0.0]
2.1614866256713867
1.3723583221435547
1.4300719499588013
1.7127768993377686
1.7917486429214478
1.0242109298706055
1.4658472537994385
2.2532172203063965
1.6241028308868408
2.136603355407715
1.440637469291687
1.7337868213653564
1.717027187347412
1.9867236614227295
1.7449692487716675
1.9997292757034302
1.82331383228302
1.8743451833724976
1.674891471862793
1.3667933940887451
1.3502452373504639
1.3655083179473877
1.8830972909927368
1.3051464557647705
1.5032628774642944
2.0712106227874756
1.765882134437561
1.6350594758987427
1.8601688146591187
1.8494277000427246
1.9746932983398438
1.5332252979278564
1.0617541074752808
2.105497121810913
2.0388681888580322
1.9419009685516357
1.6292680501937866
2.060401678085327
1.614991307258606
2.0332565307617188
1.1026535034179688
1.2652976512908936
1.4385313987731934
1.8423683643341064
2.2762856483459473
1.5154918432235718
1.7405670881271362
1.823279619216919
1.6925411224365234
1.8450589179992676
1.578986644744873
1.1948158740997314
1.9845433235168457
1.4752599000930786
2.3987011909484863
1.5661948919296265
1.977334976196289
1.4114538431167603
1.2225407361984253
1.5068910121917725
2.203507423400879
1.8862743377685547
1.8878169059753418
2.032787322998047
Epoch 19, training metrics:
--- CLASSIFICATION METRICS --- 
F1 scores: [Macro=0.999, Weighted=0.999] 
Balanced Accuracy = 0.999
Average loss = 0.04564
Epoch 19, validation metrics:
--- CLASSIFICATION METRICS --- 
F1 scores: [Macro=0.765, Weighted=0.765] 
Balanced Accuracy = 0.766
Average loss = 1.71542
avg_train_loss 0.0456428986626424
avg_val_loss 1.7154170405119658
🏃 View run Optimus_Prime at: http://84.238.41.58:5000/#/experiments/530407069449458195/runs/37350c60d48b4f0089645114a4e56053
🧪 View experiment at: http://84.238.41.58:5000/#/experiments/530407069449458195
Traceback (most recent call last):
  File "/pfs/lustrep2/projappl/project_465002413/ChessGPT/chessgpt_multitask.py", line 702, in <module>
    if epochs_non_improved == 2:
                  ^^^^^^^^^^^^^^^
  File "/opt/miniconda3/envs/pytorch/lib/python3.12/site-packages/torch/serialization.py", line 1479, in load
    with _open_file_like(f, "rb") as opened_file:
         ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/miniconda3/envs/pytorch/lib/python3.12/site-packages/torch/serialization.py", line 759, in _open_file_like
    return _open_file(name_or_buffer, mode)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/miniconda3/envs/pytorch/lib/python3.12/site-packages/torch/serialization.py", line 740, in __init__
    super().__init__(open(name, mode))
                     ^^^^^^^^^^^^^^^^
FileNotFoundError: [Errno 2] No such file or directory: '/best_models/optimus_prime_pt2.pth'
