{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "dcd9ddce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cpu\n"
     ]
    }
   ],
   "source": [
    "# import all libraries\n",
    "import numpy as np\n",
    "# import sklearn.model_selection\n",
    "from sklearn.model_selection import train_test_split\n",
    "# import os\n",
    "from tqdm import tqdm\n",
    "# from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, Subset, random_split\n",
    "# import torch.optim as optim\n",
    "# import torchvision.transforms as transforms\n",
    "from collections import Counter\n",
    "# from pathlib import Path\n",
    "# from io import BytesIO\n",
    "import itertools\n",
    "from sklearn.metrics import f1_score, confusion_matrix, balanced_accuracy_score, classification_report, mean_absolute_error, mean_squared_error, r2_score\n",
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "# from tabulate import tabulate\n",
    "from pathlib import Path\n",
    "# for logging\n",
    "import mlflow\n",
    "\n",
    "USE_LOGGING = False\n",
    "\n",
    "if USE_LOGGING:\n",
    "    # Run MLFlow locally on port 5000, set IP address here:\n",
    "    mlflow.set_tracking_uri(\n",
    "        \"http://youdontgetmyip:5000\"\n",
    "    )\n",
    "\n",
    "    # maybe make experiment first, before calling this.\n",
    "    mlflow.set_experiment(\"RNN_DL_Project\")\n",
    "\n",
    "    print(\"TRACKING URI:\", mlflow.get_tracking_uri())\n",
    "\n",
    "    # exp = mlflow.get_experiment_by_name(\"RNN_DL_Project\")\n",
    "    # print(\"EXPERIMENT:\", exp)\n",
    "    # print(\"ARTIFACT LOCATION:\", exp.artifact_location)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Using device {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fa4bfee8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mathi\\AppData\\Local\\Temp\\ipykernel_45524\\894271143.py:6: DtypeWarning: Columns (3,5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.read_csv(data_path) # loading into dataframe\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded...\n",
      "len of dataset\n",
      "372192\n",
      "Starting tokenization..\n",
      "Done\n",
      "Total games in original data: 372192\n",
      "Total samples (should be <= 2x games): 378838\n",
      "White samples: 188714\n",
      "Black samples: 190124\n",
      "Sample 0: side=White, label=Nakamura, Hikaru, first 10 tokens=[0, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "Sample 1: side=White, label=Nakamura, Hikaru, first 10 tokens=[0, 4, 11, 6, 60, 8, 61, 10, 62, 18]\n",
      "Sample 2: side=White, label=Nakamura, Hikaru, first 10 tokens=[0, 4, 11, 6, 108, 12, 13, 10, 9, 14]\n",
      "Sample 3: side=White, label=Nakamura, Hikaru, first 10 tokens=[0, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "Sample 4: side=White, label=Nakamura, Hikaru, first 10 tokens=[0, 4, 5, 6, 7, 8, 9, 10, 203, 19]\n",
      "Sample 5: side=White, label=Nakamura, Hikaru, first 10 tokens=[0, 4, 70, 6, 9, 12, 182, 19, 203, 14]\n"
     ]
    }
   ],
   "source": [
    "# import games in csv\n",
    "data_path = \"C:\\\\Users\\\\mathi\\\\Documents\\\\University\\\\Aarhus University\\\\MSc Computer Engineering\\\\Semester 1\\\\Deep Learning\\\\project\\\\DeepL_project\\\\data\\\\filtered_games_20_players.csv\"\n",
    "\n",
    "# Data loading\n",
    "print(\"Loading data...\")\n",
    "data = pd.read_csv(data_path) # loading into dataframe\n",
    "\n",
    "print(\"Data loaded...\")\n",
    "#Added first move indicating to predict either white or black\n",
    "#White = 0\n",
    "#Black = 1\n",
    "\n",
    "# --- SETTINGS ---\n",
    "NEW_LIST_OF_PLAYERS_MANUAL = [\n",
    "    'ArasanX','MassterofMayhem','JelenaZ','lestri','doreality','therealYardbird',\n",
    "    'Chesssknock','No_signs_of_V','Recobachess','drawingchest','kasparik_garik',\n",
    "    'ChainsOfFantasia','Consent_to_treatment','Alexandr_KhleBovich','unknown-maestro_2450',\n",
    "    'gefuehlter_FM','gmmitkov','positionaloldman',\"Carlsen, Magnus\",\"Nakamura, Hikaru\"\n",
    "]\n",
    "\n",
    "GAME_LENGTH = 200\n",
    "\n",
    "# --- DATA PREPARATION ---\n",
    "TRAIN_SPLIT = 0.8\n",
    "VALIDATION_SPLIT = 0.1\n",
    "TEST_SPLIT = 0.1\n",
    "MANUAL_SEED = 123  # not used for now\n",
    "\n",
    "# --- ENSURE STRINGS ---\n",
    "data[\"white_name\"] = data[\"white_name\"].astype(str)\n",
    "data[\"black_name\"] = data[\"black_name\"].astype(str)\n",
    "\n",
    "# --- PLAYER MATCH FUNCTION ---\n",
    "def player_match(name: str):\n",
    "    lowered = name.lower()\n",
    "    for player in NEW_LIST_OF_PLAYERS_MANUAL:\n",
    "        if player.lower() == lowered:\n",
    "            return player\n",
    "    return None\n",
    "\n",
    "# --- CREATE WHITE AND BLACK LABELS ---\n",
    "data[\"WhiteLabel\"] = data[\"white_name\"].apply(player_match)\n",
    "data[\"BlackLabel\"] = data[\"black_name\"].apply(player_match)\n",
    "\n",
    "# Keep games where at least one player is in the manual list\n",
    "mask = data[\"WhiteLabel\"].notna() | data[\"BlackLabel\"].notna()\n",
    "data = data[mask].reset_index(drop=True)\n",
    "\n",
    "# --- MAP PLAYERS TO INTEGER LABELS ---\n",
    "encodep = dict(zip(NEW_LIST_OF_PLAYERS_MANUAL, range(len(NEW_LIST_OF_PLAYERS_MANUAL))))\n",
    "decodep = {v: k for k, v in encodep.items()}\n",
    "\n",
    "data[\"WhiteLabelID\"] = data[\"WhiteLabel\"].map(encodep)\n",
    "data[\"BlackLabelID\"] = data[\"BlackLabel\"].map(encodep)\n",
    "\n",
    "print(\"len of dataset\")\n",
    "print(len(data))\n",
    "\n",
    "print(\"Starting tokenization..\")\n",
    "# --- TOKENIZATION ---\n",
    "cleaner = str.maketrans({\"[\": \"\", \"]\": \"\", \"'\": \"\", \",\": \"\"})\n",
    "all_step = [k for s in data[\"list_of_moves\"] for k in s.translate(cleaner).split()]\n",
    "frequency = Counter(all_step)\n",
    "dir = {\"<PAD>\": 2, \"<UNK>\": 3}\n",
    "\n",
    "side_tokens = {\"white\": 0, \"black\": 1}\n",
    "dir.update(side_tokens)\n",
    "dir.update({move: len(dir) + i for i, move in enumerate(frequency)})\n",
    "\n",
    "print(\"Done\")\n",
    "\n",
    "# --- STEP ENCODING WITH SIDE TOKEN ---\n",
    "def step_encode(step, side_token=None):\n",
    "    cleaned = step.translate(cleaner)\n",
    "    tokening = cleaned.split()\n",
    "    vector = list(map(lambda i_token: dir.get(i_token, 1), tokening[:GAME_LENGTH]))\n",
    "\n",
    "    # prepend side token if provided\n",
    "    if side_token is not None:\n",
    "        vector = [side_token] + vector\n",
    "\n",
    "    pad_length = GAME_LENGTH + (1 if side_token is not None else 0) - len(vector)\n",
    "    vector = vector + [2] * pad_length\n",
    "    return vector\n",
    "\n",
    "\n",
    "class GameSequence(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.samples = []\n",
    "        self.moves = df[\"list_of_moves\"].to_list()\n",
    "        self.white_labels = df[\"WhiteLabelID\"].to_list()\n",
    "        self.black_labels = df[\"BlackLabelID\"].to_list()\n",
    "\n",
    "        for i in range(len(df)):\n",
    "            # only add sample if label exists\n",
    "            if not pd.isna(self.white_labels[i]):\n",
    "                self.samples.append((self.moves[i], 0, int(self.white_labels[i])))  # white sample\n",
    "            if not pd.isna(self.black_labels[i]):\n",
    "                self.samples.append((self.moves[i], 1, int(self.black_labels[i])))  # black sample\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        moves, side_token, label = self.samples[idx]\n",
    "        x = torch.tensor(step_encode(moves, side_token=side_token), dtype=torch.long)\n",
    "        y = torch.tensor(label, dtype=torch.long)\n",
    "        return x, y\n",
    "\n",
    "gs_data = GameSequence(data)\n",
    "\n",
    "# --- Basic stats ---\n",
    "total_samples = len(gs_data)\n",
    "num_white = sum(1 for s in gs_data.samples if s[1] == 0)\n",
    "num_black = sum(1 for s in gs_data.samples if s[1] == 1)\n",
    "\n",
    "print(f\"Total games in original data: {len(data)}\")\n",
    "print(f\"Total samples (should be <= 2x games): {total_samples}\")\n",
    "print(f\"White samples: {num_white}\")\n",
    "print(f\"Black samples: {num_black}\")\n",
    "\n",
    "# --- Check first few samples ---\n",
    "for i in range(min(6, total_samples)):\n",
    "    moves, side_token, label = gs_data.samples[i]\n",
    "    side_name = \"White\" if side_token == 0 else \"Black\"\n",
    "    label_name = decodep[label]\n",
    "    print(f\"Sample {i}: side={side_name}, label={label_name}, first 10 tokens={step_encode(moves, side_token=side_token)[:10]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3bce0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- THE RNN MODEL ---\n",
    "# https://www.nature.com/articles/s41598-025-88378-6\n",
    "# Applying attention to LSTM outputs\n",
    "class RecurrentNN(nn.Module):\n",
    "    def __init__(self, dir, dropout, lstm_layers, dim_embedded, dim_hidden_layer, dim_out):\n",
    "        print(dim_out)\n",
    "        super(RecurrentNN, self).__init__()\n",
    "\n",
    "        # lookup table for the tokens\n",
    "        self.table = nn.Embedding( \n",
    "            num_embeddings=dir,\n",
    "            embedding_dim=dim_embedded,  # size of embeddings\n",
    "            padding_idx=2  # telling torch 0's are padding, not actual moves\n",
    "        )\n",
    "\n",
    "        # LSTM block\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=dim_embedded, \n",
    "            hidden_size=dim_hidden_layer,\n",
    "            num_layers=lstm_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=True, # backward and forward\n",
    "            dropout=dropout \n",
    "        )\n",
    "\n",
    "        # Fully connected layers with ReLU and dropout\n",
    "        self.FC = nn.Sequential(\n",
    "            nn.Linear(2*dim_hidden_layer, dim_hidden_layer),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(dim_hidden_layer, dim_out)\n",
    "        )\n",
    "        \n",
    "        self.attention = nn.Linear(2*dim_hidden_layer, 1)\n",
    "\n",
    "        self.norm = nn.LayerNorm(2 * dim_hidden_layer)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.table(x)\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        lstm_out = self.norm(lstm_out)\n",
    "        \n",
    "        attention_weights = torch.softmax(self.attention(lstm_out).squeeze(-1), dim=1)\n",
    "        context_vector = torch.sum(lstm_out * attention_weights.unsqueeze(-1), dim=1)\n",
    "\n",
    "        # Attempted, but not much difference\n",
    "        # context, _ = self.multiheadattention(lstm_out, lstm_out, lstm_out)\n",
    "        # context_vector = context.mean(dim=1)\n",
    "\n",
    "        return self.FC(context_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a7a52d",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [s[2] for s in gs_data.samples]  # s[2] is the label in (moves, side_token, label)\n",
    "\n",
    "train_indices, val_test_indices = train_test_split(\n",
    "    range(len(gs_data)),\n",
    "    test_size=VALIDATION_SPLIT + TEST_SPLIT,\n",
    "    stratify=labels,  # stratify using pre-expanded sample labels\n",
    "    random_state=123\n",
    ")\n",
    "\n",
    "validation_indices, test_indices = train_test_split(\n",
    "    val_test_indices,\n",
    "    test_size=TEST_SPLIT / (VALIDATION_SPLIT + TEST_SPLIT),\n",
    "    stratify=[labels[i] for i in val_test_indices],\n",
    "    random_state=123\n",
    ")\n",
    "\n",
    "train_data = Subset(gs_data, train_indices)\n",
    "validation_data = Subset(gs_data, validation_indices)\n",
    "test_data = Subset(gs_data, test_indices)\n",
    "print(\"Data subsets created...\")\n",
    "\n",
    "# Cross Entropy loss (ideal and simple for classification tasks)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# For RNN's, ADAM is the way to go.\n",
    "def train_validate(train_loader: DataLoader,\n",
    "                   validation_loader: DataLoader,\n",
    "                   model: nn.Module,\n",
    "                   optimizer,\n",
    "                   scheduler,\n",
    "                   device: torch.device):\n",
    "    model.train() # training mode activation before updating gradients\n",
    "\n",
    "    # Initialize variables\n",
    "    batch_losses_train = []  # each batch, the loss is stored and later averaged to get an average train loss per epoch\n",
    "\n",
    "    # used for f1 score and accuracy metrics\n",
    "    pred_labels_train = []\n",
    "    true_labels_train = []\n",
    "\n",
    "    for xbatch, ybatch in tqdm(train_loader, colour='green'): # iterating batches\n",
    "        xbatch = xbatch.to(device)\n",
    "        ybatch = ybatch.to(device)\n",
    "\n",
    "        # reset from last batch\n",
    "        optimizer.zero_grad()\n",
    "        output = model(xbatch)\n",
    "        loss = loss_fn(output, ybatch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        preds = torch.argmax(output, dim=1)\n",
    "\n",
    "        batch_losses_train.append(loss.item())\n",
    "        pred_labels_train.append(preds)\n",
    "        true_labels_train.append(ybatch)\n",
    "\n",
    "    # Format useful lists for calculation of metrics\n",
    "    pred_labels_train = torch.cat(pred_labels_train, dim=0).detach().cpu().numpy().flatten()\n",
    "    true_labels_train = torch.cat(true_labels_train, dim=0).detach().cpu().numpy().flatten()\n",
    "\n",
    "    avg_train_loss = np.mean(batch_losses_train)\n",
    "\n",
    "    # VALIDATE\n",
    "    model.eval()\n",
    "\n",
    "    # Initialize variables\n",
    "    batch_losses_val = []  # each batch, the loss is stored and later averaged to get an average train loss per epoch\n",
    "\n",
    "    # used for f1 score and accuracy metrics\n",
    "    pred_labels_val = []\n",
    "    true_labels_val = []\n",
    "\n",
    "    with torch.no_grad(): # without gradient update for evaluation\n",
    "        for xbatch, ybatch in validation_loader:\n",
    "            xbatch = xbatch.to(device)\n",
    "            ybatch = ybatch.to(device)\n",
    "\n",
    "            output = model(xbatch)\n",
    "            loss = loss_fn(output, ybatch)\n",
    "\n",
    "            batch_losses_val.append(loss.item())\n",
    "\n",
    "            preds = torch.argmax(output, dim=1)\n",
    "\n",
    "            pred_labels_val.append(preds)\n",
    "            true_labels_val.append(ybatch)\n",
    "\n",
    "    avg_val_loss = np.mean(batch_losses_val)\n",
    "\n",
    "    # Format useful lists for calculation of metrics\n",
    "    pred_labels_val = torch.cat(pred_labels_val, dim=0).cpu().detach().numpy().flatten()\n",
    "    true_labels_val = torch.cat(true_labels_val, dim=0).cpu().detach().numpy().flatten()\n",
    "\n",
    "    return avg_train_loss, avg_val_loss, \\\n",
    "        (pred_labels_train, true_labels_train), \\\n",
    "        (pred_labels_val, true_labels_val)\n",
    "\n",
    "def test(test_loader: DataLoader,\n",
    "         model: nn.Module,\n",
    "         device: torch.device):\n",
    "    # Now we test on the test data at the end\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    # Initialize variables\n",
    "    batch_losses_test = []  # each batch, the loss is stored and later averaged to get an average train loss per epoch\n",
    "\n",
    "    # used for f1 score and accuracy metrics\n",
    "    pred_labels_test = []\n",
    "    true_labels_test = []\n",
    "\n",
    "    with torch.no_grad(): # without gradient update for evaluation\n",
    "        for xbatch, ybatch in test_loader:\n",
    "            xbatch = xbatch.to(device)\n",
    "            ybatch = ybatch.to(device)\n",
    "\n",
    "            output = model(xbatch)\n",
    "            loss = loss_fn(output, ybatch)\n",
    "\n",
    "            batch_losses_test.append(loss.item())\n",
    "\n",
    "            preds = torch.argmax(output, dim=1)\n",
    "\n",
    "            pred_labels_test.append(preds)\n",
    "            true_labels_test.append(ybatch)\n",
    "\n",
    "    avg_test_loss = np.mean(batch_losses_test)\n",
    "\n",
    "    # Format useful lists for calculation of metrics\n",
    "    pred_labels_test = torch.cat(pred_labels_test, dim=0).cpu().detach().numpy().flatten()\n",
    "    true_labels_test = torch.cat(true_labels_test, dim=0).cpu().detach().numpy().flatten()\n",
    "\n",
    "    return avg_test_loss, (pred_labels_test, true_labels_test)\n",
    "\n",
    "\n",
    "'''\n",
    "To calculate different kind of metrics based on:\n",
    "- Average (train/validation/test) loss\n",
    "- Predicted labels for training/validation/test\n",
    "- True labels for training/validation/test\n",
    "\n",
    "Calculates:\n",
    "- Classification metrics\n",
    "    - F1 scores (macro, weighted, per-class)\n",
    "    - Confusion matrix (and prints)\n",
    "    - Classification report\n",
    "    - Balanced accuracy score\n",
    "- Regression metrics\n",
    "    - Mean absolute error\n",
    "    - (root) mean squared error\n",
    "    - R2 score\n",
    "\n",
    "Shows:\n",
    "    - Confusion matrix\n",
    "    - Predicted vs True regression plot\n",
    "'''\n",
    "def calculateMetrics(avg_loss : np.floating, predicted_labels : np.ndarray, true_labels : np.ndarray):\n",
    "    # --- CLASSIFICATION ---\n",
    "    '''\n",
    "    Macro F1 = The average f1 score over all classes, treating each class equally.\n",
    "    This score becomes more relevant when some players have very few games.\n",
    "    '''\n",
    "    macro_f1 = f1_score(true_labels, predicted_labels, average=\"macro\")\n",
    "\n",
    "    '''\n",
    "    Weighted F1 = Same as Macro F1, but is weighted by class frequency. It doesn't punish too hard for players with few games.\n",
    "    '''\n",
    "    weighted_f1 = f1_score(true_labels, predicted_labels, average=\"weighted\")\n",
    "\n",
    "    '''\n",
    "    Per-class F1 scores, this doesn't average over all classes and shows how different players compare\n",
    "    Is not printed, because classification_report already does it in a nice way, but wanted to include here\n",
    "    Because it shows the relevance.\n",
    "    '''\n",
    "    per_class_f1 = f1_score(true_labels, predicted_labels, average=None)\n",
    "\n",
    "    '''\n",
    "    A full on confusion matrix of shape NxN.\n",
    "    '''\n",
    "    conf_matrix = confusion_matrix(true_labels, predicted_labels)\n",
    "\n",
    "    '''\n",
    "    Balanced accuracy = Each class contributes equally to the accuracy, better than the usual way of calculating accuracy: correct / total\n",
    "    '''\n",
    "    bal_accuracy = balanced_accuracy_score(true_labels, predicted_labels)\n",
    "\n",
    "    print(\n",
    "        f\"--- CLASSIFICATION METRICS --- \\n\"\n",
    "        f\"F1 scores: [Macro={macro_f1:.3f}, Weighted={weighted_f1:.3f}] \\n\"\n",
    "        f\"Balanced Accuracy = {bal_accuracy:.3f}\\n\"\n",
    "        f\"Average loss = {avg_loss:.5f}\")\n",
    "\n",
    "    report = classification_report(true_labels, predicted_labels, output_dict=True)\n",
    "    df = pd.DataFrame(report).transpose()\n",
    "    # print(tabulate(df.round(3), headers='keys', tablefmt=\"pretty\"))\n",
    "    print(df.round(3))\n",
    "\n",
    "    # Don't show plots for now...\n",
    "    # plt.figure(figsize=(6, 6))\n",
    "    # sns.heatmap(conf_matrix, annot=False, cmap=\"Blues\")\n",
    "    # plt.title(\"Confusion matrix\")\n",
    "    # plt.show()\n",
    "    \n",
    "    return macro_f1, weighted_f1, bal_accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8a809d",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Function used for the grid search algorithm, input = grid search array\n",
    "Initialization steps unaffected by the runGridPoint are NOT ran again. \n",
    "Runs the whole RNN loop (from creating data loaders to training the model, with specified hyperparamaters)\n",
    "'''\n",
    "def runGridPoint(grid_search_array : list, id : int, early_stop_patience=5, epochs=10, logging=False):\n",
    "    # Affected by Grid Search\n",
    "    BATCH_SIZE = int(grid_search_array[0])\n",
    "    DROPOUT = float(grid_search_array[1])\n",
    "    HIDDEN_LAYER_DIM = grid_search_array[2]\n",
    "    LSTM_LAYERS = int(grid_search_array[3])\n",
    "    \n",
    "    LEARNING_RATE = 0.001\n",
    "    USE_LOGGING = logging\n",
    "\n",
    "    if BATCH_SIZE == 256:\n",
    "        LEARNING_RATE = 0.03    \n",
    "    elif BATCH_SIZE == 128:\n",
    "        LEARNING_RATE = 0.002\n",
    "    elif BATCH_SIZE == 64:\n",
    "        LEARNING_RATE = 0.001\n",
    "\n",
    "    # Unaffected by grid search\n",
    "    EMBEDDED_LAYER_DIM = 128\n",
    "    EPOCHS = epochs\n",
    "    # EARLY STOPPING\n",
    "    early_stop_counter = 0  # do not change\n",
    "    early_stop_best_loss = torch.inf\n",
    "    early_stop_best_model_state = None\n",
    "    PATIENCE = early_stop_patience  # after how many epochs of no decrease in loss should we stop\n",
    "    DELTA = 0.0  # if the loss decreases with maximum this delta, do not reset the counter\n",
    "    \n",
    "    if USE_LOGGING:\n",
    "        r_name = f\"RUN 000{str(id)}\"\n",
    "        with mlflow.start_run(run_name=r_name):\n",
    "            print(f\"Starting new run {id}...\")\n",
    "            TRAIN_DATALOADER = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "            VALIDATION_DATALOADER = DataLoader(validation_data, batch_size=BATCH_SIZE)\n",
    "            TEST_DATALOADER = DataLoader(test_data, batch_size=BATCH_SIZE)\n",
    "\n",
    "            mlflow.log_param('batch_size', BATCH_SIZE)\n",
    "            mlflow.log_param('dropout', DROPOUT)\n",
    "            mlflow.log_param('hidden_layer_dim', HIDDEN_LAYER_DIM)\n",
    "            mlflow.log_param('lstm_layers', LSTM_LAYERS)\n",
    "            mlflow.log_param('learning_rate', LEARNING_RATE)\n",
    "\n",
    "            model = RecurrentNN( # Building model\n",
    "                dir=len(dir),\n",
    "                dropout=DROPOUT,\n",
    "                lstm_layers=LSTM_LAYERS,\n",
    "                dim_embedded=EMBEDDED_LAYER_DIM,\n",
    "                dim_hidden_layer=HIDDEN_LAYER_DIM,\n",
    "                dim_out=len(encodep)).to(device)\n",
    "            \n",
    "            optimizer = torch.optim.Adam(params=list(model.parameters()), lr=LEARNING_RATE)\n",
    "                \n",
    "            # --- TRAINING, VALIDATION ---\n",
    "            train_losses, val_losses = [], []\n",
    "\n",
    "            print(f\"Beginning training... using {device} device\")\n",
    "            for iEpoch in range(EPOCHS):\n",
    "                t_loss, v_loss, (pltrain, tltrain), (plval, tlval)\\\n",
    "                = train_validate(train_loader=TRAIN_DATALOADER,\n",
    "                                validation_loader=VALIDATION_DATALOADER,\n",
    "                                model=model,\n",
    "                                optimizer=optimizer,\n",
    "                                scheduler=None,\n",
    "                                device=device)\n",
    "\n",
    "                print(f\"Epoch {iEpoch}, training metrics:\")\n",
    "                t_macro_f1, t_weighted_f1, t_balanced_acc = calculateMetrics(t_loss, pltrain, tltrain)\n",
    "\n",
    "                print(f\"Epoch {iEpoch}, validation metrics:\")\n",
    "                v_macro_f1, v_weighted_f1, v_balanced_acc = calculateMetrics(v_loss, plval, tlval)\n",
    "\n",
    "                mlflow.log_metric('train/loss', t_loss, step=iEpoch)\n",
    "                mlflow.log_metric('train/macro_f1', t_macro_f1, step=iEpoch)\n",
    "                mlflow.log_metric('train/weighted_f1', t_weighted_f1, step=iEpoch)\n",
    "                mlflow.log_metric('train/balanced_acc', t_balanced_acc, step=iEpoch)\n",
    "                mlflow.log_metric('val/loss', v_loss, step=iEpoch)\n",
    "                mlflow.log_metric('val/macro_f1', v_macro_f1, step=iEpoch)\n",
    "                mlflow.log_metric('val/weighted_f1', v_weighted_f1, step=iEpoch)\n",
    "                mlflow.log_metric('val/balanced_acc', v_balanced_acc, step=iEpoch)\n",
    "                \n",
    "                early_stop_best_model_state = model.state_dict()\n",
    "\n",
    "                # -- EARLY STOPPING CHECK --\n",
    "                if v_loss < early_stop_best_loss - DELTA:\n",
    "                    # A better loss was found, so reset counter and save model state\n",
    "                    early_stop_best_loss = v_loss\n",
    "                    early_stop_counter = 0\n",
    "                    # Save the best model so we can restore it later and get the best model performance to use the test data for.\n",
    "                    early_stop_best_model_state = model.state_dict()\n",
    "                else:\n",
    "                    early_stop_counter += 1\n",
    "                    if early_stop_counter >= PATIENCE:\n",
    "                        print(f\"Early stopping...\")\n",
    "\n",
    "                        # Restore the best model which was saved earlier.\n",
    "                        model.load_state_dict(early_stop_best_model_state)\n",
    "                        break\n",
    "\n",
    "            # --- TESTING ---\n",
    "            avg_test_loss, (pred_labels_test, true_labels_test) = test(test_loader=TEST_DATALOADER,\n",
    "                                model=model,\n",
    "                                device=device)\n",
    "            print(f\"Epoch {iEpoch}, testing metrics:\")\n",
    "            t_macro_f1, t_weighted_f1, t_balanced_acc = calculateMetrics(avg_test_loss, pred_labels_test, true_labels_test)\n",
    "            \n",
    "            mlflow.log_metric('test/loss', avg_test_loss)\n",
    "            mlflow.log_metric('test/macro_f1', t_macro_f1)\n",
    "            mlflow.log_metric('test/weighted_f1', t_weighted_f1)\n",
    "            mlflow.log_metric('test/balanced_acc', t_balanced_acc)\n",
    "            #mlflow.keras.log_model(model, 'rnn_model')\n",
    "    else:\n",
    "        print(f\"Starting new run, NO LOGGING...\")\n",
    "        TRAIN_DATALOADER = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "        VALIDATION_DATALOADER = DataLoader(validation_data, batch_size=BATCH_SIZE)\n",
    "        TEST_DATALOADER = DataLoader(test_data, batch_size=BATCH_SIZE)\n",
    "\n",
    "        model = RecurrentNN( # Building model\n",
    "            dir=len(dir),\n",
    "            dropout=DROPOUT,\n",
    "            lstm_layers=LSTM_LAYERS,\n",
    "            dim_embedded=EMBEDDED_LAYER_DIM,\n",
    "            dim_hidden_layer=HIDDEN_LAYER_DIM,\n",
    "            dim_out=len(encodep)).to(device)\n",
    "        \n",
    "        optimizer = torch.optim.Adam(params=list(model.parameters()), lr=LEARNING_RATE)\n",
    "            \n",
    "        # --- TRAINING, VALIDATION ---\n",
    "        train_losses, val_losses = [], []\n",
    "\n",
    "        print(f\"Beginning training... using {device} device\")\n",
    "        for iEpoch in range(EPOCHS):\n",
    "            t_loss, v_loss, (pltrain, tltrain), (plval, tlval)\\\n",
    "            = train_validate(train_loader=TRAIN_DATALOADER,\n",
    "                            validation_loader=VALIDATION_DATALOADER,\n",
    "                            model=model,\n",
    "                            optimizer=optimizer,\n",
    "                            scheduler=None,\n",
    "                            device=device)\n",
    "\n",
    "            print(f\"Epoch {iEpoch}, training metrics:\")\n",
    "            t_macro_f1, t_weighted_f1, t_balanced_acc = calculateMetrics(t_loss, pltrain, tltrain)\n",
    "\n",
    "            print(f\"Epoch {iEpoch}, validation metrics:\")\n",
    "            v_macro_f1, v_weighted_f1, v_balanced_acc = calculateMetrics(v_loss, plval, tlval)\n",
    "            \n",
    "            early_stop_best_model_state = model.state_dict()\n",
    "\n",
    "            # -- EARLY STOPPING CHECK --\n",
    "            if v_loss < early_stop_best_loss - DELTA:\n",
    "                # A better loss was found, so reset counter and save model state\n",
    "                early_stop_best_loss = v_loss\n",
    "                early_stop_counter = 0\n",
    "                # Save the best model so we can restore it later and get the best model performance to use the test data for.\n",
    "                early_stop_best_model_state = model.state_dict()\n",
    "            else:\n",
    "                early_stop_counter += 1\n",
    "                if early_stop_counter >= PATIENCE:\n",
    "                    print(f\"Early stopping...\")\n",
    "\n",
    "                    # Restore the best model which was saved earlier.\n",
    "                    model.load_state_dict(early_stop_best_model_state)\n",
    "                    break\n",
    "\n",
    "        # --- TESTING ---\n",
    "        avg_test_loss, (pred_labels_test, true_labels_test) = test(test_loader=TEST_DATALOADER,\n",
    "                            model=model,\n",
    "                            device=device)\n",
    "        print(f\"Epoch {iEpoch}, testing metrics:\")\n",
    "        t_macro_f1, t_weighted_f1, t_balanced_acc = calculateMetrics(avg_test_loss, pred_labels_test, true_labels_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe65ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This code thinks about how to implement grid search (for our RNN model), given the lecture\n",
    "given by Kaare about Hyperparameter Search. \n",
    "\n",
    "Notes on the lecture / things to keep in mind:\n",
    "- Random grid search is probably not the way to go since big gaps can occur\n",
    "- Rotated grid search sounds really promising, also given the paper \"Rotated Grid Search for Hyperparamater Optimization\" by Allawala et al.\n",
    "    -> However, it's complicating to think about rotations in >2D space\n",
    "- Make sure to search the full range, there will be regions of trainability, which we can search better afterwards\n",
    "- Models than end low often drop in early epochs. Hence,\n",
    "    -> Optimize searching algorithm by not training unpromising hyperparameters\n",
    "- Learning rate scheduler is probably a good idea in the end if we are plateauing\n",
    "    - Cyclical schedulers seem very promising, but for RNN's it may be too complex, as training in general does not take that long\n",
    "\n",
    "\n",
    "Hyperparameter specific notes:\n",
    "- Bigger learning rate is already regularization form\n",
    "    - Decrease other regularization methods (dropout, batch normalization), works vice versa too!\n",
    "- Batch size -> If using batch_loss as guidance, set reduction to mean. \n",
    "    - Also, store number of time passed when comparing batch sizes, smaller batch sizes usually take less epochs\n",
    "\n",
    "'''\n",
    "# --- GRID SEARCH PARAMETERS ---\n",
    "\n",
    "# Questions:\n",
    "# Should we tokenize on a random 80% split, or should we tokenize within the fold?\n",
    "\n",
    "# These will have huge gaps, to see where the most potential lies.\n",
    "initial_grid_search_parameters = {\n",
    "    'batch_size': [16, 256, 1024],\n",
    "    'dropout': [0, 0.3, 0.9],\n",
    "    'hidden_layer_dim': [32, 256, 1024],\n",
    "    'lstm_layers': [2, 3],\n",
    "}\n",
    "\n",
    "concentrated_grid_search_parameters = {\n",
    "    'batch_size': [64, 128, 256],\n",
    "    'dropout': [0.2, 0.3, 0.4, 0.5],\n",
    "    'hidden_layer_dim': [64, 128, 256],\n",
    "    'lstm_layers': [2, 3],\n",
    "}\n",
    "\n",
    "def return_combinations(dict_hyperparameters):\n",
    "    # We want to create all possible combinations, but systematically so we can interrupt if we don't see good results\n",
    "    all_parameters = ([list(x) for x in dict_hyperparameters.values()])\n",
    "\n",
    "    return list(itertools.product(*all_parameters))\n",
    "\n",
    "\n",
    "runGridPoint([128, 0.5, 128, 2, 0.001], id=2)\n",
    "# runGridPoint([512, 0.5, 512, 2, 0.01], 3)\n",
    "# runGridPoint([512, 0.2, 128, 5, 0.01], 4)\n",
    "# runGridPoint([64, 0.5, 128, 2, 0.001], 5)\n",
    "# runGridPoint([64, 0.2, 128, 2, 0.001], 6)\n",
    "\n",
    "print(len(return_combinations(initial_grid_search_parameters)))\n",
    "\n",
    "\n",
    "# for i, comb in enumerate(return_combinations(initial_grid_search_parameters)):\n",
    "#     runGridPoint(comb, id=i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58170610",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting new run, NO LOGGING...\n",
      "Beginning training... using cpu device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|\u001b[32m          \u001b[0m| 0/592 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|\u001b[32m          \u001b[0m| 0/592 [00:19<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[44]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mrunGridPoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m512\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m128\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mid\u001b[39;49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 351\u001b[39m, in \u001b[36mrunGridPoint\u001b[39m\u001b[34m(gridSearchArray, id)\u001b[39m\n\u001b[32m    348\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mBeginning training... using \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdevice\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m device\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    349\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m iEpoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(EPOCHS):\n\u001b[32m    350\u001b[39m     t_loss, v_loss, (pltrain, tltrain), (plval, tlval)\\\n\u001b[32m--> \u001b[39m\u001b[32m351\u001b[39m     = \u001b[43mtrain_validate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mTRAIN_DATALOADER\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    352\u001b[39m \u001b[43m                    \u001b[49m\u001b[43mvalidation_loader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mVALIDATION_DATALOADER\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    353\u001b[39m \u001b[43m                    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m                    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m                    \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m                    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    358\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00miEpoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, training metrics:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    359\u001b[39m     t_macro_f1, t_weighted_f1, t_balanced_acc = calculateMetrics(t_loss, pltrain, tltrain)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 49\u001b[39m, in \u001b[36mtrain_validate\u001b[39m\u001b[34m(train_loader, validation_loader, model, optimizer, scheduler, device)\u001b[39m\n\u001b[32m     47\u001b[39m output = model(xbatch)\n\u001b[32m     48\u001b[39m loss = loss_fn(output, ybatch)\n\u001b[32m---> \u001b[39m\u001b[32m49\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     50\u001b[39m optimizer.step()\n\u001b[32m     52\u001b[39m preds = torch.argmax(output, dim=\u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mathi\\miniconda3\\envs\\myenv\\Lib\\site-packages\\torch\\_tensor.py:648\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    638\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    639\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    640\u001b[39m         Tensor.backward,\n\u001b[32m    641\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    646\u001b[39m         inputs=inputs,\n\u001b[32m    647\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m648\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mathi\\miniconda3\\envs\\myenv\\Lib\\site-packages\\torch\\autograd\\__init__.py:353\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    348\u001b[39m     retain_graph = create_graph\n\u001b[32m    350\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m353\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mathi\\miniconda3\\envs\\myenv\\Lib\\site-packages\\torch\\autograd\\graph.py:824\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    822\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    823\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m824\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    825\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    826\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    827\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    828\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "USE_LOGGING = True\n",
    "if USE_LOGGING:\n",
    "        # Run MLFlow locally on port 5000, set IP address here:\n",
    "    mlflow.set_tracking_uri(\n",
    "        \"http://ip:5000\"\n",
    "    )\n",
    "\n",
    "\n",
    "    # maybe make experiment first, before calling this.\n",
    "    mlflow.set_experiment(\"RNN_DL_Project\")\n",
    "\n",
    "    print(\"TRACKING URI:\", mlflow.get_tracking_uri())\n",
    "\n",
    "    # exp = mlflow.get_experiment_by_name(\"RNN_DL_Project\")\n",
    "    # print(\"EXPERIMENT:\", exp)\n",
    "    # print(\"ARTIFACT LOCATION:\", exp.artifact_location)\n",
    "\n",
    "runGridPoint(grid_search_array=[64, 0.5, 128, 3], id=2, early_stop_patience=5, epochs=10, logging=USE_LOGGING)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
